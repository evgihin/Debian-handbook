# AUTHOR <EMAIL@ADDRESS>, YEAR.
# Muhammad Saied <msaied93@gmail.com>, 2014, 2015, 2016.
msgid ""
msgstr "Project-Id-Version: 0\nPOT-Creation-Date: 2020-08-28 10:15+0200\nPO-Revision-Date: 2021-01-14 05:32+0000\nLast-Translator: Jad Madi <dev@madi.se>\nLanguage-Team: Arabic (Morocco) <https://hosted.weblate.org/projects/debian-handbook/12_advanced-administration/ar_MA/>\nLanguage: ar-MA\nMIME-Version: 1.0\nContent-Type: text/plain; charset=UTF-8\nContent-Transfer-Encoding: 8bit\nPlural-Forms: nplurals=6; plural=n==0 ? 0 : n==1 ? 1 : n==2 ? 2 : n%100>=3 && n%100<=10 ? 3 : n%100>=11 ? 4 : 5;\nX-Generator: Weblate 4.4.1-dev\n"

msgid "RAID"
msgstr "RAID"

msgid "LVM"
msgstr "LVM"

msgid "FAI"
msgstr "FAI"

msgid "Preseeding"
msgstr "تغذية"

msgid "Monitoring"
msgstr "المراقبة"

msgid "Virtualization"
msgstr "الحوسبة الظاهرية"

msgid "Xen"
msgstr "‏Xen"

msgid "LXC"
msgstr "‏LXC"

msgid "Advanced Administration"
msgstr "الإدارة المتقدمة"

msgid "This chapter revisits some aspects we already described, with a different perspective: instead of installing one single computer, we will study mass-deployment systems; instead of creating RAID or LVM volumes at install time, we'll learn to do it by hand so we can later revise our initial choices. Finally, we will discuss monitoring tools and virtualization techniques. As a consequence, this chapter is more particularly targeting professional administrators, and focuses somewhat less on individuals responsible for their home network."
msgstr "يعيد هذا الفصل النظر في بعض القضايا التي ناقشناها سابقاً، لكن من وجهة نظر مختلفة: سوف ندرس تجهيز الأنظمة الكبيرة بدلاً من تجهيز حاسوب مفرد؛ وسوف نتعلم ضبط LVM و RAID يدوياً بدل الضبط الآلي عند التثبيت، حتى نتمكن من تعديل الخيارات التي حددناها سابقاً. أخيراً، سوف نتحدث عن أدوات المراقبة وتقنيات المحاكاة. أي أن هذا الفصل موجَّه لمديري النظم المحترفين أكثر مما يركز على ما يهم الأفراد الذين يديرون شبكة منزلية."

msgid "RAID and LVM"
msgstr "‏RAID وLVM"

msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive reinstallation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgstr "استعرض <xref linkend=\"installation\" /> هذه التقنيات من وجهة نظر برنامج التثبيت، والطريقة التي دمجت فيها هذه التقنيات حتى يكون إعدادها سهلاً منذ البداية. يجب على مدير النظام أن يستطيع معالجة الحاجات المتزايدة للمساحة التخزينية بعد التثبيت الأولي للنظام، دون اللجوء إلى عملية إعادة التثبيت المكلفة (من ناحية الوقت والجهد). أي أن مدير النظام يجب أن يستخدم الأدوات المطلوبة لتعديل نظامي LVM و RAID بمهارة."

msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former ensures the security and availability of the data in case of hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgstr "تستخدم تقنيتا LVM و RAID لعزل الحيز التخزيني المتاح لنظام الملفات عن الحيز التخزيني الفيزيائي (الأقراص الصلبة الفعلية أو الأقسام partitions)؛ تحمي تقنية RAID البيانات من خلال التخزين الفائض، بينما تجعل تقنية LVM إدارة البيانات أكثر مرونة واستقلالاً عن السَّعَة الحقيقية للأقراص التي تحميل تلك البيانات. في الحالتين، يعتمد النظام على أجهزة تخزينية جديدة، يمكن استخدامها لإنشاء نظم ملفات أو مساحات الإبدال Swap، دون أن ترتبط بقرص فيزيائي واحد. إن جذور التقنيتين مختلفة كثيرًا، لكن وظائفهما متشابهة نوعًا ما، ولهذا غالبًا ما تذكران معًا."

msgid "<emphasis>PERSPECTIVE</emphasis> Btrfs combines LVM and RAID"
msgstr "<emphasis>منظور</emphasis> Btrfs يجمع بين LVM وRAID "

#, fuzzy
#| msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a new filesystem, initially developed at Oracle, that purports to combine the featuresets of LVM and RAID and much more. It is mostly functional, and although it is still tagged “experimental” because its development is incomplete (some features aren't implemented yet), it has already seen some use in production environments. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"
msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a filesystem, initially developed at Oracle, that purports to combine the featuresets of LVM and RAID and much more. <ulink type=\"block\" url=\"https://btrfs.wiki.kernel.org/index.php/Main_Page\" />"
msgstr "في حين LVM و RAID هما نظامان فرعيان متميزان من النواة التي تأتي بين أجهزة كتلة القرص وأنظمة الملفات الخاصة بها ، <emphasis> btrfs </emphasis> هو نظام ملفات ، تم تطويره في البداية في Oracle ، الذي يزعم الجمع بين ميزات LVM و RAID وأكثر من ذلك بكثير. <ulink type=\"block\" url=\"https://btrfs.wiki.kernel.org/index.php/Main_Page\" />"

msgid "Among the noteworthy features are the ability to take a snapshot of a filesystem tree at any point in time. This snapshot copy doesn't initially use any disk space, the data only being duplicated when one of the copies is modified. The filesystem also handles transparent compression of files, and checksums ensure the integrity of all stored data."
msgstr "من المزايا التي تستحق الذكر هي إمكانية أخذ لقطة snapshot لشجرة نظام الملفات عند أي لحظة زمنية. هذه اللقطة لا تحجز أي مساحة على القرص، إذا أن البيانات لا تنسخ قبل أن تجرى بعض التغييرات عليها. كما أن نظام الملفات يعالج أيضًا الضغط الشفاف للملفات، وهناك checksums تضمن سلامة كافة البيانات المخزنة."

msgid "In both the RAID and LVM cases, the kernel provides a block device file, similar to the ones corresponding to a hard disk drive or a partition. When an application, or another part of the kernel, requires access to a block of such a device, the appropriate subsystem routes the block to the relevant physical layer. Depending on the configuration, this block can be stored on one or several physical disks, and its physical location may not be directly correlated to the location of the block in the logical device."
msgstr "في حال استخدام RAID أو LVM، توفر النواة ملف جهاز تخزيني (كتلي) block device file، يشبه الملفات التي تمثل الأقراص الصلبة أو أقسام الأقراص. عندما يحتاج أحد التطبيقات، أو أحد أجزاء النواة، للوصول إلى كتلة block من جهاز تخزيني من هذا النوع، يعمل النظام الفرعي المناسب (نظام LVM أو RAID) على توجيه هذه الكتلة إلى الطبقة الفيزيائية الموافقة. وحسب إعداد النظام، يمكن أن تُخزَّن هذه الكتلة على قرص فيزيائي واحد أو أكثر، كما أن موقعها الفيزيائي قد لا يرتبط بموقعها ضمن الجهاز المنطقي."

msgid "Software RAID"
msgstr "‏Software RAID"

msgid "<primary>RAID</primary>"
msgstr "<primary>RAID</primary>"

#, fuzzy
#| msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss and ensure availability in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgstr "كلمة RAID تعني <emphasis>Redundant Array of Independent Disks</emphasis>. يهدف هذا النظام إلى حماية البيانات من الضياع في حال عطب القرص الصلب. المبدأ العام بسيط جدًا: تخزن البيانات على عدة أقراص فيزيائية بدلًا من تخزينها على قرص واحد، ويكون مستوى التخزين الفائض قابلاً للضبط. بالاعتماد على هذا التخزين الفائض، يمكن استعادة البيانات دون أية خسارة حتى في حال تعطل أحد الأقراص بشكل غير متوقع."

msgid "<emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?"
msgstr "<emphasis>ثقافة</emphasis> <foreignphrase>Independent</foreignphrase> أو <foreignphrase>inexpensive</foreignphrase>؟"

#, fuzzy
#| msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavour of cheapness."
msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavor of cheapness."
msgstr "كان حرف I في الاختصار RAID يرمز لكلمة <emphasis>inexpensive</emphasis>، لأن RAID قدمت نقلة نوعية في أمان البيانات دون الاضطرار لشراء أقراص متطورة باهظة الثمن. إلا أنها اليوم تروج على أنها تشير إلى <emphasis>independent</emphasis>، ربما حتى لا تعطي انطباعاً غير مرغوب بالفقر."

msgid "RAID can be implemented either by dedicated hardware (RAID modules integrated into SCSI or SATA controller cards) or by software abstraction (the kernel). Whether hardware or software, a RAID system with enough redundancy can transparently stay operational when a disk fails; the upper layers of the stack (applications) can even keep accessing the data in spite of the failure. Of course, this “degraded mode” can have an impact on performance, and redundancy is reduced, so a further disk failure can lead to data loss. In practice, therefore, one will strive to only stay in this degraded mode for as long as it takes to replace the failed disk. Once the new disk is in place, the RAID system can reconstruct the required data so as to return to a safe mode. The applications won't notice anything, apart from potentially reduced access speed, while the array is in degraded mode or during the reconstruction phase."
msgstr "يمكن تطبيق RAID باستخدام عتاد خاص (وحدات RAID مدمجة في متحكِّمات SCSI أو SATA) أو برمجيًا (عبر النواة). سواء كان النظام يعتمد على العتاد أو البرمجيات، يستطيع RAID أن يبقى في الخدمة عند عطب أحد الأقراص إذا كان هناك تخزين فائض كاف؛ إذا يمكن للطبقة العليا (التطبيقات) أن تستمر بالوصول إلى البيانات بغض النظر عن العطل. طبعاً، يمكن أن يؤثر ”وضع degraded“ هذا على الأداء، كما أن الفائض التخزيني ينخفض، ما يعني إمكانية خسارة البيانات إذا حصل عطل آخر في الأقراص. ولهذا لا يتم الاعتماد على degraded mode عمليًا إلا خلال المدة اللازمة لاستبدال القرص المعطوب. يستطيع نظام RAID إعادة بناء المعلومات اللازمة للعودة إلى الوضع الآمن بعد تثبيت القرص الجديد. لن تلاحظ البرمجيات أي شيء، أو ربما تشعر ببعض البطء في سرعة الوصول إلى البيانات عندما تكون المصفوفة في الوضع degraded أو أثناء مرحلة إعادة بناء البيانات المفقودة."

msgid "When RAID is implemented by hardware, its configuration generally happens within the BIOS setup tool, and the kernel will consider a RAID array as a single disk, which will work as a standard physical disk, although the device name may be different (depending on the driver)."
msgstr "عندما يعتمد على العتاد لبناء مصفوفات RAID، فغالباً ما يتم إعداد النظام عبر أداة إعداد BIOS، وتعتبر النواة مصفوفة RAID كقرص واحد، يعمل مثل قرص فيزيائي قياسي، إلا أن اسم الجهاز قد يختلف (تبعاً لبرنامج التعريف)."

msgid "We only focus on software RAID in this book."
msgstr "سوف نركز على RAID البرمجي فقط في هذا الكتاب."

msgid "Different RAID Levels"
msgstr "مستويات RAID المختلفة"

msgid "RAID is actually not a single system, but a range of systems identified by their levels; the levels differ by their layout and the amount of redundancy they provide. The more redundant, the more failure-proof, since the system will be able to keep working with more failed disks. The counterpart is that the usable space shrinks for a given set of disks; seen the other way, more disks will be needed to store a given amount of data."
msgstr "في الواقع RAID ليس نظاماً واحداً، بل مجموعة من النظم لكل منها مستوى؛ وتختلف المستويات عن بعضها بالتنظيم وكمية الفائض التي تقدمها. كلما كان الفائض أكبر كلما كان النظام أكثر مقاومة للأعطال، ذلك لأن النظام سيبقى في الخدمة مع المزيد من الأقراص المعطوبة. الناحية السلبية هي أن المساحة التخزينية المتاحة للاستعمال تصغر؛ وذلك بسبب الحاجة لأقراص أكثر لتخزين الكمية نفسها من البيانات."

msgid "Linear RAID"
msgstr "‏Linear RAID"

#, fuzzy
#| msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That's about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That is about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgstr "مع أن نظام RAID الفرعي في النواة يدعم إنشاء ”Linear RAID“، إلا أن هذا النوع ليس RAID أصلاً، إذا أن هذا الإعداد ليس فيه أي فائض. كل ما يحدث هو أن النواة تجمع عدة أقراص مع بعضها بأسلوب end-to-end (نهاية القرص الأول مع بداية الثاني وهكذا) وتقدم مجموع الحجم التخزيني بشكل قرص ظاهري واحد (one block device). هذه هي وظيفته كلها. نادرًا ما يستخدم هذا النمط وحده (اقرأ الفقرات التالية لتتعرف على الحالات الاستثنائية)، خصوصًا أن افتقاره للفائض يعني أن تعطل أحد الأقراص سيودي بالمجموع التخزيني كله، مع بياناته."

msgid "RAID-0"
msgstr "‏RAID-0"

msgid "This level doesn't provide any redundancy either, but disks aren't simply stuck on end one after another: they are divided in <emphasis>stripes</emphasis>, and the blocks on the virtual device are stored on stripes on alternating physical disks. In a two-disk RAID-0 setup, for instance, even-numbered blocks of the virtual device will be stored on the first physical disk, while odd-numbered blocks will end up on the second physical disk."
msgstr "لا يقدم هذا المستوى أية فائض أيضًا، لكن الأقراص لا تتقاطر خلف بعضها بشكل بسيط: بل تقسم إلى شرائط <emphasis>stripes</emphasis>، ويتم تخزين أجزاء القرص الظاهري على الشرائط بشكل متناوب بين الأقراص الفيزيائية. في نظام RAID-0 ذو قرصين، مثلًا، تُخَزَّن الأجزاء الزوجية من القرص الظاهري على القرص الفيزيائي الأول، والأجزاء الفردية على القرص الفيزيائي الثاني."

#, fuzzy
#| msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. However, RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. The disks are utilized entirely by the RAID device, so they should have the same size not to lose performance."
msgstr "لا يسعى هذا النظام لزيادة الوثوقية، نظرًا لأن كافة البيانات ستضيع إذا فشل أحد الأقراص (كما في حالة Linear RAID)، لكنه يهدف لرفع الأداء: سوف تتمكن النواة أثناء الوصول التسلسلي لكميات كبيرة من البيانات المستمرة من القراءة من القرصين معًا (أو الكتابة عليهما معًا) على التوازي، وهو ما يزيد مستوى نقل البيانات. على أية حال، فإن استخدام RAID-0 في تناقص، بعد أن احتلّ LVM مكانه في تحقيق هذه الميزة (انظر لاحقاً)."

msgid "RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgstr ""

msgid "RAID-1"
msgstr "‏RAID-1"

msgid "This level, also known as “RAID mirroring”, is both the simplest and the most widely used setup. In its standard form, it uses two physical disks of the same size, and provides a logical volume of the same size again. Data are stored identically on both disks, hence the “mirror” nickname. When one disk fails, the data is still available on the other. For really critical data, RAID-1 can of course be set up on more than two disks, with a direct impact on the ratio of hardware cost versus available payload space."
msgstr "يعرف هذا المستوى أيضًا باسم ”RAID mirroring“، وهو الأبسط والأكثر انتشاراً. يعتمد هذا المستوى –في شكله المعياري– على قرصين فيزيائيين لهما السعة ذاتها، ويعطي قرصًا منطقيًا له نفس السعة أيضًا. تخزن البيانات نفسها على القرصين، ولذلك كان ”mirror“ هو الاسم الثاني لهذا المستوى. إذا تعطّل أحد القرصين، تبقى البيانات متوفرة على الآخر. يمكن طبعاً إعداد RAID-1 على أكثر من قرصين بالنسبة للبيانات الهامة جدًا، لكن هذا سيزيد نسبة الكلفة للمساحة التخزينية."

msgid "<emphasis>NOTE</emphasis> Disks and cluster sizes"
msgstr "<emphasis>ملاحظة</emphasis> سعة الأقراص وسعة العنقود"

msgid "If two disks of different sizes are set up in a mirror, the bigger one will not be fully used, since it will contain the same data as the smallest one and nothing more. The useful available space provided by a RAID-1 volume therefore matches the size of the smallest disk in the array. This still holds for RAID volumes with a higher RAID level, even though redundancy is stored differently."
msgstr "إذا تم إعداد قرصين من سعتين مختلفتين في مرآة RAID-1، لن يستخدم القرص الأكبر بشكل كامل، لأنه سيحوي نفس البيانات التي يحويها القرص الأصغر فقط. أي أن المساحة المتوفرة للاستخدام في قرص RAID-1 الناتج ستطابق سعة أصغر قرص في المصفوفة. هذا القانون ينطبق على مستويات RAID اللاحقة أيضًا، رغم أن الفائض مخزن بأسلوب مختلف."

msgid "It is therefore important, when setting up RAID arrays (except for RAID-0 and “linear RAID”), to only assemble disks of identical, or very close, sizes, to avoid wasting resources."
msgstr "لذلك كان مهماً أن تجمع الأقراص ذات السعات المتساوية أو المتقاربة جدًا عند إعداد مصفوفات RAID (ما عدا RAID-0 و Linear RAID)، حتى تتجنب الهدر في الموارد."

msgid "<emphasis>NOTE</emphasis> Spare disks"
msgstr "<emphasis>ملاحظة</emphasis> الأقراص الاحتياطية"

msgid "RAID levels that include redundancy allow assigning more disks than required to an array. The extra disks are used as spares when one of the main disks fails. For instance, in a mirror of two disks plus one spare, if one of the first two disks fails, the kernel will automatically (and immediately) reconstruct the mirror using the spare disk, so that redundancy stays assured after the reconstruction time. This can be used as another kind of safeguard for critical data."
msgstr "يمكن إضافة أقراص أكثر مما هو مطلوب للمصفوفة في مستويات RAID التي تحتوي على فائض. يمكن استخدام الأقراص الإضافية كبديل عندما يتعطّل أحد الأقراص الرئيسية. مثلًا، في حالة تطبيق مرآة بقرصين مع قرص احتياطي واحد، سوف تعيد النواة بناء المرآة تلقائيًا (وفوريًا) باستخدام القرص الاحتياطي إذا تعطّل أحد القرصين الرئيسيين. يمكن اعتماد هذا الأسلوب كخط أمان إضافي للبيانات الحساسة."

msgid "One would be forgiven for wondering how this is better than simply mirroring on three disks to start with. The advantage of the “spare disk” configuration is that the spare disk can be shared across several RAID volumes. For instance, one can have three mirrored volumes, with redundancy assured even in the event of one disk failure, with only seven disks (three pairs, plus one shared spare), instead of the nine disks that would be required by three triplets."
msgstr "قد يتساءل المرء عن سبب تفضيل هذا الأسلوب على إعداد مرآة بثلاثة أقراص ببساطة. إن ميزة إعداد ”القرص الاحتياطي“ هي إمكانية مشاركة القرص الاحتياطي بين عدة مصفوفات RAID. يمكن مثلًا، إعداد ثلاثة مصفوفات RAID-1، مع ضمان حماية الفائض حتى في حال تعطل أحد الأقراص باستخدام سبعة أقراص فقط (ثلاثة أزواج واحتياطي واحد)، بدلاً من تسعة أقراص كنا سنحتاجها لإعداد ثلاثة مرايا ثلاثية."

msgid "This RAID level, although expensive (since only half of the physical storage space, at best, is useful), is widely used in practice. It is simple to understand, and it allows very simple backups: since both disks have identical contents, one of them can be temporarily extracted with no impact on the working system. Read performance is often increased since the kernel can read half of the data on each disk in parallel, while write performance isn't too severely degraded. In case of a RAID-1 array of N disks, the data stays available even with N-1 disk failures."
msgstr "بالرغم من ارتفاع كلفة هذا المستوى (نظراً لأن المساحة التخزينية المتاحة تساوي نصف المساحة الفيزيائية في أحسن الأحوال)، إلا أنه استخدامه منتشر عملياً. فهم هذا المستوى بسيط، وهو يؤدي عملية نسخ احتياطي بسيطة جدًا: بما أن القرصين يخزنان المحتوى نفسه، يمكن فصل أحدهما مؤقتًا دون التأثير على عمل النظام. غالبًا ما يكون أداء الأقراص عند القراءة مرتفعاً، لأن النواة تستطيع قراءة نصف البيانات من كل قرص على التوازي، في حين لا ينخفض الأداء كثيراً عند الكتابة. تبقى البيانات متاحة في مصفوفة RAID-1 ذات N قرص، حتى في حال تعطل N-1 قرص."

msgid "RAID-4"
msgstr "‏RAID-4"

msgid "This RAID level, not widely used, uses N disks to store useful data, and an extra disk to store redundancy information. If that disk fails, the system can reconstruct its contents from the other N. If one of the N data disks fails, the remaining N-1 combined with the “parity” disk contain enough information to reconstruct the required data."
msgstr "هذا المستوى من RAID غير منتشر كثيراً. يستخدم هذا المستوى N قرص لتخزين البيانات المفيدة، وقرص إضافي لتخزين معلومات فائضة. إذا تعطل القرص الإضافي، يستطيع النظام إعادة بناء محتوياته اعتمادًا على الأقراص الأخرى. أما إذا تعطل أحد أقراص المعلومات فيستخدم النظام الأقراص المتبقية منها (N-1 قرص) مع القرص الإضافي (قرص الازدواجية – ‎“parity” disk) لإعادة بناء البيانات المفقودة."

msgid "RAID-4 isn't too expensive since it only involves a one-in-N increase in costs and has no noticeable impact on read performance, but writes are slowed down. Furthermore, since a write to any of the N disks also involves a write to the parity disk, the latter sees many more writes than the former, and its lifespan can shorten dramatically as a consequence. Data on a RAID-4 array is safe only up to one failed disk (of the N+1)."
msgstr "إن كلفة RAID-4 ليست مرتفعة جداً بما أن الزيادة في الكلفة هي 1 إلى N كما أنه تأثيره على سرعة القراءة غير ملحوظ، لكن أداء الكتابة ينخفض. من ناحية أخرى، عند كل عملية كتابة على أحد أقراص المعلومات يجب الكتابة على قرص الازدواجية أيضًا، ما قد يؤدي لتقصير عمره بشكل كبير. تبقى البيانات في مصفوفة RAID-4 بأمان في حال عطب قرص واحد (من المصفوفة كلها ذات N+1 قرص)."

msgid "RAID-5"
msgstr "‏RAID-5"

msgid "RAID-5 addresses the asymmetry issue of RAID-4: parity blocks are spread over all of the N+1 disks, with no single disk having a particular role."
msgstr "يعالج المستوى RAID-5 مشكلة اللاتناظر التي يعاني منها RAID-4: حيث تنتشر معلومات الازدواجية على جميع الأقراص في مصفوفة N+1، ولا يوجد دور محدد لأي قرص منها."

msgid "Read and write performance are identical to RAID-4. Here again, the system stays functional with up to one failed disk (of the N+1), but no more."
msgstr "أداء القراءة والكتابة مطابق لأداء RAID-4. كما أن النظام هنا أيضًا يتحمل تعطل قرص واحد فقط (من أصل N+1 قرص)."

msgid "RAID-6"
msgstr "‏RAID-6"

msgid "RAID-6 can be considered an extension of RAID-5, where each series of N blocks involves two redundancy blocks, and each such series of N+2 blocks is spread over N+2 disks."
msgstr "يمكن اعتبار RAID-6 كامتداد للمستوى RAID-5، إذ أن كل سلسلة مؤلفة من N كتلة تحتاج إلى كتلتين فائضتين، وكل سلسلة من N+2 كتلة تنتشر على N+2 قرص."

msgid "This RAID level is slightly more expensive than the previous two, but it brings some extra safety since up to two drives (of the N+2) can fail without compromising data availability. The counterpart is that write operations now involve writing one data block and two redundancy blocks, which makes them even slower."
msgstr "كلفة هذا المستوى أعلى بقليل من المستويين السابقين، لكنه يزيد مستوى الأمان إذا يستطيع العمل حتى لو تعطل قرصين (من أصل N+2) دون تأثر البيانات. الجانب السلبي هو أن عمليات الكتابة على الأقراص تحتاج لكتابة كتلة بيانات واحدة وكتلتين فائضتين، وهذا يجعل الكتابة أبطأ."

msgid "RAID-1+0"
msgstr "‏RAID-1+0"

#, fuzzy
#| msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there's no problem with that."
msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there is no problem with that."
msgstr "للأمانة العلمية هذا ليس مستوى RAID، لكنه تركيب لمستويين وراء بعضهما. إذا كان لدينا N‏×2 قرص، يمكننا أن نجمع كل زوج منها للحصول على N قرص من مستوى RAID-1؛ ثم نجمع هذه الأقراص في قرص واحد إما باستخدام ”linear RAID“ أو عبر LVM. إذا استخدمنا LVM فإننا نتجاوز حدود RAID، لكن هذه ليست مشكلة في الواقع."

msgid "RAID-1+0 can survive multiple disk failures: up to N in the 2×N array described above, provided that at least one disk keeps working in each of the RAID-1 pairs."
msgstr "تتحمل مصفوفات RAID-1+0 تعطل عدة أقراص: فالمصفوفة الموضحة سابقاً يمكن أن تتحمل تعطل N قرص إذا كانت تحوي ‎2×‎N قرص، بشرط أن ينجو قرص واحد على الأقل من كل زوج من أقراص RAID-1."

msgid "<emphasis>GOING FURTHER</emphasis> RAID-10"
msgstr "<emphasis>التعمق أكثر</emphasis> RAID-10"

msgid "RAID-10 is generally considered a synonym of RAID-1+0, but a Linux specificity makes it actually a generalization. This setup allows a system where each block is stored on two different disks, even with an odd number of disks, the copies being spread out along a configurable model."
msgstr "يعتبر RAID-10 كمرادف للمستوى RAID-1+0 عموماً، لكن هناك خاصية في لينكس تجعل الثاني حالة خاصة من الأول. يسمح هذا الإعداد ببناء نظام تُخزَّن فيه كل كتلة على قرصين مختلفين، حتى لو كان عدد الأقراص في النظام فردياً، ويتبع توزيع النسخ على الأقراص نموذجاً محدداً يمكن تعديله."

msgid "Performances will vary depending on the chosen repartition model and redundancy level, and of the workload of the logical volume."
msgstr "سيختلف مستوى الأداء تبعاً لنموذج التقسيم المتبع ومستوى الفائض، وحمل الحيز التخزيني المنطقي."

msgid "Obviously, the RAID level will be chosen according to the constraints and requirements of each application. Note that a single computer can have several distinct RAID arrays with different configurations."
msgstr "من الواضح أن اختيار مستوى RAID الملائم يعتمد على متطلبات وقيود كل تطبيق. لاحظ أن الحاسوب الواحد يمكن أن يحوي عدة مصفوفات RAID ذات مستويات مختلفة."

msgid "Setting up RAID"
msgstr "إعداد RAID"

msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Setting up RAID volumes requires the <emphasis role=\"pkg\">mdadm</emphasis> package; it provides the <command>mdadm</command> command, which allows creating and manipulating RAID arrays, as well as scripts and tools integrating it to the rest of the system, including the monitoring system."
msgstr "يحتاج إعداد RAID لحزمة <emphasis role=\"pkg\">mdadm</emphasis>؛ التي توفر الأمر <command>mdadm</command> الذي يستخدم لإنشاء وتعديل مصفوفات RAID، كما توفر أيضًا سكربتات وأدوات تدمج البرنامج في أجزاء نظام التشغيل الأخرى، بما فيه نظام المراقبة."

msgid "Our example will be a server with a number of disks, some of which are already used, the rest being available to setup RAID. We initially have the following disks and partitions:"
msgstr "مثالنا هو مُخدِّم فيه عدد من الأقراص، بعضها مستخدم، والباقي متاح لإعداد مصفوفة RAID. هذه هي الحالة الإبتدائية للأقراص والأقسام:"

msgid "the <filename>sdb</filename> disk, 4 GB, is entirely available;"
msgstr "القرص <filename>sdb</filename>، ‏4 غ.ب، متاح بالكامل؛"

msgid "the <filename>sdc</filename> disk, 4 GB, is also entirely available;"
msgstr "القرص <filename>sdc</filename>، ‏4 غ.ب، متاح بالكامل أيضاً؛"

msgid "on the <filename>sdd</filename> disk, only partition <filename>sdd2</filename> (about 4 GB) is available;"
msgstr "القسم <filename>sdd2</filename> من القرص <filename>sdd</filename> متاح (حوالي 4 غ.ب)؛"

msgid "finally, a <filename>sde</filename> disk, still 4 GB, entirely available."
msgstr "أخيراً، القرص <filename>sde</filename>، أيضاً 4 غ.ب متاح بالكامل."

msgid "<emphasis>NOTE</emphasis> Identifying existing RAID volumes"
msgstr "<emphasis>ملاحظة</emphasis> التعرف على أقراص RAID القديمة"

msgid "The <filename>/proc/mdstat</filename> file lists existing volumes and their states. When creating a new RAID volume, care should be taken not to name it the same as an existing volume."
msgstr "يسرد الملف <filename dir=\"ltr\">/proc/mdstat</filename> جميع أقراص RAID السابقة وحالاتها. يجب أن تنتبه إلى عدم استخدام اسم قرص مستخدم مسبقًا عند إنشاء قرص جديد."

msgid "We're going to use these physical elements to build two volumes, one RAID-0 and one mirror (RAID-1). Let's start with the RAID-0 volume:"
msgstr "سوف نستخدم هذه العناصر الفيزيائية لبناء حيزين تخزينيين، أحدهما RAID-0، والآخر RAID-1 (مرآة). دعنا نبدأ ببناء حيز RAID-0:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md0 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0:\n"
#| "        Version : 1.2\n"
#| "  Creation Time : Wed May  6 09:24:34 2015\n"
#| "     Raid Level : raid0\n"
#| "     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 2\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:24:34 2015\n"
#| "          State : clean \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 0\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "     Chunk Size : 512K\n"
#| "\n"
#| "           Name : mirwiz:0  (local to host mirwiz)\n"
#| "           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
#| "         Events : 0\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       16        0      active sync   /dev/sdb\n"
#| "       1       8       32        1      active sync   /dev/sdc\n"
#| "# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
#| "<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
#| "Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
#| "Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
#| "Superblock backups stored on blocks: \n"
#| "        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
#| "\n"
#| "Allocating group tables: done                            \n"
#| "Writing inode tables: done                            \n"
#| "Creating journal (32768 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
#| "<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
#| "/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"           Version : 1.2\n"
"     Creation Time : Tue Jun 25 08:47:49 2019\n"
"        Raid Level : raid0\n"
"        Array Size : 8378368 (7.99 GiB 8.58 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 08:47:49 2019\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"        Chunk Size : 512K\n"
"\n"
"Consistency Policy : none\n"
"\n"
"              Name : mirwiz:0  (local to host debian)\n"
"              UUID : 146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"            Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       32        0      active sync   /dev/sdb\n"
"       1       8       48        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 2094592 4k blocks and 524288 inodes\n"
"Filesystem UUID: 413c3dff-ab5e-44e7-ad34-cf1a029cfe98\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.9G   36M  7.4G   1% /srv/raid-0\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:24:34 2015\n"
"     Raid Level : raid0\n"
"     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:24:34 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"     Chunk Size : 512K\n"
"\n"
"           Name : mirwiz:0  (local to host mirwiz)\n"
"           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       16        0      active sync   /dev/sdb\n"
"       1       8       32        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
"Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
"Superblock backups stored on blocks: \n"
"        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0\n"
"</computeroutput>"

#, fuzzy
#| msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It's also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It is also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgstr "يحتاج الأمر <command>mdadm --create</command> عدة متغيرات: اسم الحيز الذي سيتم إنشاؤه (<filename dir=\"ltr\">/dev/md*</filename>، حيث ترمز md إلى <foreignphrase>Multiple Device</foreignphrase>―”أجهزة متعددة“)، ومستوى RAID، وعدد الأقراص (هذا المتغير إلزامي رغم أنه لا يفيد إلا مع مستويات RAID-1 وما فوق)، والأجهزة الفيزيائية التي ستستخدم. بعد إنشاء الحيز، يمكننا استخدامه كما نستخدم أي قسم عادي، فيمكن إنشاء نظام ملفات عليه، وربطه بشجرة الملفات، وغير ذلك. لاحظ أن إنشاء حيز RAID-0 على <filename>md0</filename> هو محض صدفة، وترقيم المصفوفة لا يشترط أن يتعلق بمستوى RAID المختار. كما يمكن إنشاء مصفوفات RAID بأسماء محددة، عبر إعطاء <command>mdadm</command> متغير مثل <filename dir=\"ltr\">/dev/md/linear</filename> بدلاً من <filename dir=\"ltr\">/dev/md0</filename>."

msgid "Creation of a RAID-1 follows a similar fashion, the differences only being noticeable after the creation:"
msgstr "يتم إنشاء RAID-1 بأسلوب مشابه، ولا تظهر الاختلافات إلا بعد عملية الإنشاء:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: Note: this array has metadata at the start and\n"
#| "    may not be suitable as a boot device.  If you plan to\n"
#| "    store '/boot' on this device please ensure that\n"
#| "    your boot-loader understands md/v1.x metadata, or use\n"
#| "    --metadata=0.90\n"
#| "mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
#| "Continue creating array? </computeroutput><userinput>y</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md1 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "        Version : 1.2\n"
#| "  Creation Time : Wed May  6 09:30:19 2015\n"
#| "     Raid Level : raid1\n"
#| "     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
#| "  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 2\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:30:40 2015\n"
#| "          State : clean, resyncing (PENDING) \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 0\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 0\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       1       8       64        1      active sync   /dev/sde\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "          State : clean\n"
#| "[...]\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Tue Jun 25 10:21:22 2019\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 10:22:03 2019\n"
"             State : clean, resyncing \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"     Resync Status : 93% complete\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 16\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       64        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array؟ </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:30:19 2015\n"
"     Raid Level : raid1\n"
"     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
"  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:30:40 2015\n"
"          State : clean, resyncing (PENDING) \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       1       8       64        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"

msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgstr "<emphasis>تلميح</emphasis> RAID والأقراص والأقسام"

msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions, and do not require full disks."
msgstr "كما هو واضح من المثال، يمكن بناء أجهزة RAID من أقسام الأقراص، ولا يشترط استخدام أقراص كاملة."

msgid "A few remarks are in order. First, <command>mdadm</command> notices that the physical elements have different sizes; since this implies that some space will be lost on the bigger element, a confirmation is required."
msgstr "هناك بضعة ملاحظات. أولاً، يلاحظ <command>mdadm</command> اختلاف سعة العناصر الفيزيائية؛ وبما أن هذا يعني ضياع بعض المساحة من العنصر الأكبر، يطلب من المستخدم تأكيد العملية."

msgid "More importantly, note the state of the mirror. The normal state of a RAID mirror is that both disks have exactly the same contents. However, nothing guarantees this is the case when the volume is first created. The RAID subsystem will therefore provide that guarantee itself, and there will be a synchronization phase as soon as the RAID device is created. After some time (the exact amount will depend on the actual size of the disks…), the RAID array switches to the “active” or “clean” state. Note that during this reconstruction phase, the mirror is in a degraded mode, and redundancy isn't assured. A disk failing during that risk window could lead to losing all the data. Large amounts of critical data, however, are rarely stored on a freshly created RAID array before its initial synchronization. Note that even in degraded mode, the <filename>/dev/md1</filename> is usable, and a filesystem can be created on it, as well as some data copied on it."
msgstr "الأهم من هذا هو حالة المرآة. لاحظ كيف كانت resyncing ثم انتقلت إلى active. إن الحالة الطبيعية لمرآة RAID هي أن تتطابق محتويات القرصين. لكن لا شيء يضمن هذا التطابق عند إنشاء المصفوفة أول مرة، ولذلك يعمل نظام RAID الفرعي على ضمان هذا بنفسه، ويبدأ طور مزامنة المحتويات بعد إنشاء المصفوفة مباشرة. بعد فترة من الزمن (تختلف المدة حسب حجم الأقراص الفعلي...)، تنتقل مصفوفة RAID إلى حالة ”active“ أو ”clean“. لاحظ أن المصفوفة تكون في الوضع degraded خلال طور إعادة البناء، وأن الفائض التخزيني غير جاهز بعد. إذا تعطل قرص أثناء مرحلة الخطر تلك، فسوف يؤدي ذلك إلى خسارة البيانات كلها. لكن نادرًا ما تستخدم مصفوفات RAID الجديدة لتخزين كميات كبيرة من البيانات الحساسة قبل أن تنتهي مرحلة تهيئتها الأولية. لاحظ أيضًا أن <filename dir=\"ltr\">/dev/md1</filename> جاهز للاستخدام حتى في وضع degraded، وأنه يمكن إنشاء نظام ملفات عليه، كما يمكن نسخ البيانات إليه أيضًا."

msgid "<emphasis>TIP</emphasis> Starting a mirror in degraded mode"
msgstr "<emphasis>تلميح</emphasis> إنشاء مرآة في وضع degraded"

msgid "Sometimes two disks are not immediately available when one wants to start a RAID-1 mirror, for instance because one of the disks one plans to include is already used to store the data one wants to move to the array. In such circumstances, it is possible to deliberately create a degraded RAID-1 array by passing <filename>missing</filename> instead of a device file as one of the arguments to <command>mdadm</command>. Once the data have been copied to the “mirror”, the old disk can be added to the array. A synchronization will then take place, giving us the redundancy that was wanted in the first place."
msgstr "أحيانًا لا يكون القرصان جاهزين فورًا لحظة إنشاء مرآة RAID-1، مثلاً يمكن أن أحد القرصين الذين نريد استخدامهما مستخدم أصلاً لتخزين البيانات التي نريد نقلها إلى المصفوفة. في مثل هذه الحالات، من الممكن إنشاء مصفوفة RAID-1 في الوضع degraded باستخدام قرص واحد من خلال تمرير <filename>missing</filename> كمعامل للأمر <command>mdadm</command> بدلاً من تمرير اسم الملف الذي يمثل القرص. بعد نسخ البيانات إلى ”المرآة“، يمكن إضافة القرص القديم إلى المصفوفة. عندها تبدأ عملية المزامنة، للوصول إلى الحالة الآمنة التي أردناها في البداية."

msgid "<emphasis>TIP</emphasis> Setting up a mirror without synchronization"
msgstr "<emphasis>تلميح</emphasis> إعداد مرآة بدون مزامنة"

msgid "RAID-1 volumes are often created to be used as a new disk, often considered blank. The actual initial contents of the disk is therefore not very relevant, since one only needs to know that the data written after the creation of the volume, in particular the filesystem, can be accessed later."
msgstr "تستخدم مصفوفات RAID-1 بعد إنشائها غالبًا كأقراص جديدة، وتعامل على أنها فارغة. أي أن المحتويات الأولية للقرص عديمة القيمة، لأن كل ما نحتاجه هو أن نتأكد أننا سوف نستطيع لاحقاً الوصول البيانات التي سنكتبها بعد إنشاء الحيز التخزيني الجديد، خصوصاً نظام الملفات."

msgid "One might therefore wonder about the point of synchronizing both disks at creation time. Why care whether the contents are identical on zones of the volume that we know will only be read after we have written to them?"
msgstr "قد يتساءل المرء عندئذ عن فائدة مزامنة الأقراص عند إنشائها. ما الفرق إذا كانت محتويات المصفوفة متزامنة إذا كنا لن نقرأ من المصفوفة شيئًا قبل محوها وتهيئتها؟"

msgid "Fortunately, this synchronization phase can be avoided by passing the <literal>--assume-clean</literal> option to <command>mdadm</command>. However, this option can lead to surprises in cases where the initial data will be read (for instance if a filesystem is already present on the physical disks), which is why it isn't enabled by default."
msgstr "لحسن الحظ، يمكن تفادي طور المزامنة هذا بتمرير الخيار‎ <literal dir=\"ltr\">--assume-clean</literal> للأمر <command>mdadm</command>. لكن هذا الخيار قد يسبب مفاجآت لو حاولنا قراءة البيانات الأولية (مثلاً إذا كانت الأقراص الفيزيائية تحوي نظام ملفات مسبقًا)، لذلك فإن هذا الخيار معطل افتراضيًا."

msgid "Now let's see what happens when one of the elements of the RAID-1 array fails. <command>mdadm</command>, in particular its <literal>--fail</literal> option, allows simulating such a disk failure:"
msgstr "دعنا نرى ما سيحدث عندما يتعطل أحد عناصر مصفوفة RAID-1. يمكن محاكاة عطب قرص ما باستخدام الخيار <literal dir=\"ltr\">--fail</literal> مع الأمر <command>mdadm</command>:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Update Time : Wed May  6 09:39:39 2015\n"
#| "          State : clean, degraded \n"
#| " Active Devices : 1\n"
#| "Working Devices : 1\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 19\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       0        0        2      removed\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"       Update Time : Tue Jun 25 11:03:44 2019\n"
"             State : clean, degraded \n"
"    Active Devices : 1\n"
"   Working Devices : 1\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 20\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       -       0        0        0      removed\n"
"       1       8       80        1      active sync   /dev/sdd2\n"
"\n"
"       0       8       64        -      faulty   /dev/sde</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:39:39 2015\n"
"          State : clean, degraded \n"
" Active Devices : 1\n"
"Working Devices : 1\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       0        0        2      removed\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "The contents of the volume are still accessible (and, if it is mounted, the applications don't notice a thing), but the data safety isn't assured anymore: should the <filename>sdd</filename> disk fail in turn, the data would be lost. We want to avoid that risk, so we'll replace the failed disk with a new one, <filename>sdf</filename>:"
msgstr "تبقى محتويات المصفوفة متاحة (وإذا كانت مرتبطة بشجرة الملفات، فلن تشعر التطبيقات بشيء)، لكن البيانات لم تعد بأمان: فإذا تعطل القرص <filename>sdd</filename> أيضًا، سوف تضيع البيانات. نحن لا نريد أن نخاطر بذلك، ولهذا سوف نستبدل القرص المعطوب بقرص جديد، <filename>sdf</filename>:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
#| "<computeroutput>mdadm: added /dev/sdf\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 3\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:48:49 2015\n"
#| "          State : clean, degraded, recovering \n"
#| " Active Devices : 1\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 1\n"
#| "\n"
#| " Rebuild Status : 28% complete\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 26\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      spare rebuilding   /dev/sdf\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Update Time : Wed May  6 09:49:08 2015\n"
#| "          State : clean \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 41\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      active sync   /dev/sdf\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 11:09:42 2019\n"
"             State : clean, degraded, recovering \n"
"    Active Devices : 1\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 1\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 27% complete\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      spare rebuilding   /dev/sdf\n"
"       1       8       80        1      active sync   /dev/sdd2\n"
"\n"
"       0       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"       Update Time : Tue Jun 25 11:10:47 2019\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 39\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sdf\n"
"\n"
"       0       8       64        -      faulty   /dev/sde</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"   Raid Devices : 2\n"
"  Total Devices : 3\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:48:49 2015\n"
"          State : clean, degraded, recovering \n"
" Active Devices : 1\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 1\n"
"\n"
" Rebuild Status : 28% complete\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:49:08 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "Here again, the kernel automatically triggers a reconstruction phase during which the volume, although still accessible, is in a degraded mode. Once the reconstruction is over, the RAID array is back to a normal state. One can then tell the system that the <filename>sde</filename> disk is about to be removed from the array, so as to end up with a classical RAID mirror on two disks:"
msgstr "هنا أيضاً تبدأ النواة طور إعادة بناء تلقائيًا، وتبقى المصفوفة خلال هذا الطور في الوضع degraded أيضًا لكنها متاحة للوصول. ترجع مصفوفة RAID-1 إلى الحالة الطبيعية فور انتهاء إعادة البناء. يمكن عندها أن نخبر النظام أننا سوف نزيل القرص <filename>sde</filename> من المصفوفة، حتى تبقى كمرآة RAID كلاسيكية بقرصين فقط:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      active sync   /dev/sdf</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sdf</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf</computeroutput>\n"

msgid "From then on, the drive can be physically removed when the server is next switched off, or even hot-removed when the hardware configuration allows hot-swap. Such configurations include some SCSI controllers, most SATA disks, and external drives operating on USB or Firewire."
msgstr "عند هذه اللحظة يمكن فصل القرص الفيزيائي عند إيقاف تشغيل المخدم، أو يمكن حتى فصلها مباشرة إذا كان العتاد يسمح بالتبديل الساخن hot-swap. تسمح بعض متحكمات SCSI، ومعظم أقراص SATA، والسواقات الخارجية التي تعمل عبر USB أو Firewire بهذا النوع من التبديل."

msgid "Backing up the Configuration"
msgstr "النسخ الاحتياطي للإعدادات"

msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgstr "تُحفَظ معظم البيانات الفوقية (meta-data) الخاصة بمصفوفات RAID مباشرة على الأقراص التي تنتمي لهذه المصفوفات، حتى تتعرف النواة على المصفوفات ومكوناتها وتجمعها آليًا عند إقلاع النظام. لكن الأفضل أخذ نسخة احتياطية عن هذه البيانات، لأن عملية التعرف هذه قد تفشل، ومن المتوقع ألا تفشل هذه العملية إلا في الظروف الحساسة. فلو كان عطل القرص <filename>sde</filename> في مثالنا حقيقيًا (وليس ظاهريًا كما فعلنا) ثم أعيد تشغيل النظام دون إزالة هذا القرص <filename>sde</filename>، فقد يعود هذا القرص إلى العمل ثانية نتيجة عملية الاستكشاف أثناء إعادة الإقلاع. سوف تصطدم النواة إذًا بثلاثة أقراص فيزيائية، كلٌّ منها يدعي أنه يحوي نصف الحيز التخزيني المقابل للمصفوفة نفسها. أو يمكن أن يحدث التباس عند دمج مصفوفات RAID من مخدمين إلى مخدم واحد فقط. إذا كانت هذه المصفوفات تعمل بشكل صحيح قبل نقل الأقراص، سوف تتمكن النواة من التعرف على الأزواج وجمعها بشكل صحيح؛ لكن إذا كانت الأقراص على المخدم القديم مجموعة مع بعضها في مصفوفة اسمها <filename>md1</filename>، وكان المخدم الجديد يحوي <filename>md1</filename> أيضًا، فسوف تعاد تسمية إحدى المرآتين."

msgid "Backing up the configuration is therefore important, if only for reference. The standard way to do it is by editing the <filename>/etc/mdadm/mdadm.conf</filename> file, an example of which is listed here:"
msgstr "إذاً لا بد من أخذ نسخة احتياطية عن الإعدادات، حتى لو كانت للاستئناس فقط. الطريقة المعيارية لعمل هذا هي تحرير الملف <filename dir=\"ltr\">/etc/mdadm/mdadm.conf</filename>، إليك مثالاً عن هذا الملف:"

msgid "<command>mdadm</command> configuration file"
msgstr "ملف إعداد <command>mdadm</command>"

#, fuzzy
#| msgid ""
#| "# mdadm.conf\n"
#| "#\n"
#| "# Please refer to mdadm.conf(5) for information about this file.\n"
#| "#\n"
#| "\n"
#| "# by default (built-in), scan all partitions (/proc/partitions) and all\n"
#| "# containers for MD superblocks. alternatively, specify devices to scan, using\n"
#| "# wildcards if desired.\n"
#| "DEVICE /dev/sd*\n"
#| "\n"
#| "# auto-create devices with Debian standard permissions\n"
#| "CREATE owner=root group=disk mode=0660 auto=yes\n"
#| "\n"
#| "# automatically tag new arrays as belonging to the local system\n"
#| "HOMEHOST &lt;system&gt;\n"
#| "\n"
#| "# instruct the monitoring daemon where to send mail alerts\n"
#| "MAILADDR root\n"
#| "\n"
#| "# definitions of existing MD arrays\n"
#| "ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "\n"
#| "# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100\n"
#| "# by mkconf 3.2.5-3"
msgid ""
"# mdadm.conf\n"
"#\n"
"# !NB! Run update-initramfs -u after updating this file.\n"
"# !NB! This will ensure that initramfs has an uptodate copy.\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# auto-create devices with Debian standard permissions\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c\n"
"\n"
"# This configuration was auto-generated on Tue, 25 Jun 2019 07:54:35 -0400 by mkconf"
msgstr ""
"# mdadm.conf\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# auto-create devices with Debian standard permissions\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
"\n"
"# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100\n"
"# by mkconf 3.2.5-3\n"

msgid "One of the most useful details is the <literal>DEVICE</literal> option, which lists the devices where the system will automatically look for components of RAID volumes at start-up time. In our example, we replaced the default value, <literal>partitions containers</literal>, with an explicit list of device files, since we chose to use entire disks and not only partitions, for some volumes."
msgstr "أحد أهم التفاصيل هو خيار <literal>DEVICE</literal>، الذي يعدد الأجهزة التي يفحصها النظام بحثًا عن مكونات مصفوفات RAID عند الإقلاع. لقد استبدلنا في مثالنا القيمة الافتراضية – <literal>partitions containers</literal> – بلائحة واضحة تسرد أسماء ملفات الأجهزة، ذلك لأننا اخترنا استخدام بعض الأقراص الكاملة وليس الأقسام فقط."

msgid "The last two lines in our example are those allowing the kernel to safely pick which volume number to assign to which array. The metadata stored on the disks themselves are enough to re-assemble the volumes, but not to determine the volume number (and the matching <filename>/dev/md*</filename> device name)."
msgstr "آخر سطرين في مثالنا يسمحان للنواة بإسناد رقم الحيز المناسب إلى المصفوفة المناسبة. إن البيانات الفوقية المخزنة على الأقراص نفسها تكفي لإعادة جمع المصفوفات، لكنها لا تكفي لمعرفة رقم الحيز (ولا معرفة اسم <filename dir=\"ltr\">/dev/md*</filename> الموافق للجهاز)."

msgid "Fortunately, these lines can be generated automatically:"
msgstr "لحسن الحظ، يمكن توليد هذه الأسطر آليًا:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
#| "<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>\n"

msgid "The contents of these last two lines doesn't depend on the list of disks included in the volume. It is therefore not necessary to regenerate these lines when replacing a failed disk with a new one. On the other hand, care must be taken to update the file when creating or deleting a RAID array."
msgstr "لا تعتمد محتويات هذه السطور على الأقراص المتضمنة في المصفوفة. فلا حاجة إلى إعادة توليدها عند استبدال قرص معطوب بآخر جديد. لكن يجب الانتباه إلى تحديث الملف عند إنشاء مصفوفة RAID جديدة أو حذف واحدة قديمة."

msgid "<primary>LVM</primary>"
msgstr "<primary>LVM</primary>"

msgid "<primary>Logical Volume Manager</primary>"
msgstr "<primary>Logical Volume Manager</primary>"

msgid "LVM, the <emphasis>Logical Volume Manager</emphasis>, is another approach to abstracting logical volumes from their physical supports, which focuses on increasing flexibility rather than increasing reliability. LVM allows changing a logical volume transparently as far as the applications are concerned; for instance, it is possible to add new disks, migrate the data to them, and remove the old disks, without unmounting the volume."
msgstr "<emphasis>Logical Volume Manager</emphasis> ًأو اختصارا LVM هو أسلوب آخر لعزل الأقراص التخزينية المنطقية عن الأقراص الفيزيائية، وهو يركز على زيادة المرونة بدلاً من زيادة الوثوقية. يسمح LVM بتغيير القرص المنطقي بشكل شفاف بالنسبة للتطبيقات؛ فمثلاً، يمكن إضافة أقراص فيزيائية جديدة، ونقل البيانات إليها، وإزالة القديمة، دون فصل القرص المنطقي عن شجرة الملفات."

msgid "LVM Concepts"
msgstr "مفاهيم LVM"

msgid "This flexibility is attained by a level of abstraction involving three concepts."
msgstr "هذه المرونة نحرزها من خلال مستوى من العزل يشمل ثلاثة مفاهيم."

msgid "First, the PV (<emphasis>Physical Volume</emphasis>) is the entity closest to the hardware: it can be partitions on a disk, or a full disk, or even any other block device (including, for instance, a RAID array). Note that when a physical element is set up to be a PV for LVM, it should only be accessed via LVM, otherwise the system will get confused."
msgstr "الأول هو PV، أي <emphasis>Physical Volume</emphasis> (الحيز الفيزيائي) وهو أقرب وحدة إلى العتاد: يمكن أن يتألف من قسم من أحد الأقراص، أو قرص كامل، أو أي جهاز كتلي آخر (بما في ذلك مصفوفات RAID على سبيل المثال). لاحظ أنه عندما يتم إعداد عنصر فيزيائي ليشغل دور PV في LVM، فيجب التعامل معه من LVM فقط، وإلا فإن النظام سوف يضطرب."

#, fuzzy
#| msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there's no risk of using them directly."
msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there is no risk of using them directly."
msgstr "يمكن تجميع عدة PV ضمن VG ‏(<emphasis>Volume Group</emphasis>)، التي يمكن أن نعتبرها بمثابة أقراص ظاهرية قابلة للتوسعة. إن VGs مكونات مجردة، ولا تظهر بشكل ملفات أجهزة في فرع <filename dir=\"ltr\">/dev</filename>، لذلك لا يمكن استخدامها مباشرة."

msgid "The third kind of object is the LV (<emphasis>Logical Volume</emphasis>), which is a chunk of a VG; if we keep the VG-as-disk analogy, the LV compares to a partition. The LV appears as a block device with an entry in <filename>/dev</filename>, and it can be used as any other physical partition can be (most commonly, to host a filesystem or swap space)."
msgstr "النوع الثالث من المكونات هو LV‏ (<emphasis>Logical Volume</emphasis> – الحيز المنطقي)، وهو قطعة من VG؛ فإذا اعتبرنا VG بمثابة قرص، عندها يقابل LV القسم من القرص. يظهر LV كجهاز كتلي له مدخلة في <filename dir=\"ltr\">/dev</filename>، ويمكن استخدامه كما يستخدم أي قسم فيزيائي آخر (لاستضافة نظام ملفات أو مساحة swap عادة)."

msgid "The important thing is that the splitting of a VG into LVs is entirely independent of its physical components (the PVs). A VG with only a single physical component (a disk for instance) can be split into a dozen logical volumes; similarly, a VG can use several physical disks and appear as a single large logical volume. The only constraint, obviously, is that the total size allocated to LVs can't be bigger than the total capacity of the PVs in the volume group."
msgstr "أهم شيء هنا هو أن تقسيم VG إلى LVs مستقل تمامًا عن المكونات الفيزيائية للـ VG (وهي PVs). يمكن تقسيم VG يتألف من مكون فيزيائي واحد (قرص مثلاً) إلى دزينة من الأقراص المنطقية؛ كما يمكن أن يتألف VG من العديد من الأقراص الفيزيائية ثم يظهر كحيز منطقي كبير مفرد. القيد الوحيد طبعاً هو أن الحجم الكلي المتاح للتخزين على LVs لا يمكن أن يكون أكبر من السعة الكلية للحيزات الفيزيائية في الـVG."

msgid "It often makes sense, however, to have some kind of homogeneity among the physical components of a VG, and to split the VG into logical volumes that will have similar usage patterns. For instance, if the available hardware includes fast disks and slower disks, the fast ones could be clustered into one VG and the slower ones into another; chunks of the first one can then be assigned to applications requiring fast data access, while the second one will be kept for less demanding tasks."
msgstr "إلا أن المنطق يطلب شيئًا من التجانس بين المكونات الفيزيائية للـVG، وأن تقسم الـVG إلى حيزات منطقية لها استخدامات متشابهة. مثلاً، إذا كان العتاد المتوفر يحوي أقراصًا سريعة وأخرى بطيئة، فيمكن تجميع السريعة منها في VG واحدة والأقراص البطيئة في أخرى؛ يمكن تخصيص أجزاء من الأولى للتطبيقات التي تحتاج وصولاً سريعًا للبيانات، بينما تبقى الأخرى للمهام الأقل إلحاحاً."

msgid "In any case, keep in mind that an LV isn't particularly attached to any one PV. It is possible to influence where the data from an LV are physically stored, but this possibility isn't required for day-to-day use. On the contrary: when the set of physical components of a VG evolves, the physical storage locations corresponding to a particular LV can be migrated across disks (while staying within the PVs assigned to the VG, of course)."
msgstr "وعلى أية حال، تذكر أن LV لا يرتبط مباشرة بأي PV معيّن. من الممكن التأثير على موقع تخزين بيانات أحد الحيزات المنطقية فيزيائيًا، لكن هذه الإمكانية ليست جوهرية في الاستخدامات العادية. وعلى صعيد آخر: عندما تتطور المكونات الفيزيائية للـVG، يمكن تهجير مواقع التخزين الفيزيائية لأحد LVs بين الأقراص (مع البقاء ضمن PVs المخصصة للـVG بالطبع)."

msgid "Setting up LVM"
msgstr "إعداد LVM"

msgid "Let us now follow, step by step, the process of setting up LVM for a typical use case: we want to simplify a complex storage situation. Such a situation usually happens after some long and convoluted history of accumulated temporary measures. For the purposes of illustration, we'll consider a server where the storage needs have changed over time, ending up in a maze of available partitions split over several partially used disks. In more concrete terms, the following partitions are available:"
msgstr "دعنا الآن نتبع –خطوة بخطوة– طريقة إعداد LVM لحالة استخدام نموذجية: حيث نريد تبسيط حالة تخزينية معقدة. تحدث هذه الحالات عادة بعد تاريخ طويل ومعقد من تراكم التدابير المؤقتة. سوف ندرس كمثال حالة مخدم تغيرت فيه الحاجات التخزينية مع الزمن، وانتهى المطاف بمتاهة من الأقسام المتاحة الموزعة على عدد من الأقراص المستخدمة جزئيًا. بكلام واضح أكثر، الأقسام التالية هي المتاحة:"

msgid "on the <filename>sdb</filename> disk, a <filename>sdb2</filename> partition, 4 GB;"
msgstr "من القرص <filename>sdb</filename>، القسم <filename>sdb2</filename>، الحجم 4 غ.ب؛"

msgid "on the <filename>sdc</filename> disk, a <filename>sdc3</filename> partition, 3 GB;"
msgstr "من القرص <filename>sdc</filename>، القسم <filename>sdc3</filename>، الحجم 3 غ.ب؛"

msgid "the <filename>sdd</filename> disk, 4 GB, is fully available;"
msgstr "القرص <filename>sdd</filename>، متاح بالكامل، 4 غ.ب؛"

msgid "on the <filename>sdf</filename> disk, a <filename>sdf1</filename> partition, 4 GB; and a <filename>sdf2</filename> partition, 5 GB."
msgstr "من القرص <filename>sdf</filename>، القسم <filename>sdf1</filename>، ‏4 غ.ب؛ والقسم <filename>sdf2</filename>، ‏5 غ.ب."

msgid "In addition, let's assume that disks <filename>sdb</filename> and <filename>sdf</filename> are faster than the other two."
msgstr "بالإضافة لذلك، دعنا نفترض أن القرصين <filename>sdb</filename> و<filename>sdf</filename> أسرع من البقية."

msgid "Our goal is to set up three logical volumes for three different applications: a file server requiring 5 GB of storage space, a database (1 GB) and some space for back-ups (12 GB). The first two need good performance, but back-ups are less critical in terms of access speed. All these constraints prevent the use of partitions on their own; using LVM can abstract the physical size of the devices, so the only limit is the total available space."
msgstr "هدفنا هو إعداد ثلاثة حيزات منطقية لثلاثة تطبيقات: مخدم ملفات يحتاج 5 غ.ب. من المساحة التخزينية، وقاعدة بيانات (1 غ.ب) وبعض المساحة للنسخ الاحتياطية (12 غ.ب). يحتاج التطبيقان الأوليان أداء جيداً، بينما النسخ الاحتياطية أقل حرجاً من حيث الحاجة لسرعة النقل. تمنعنا كل هذه القيود من استخدام الأقسام المتاحة مباشرة كما هي؛ لكن يمكن أن يسمح استخدام LVM بعزل الحجم الفيزيائي للأجهزة، بحيث يبقى القيد الوحيد هو المساحة الكلية المتوفرة فقط."

msgid "The required tools are in the <emphasis role=\"pkg\">lvm2</emphasis> package and its dependencies. When they're installed, setting up LVM takes three steps, matching the three levels of concepts."
msgstr "الأدوات المطلوبة كلها في حزمة <emphasis role=\"pkg\">lvm2</emphasis> واعتمادياتها. بعد تثبيتها، يتطلب إعداد LVM ثلاث خطوات، تقابل المستويات الثلاث للمفاهيم."

msgid "First, we prepare the physical volumes using <command>pvcreate</command>:"
msgstr "أولاً، نجهز الحيزات الفيزيائية باستخدام <command>pvcreate</command>:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
#| "# </computeroutput><userinput>pvdisplay</userinput>\n"
#| "<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
#| "  --- NEW Physical volume ---\n"
#| "  PV Name               /dev/sdb2\n"
#| "  VG Name               \n"
#| "  PV Size               4.00 GiB\n"
#| "  Allocatable           NO\n"
#| "  PE Size               0   \n"
#| "  Total PE              0\n"
#| "  Free PE               0\n"
#| "  Allocated PE          0\n"
#| "  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
#| "\n"
#| "# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
#| "  Physical volume \"/dev/sdd\" successfully created\n"
#| "  Physical volume \"/dev/sdf1\" successfully created\n"
#| "  Physical volume \"/dev/sdf2\" successfully created\n"
#| "# </computeroutput><userinput>pvdisplay -C</userinput>\n"
#| "<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
#| "  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
#| "  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
#| "  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
#| "  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
#| "  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               z4Clgk-T5a4-C27o-1P0E-lIAF-OeUM-e7EMwq\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n"
"  Physical volume \"/dev/sdd\" successfully created.\n"
"  Physical volume \"/dev/sdf1\" successfully created.\n"
"  Physical volume \"/dev/sdf2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay -C</userinput><computeroutput>\n"
"  PV         VG Fmt  Attr PSize  PFree \n"
"  /dev/sdb2     lvm2 ---   4.00g  4.00g\n"
"  /dev/sdc3     lvm2 ---   3.00g  3.00g\n"
"  /dev/sdd      lvm2 ---   4.00g  4.00g\n"
"  /dev/sdf1     lvm2 ---   4.00g  4.00g\n"
"  /dev/sdf2     lvm2 ---  &lt;5.00g &lt;5.00g\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
"  Physical volume \"/dev/sdd\" successfully created\n"
"  Physical volume \"/dev/sdf1\" successfully created\n"
"  Physical volume \"/dev/sdf2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay -C</userinput>\n"
"<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
"  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
"  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
"  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
"</computeroutput>"

msgid "So far, so good; note that a PV can be set up on a full disk as well as on individual partitions of it. As shown above, the <command>pvdisplay</command> command lists the existing PVs, with two possible output formats."
msgstr "حتى الآن، كل شيء على ما يرام؛ لاحظ أنه يمكن إعداد PV على قرص كامل كما يمكن ذلك على أقسام الأقراص. يسرد الأمر <command>pvdisplay</command> الحيزات الفيزيائية الموجودة، وذلك في صيغتين مختلفتين للخرج، كما هو موضح أعلاه."

msgid "Now let's assemble these physical elements into VGs using <command>vgcreate</command>. We'll gather only PVs from the fast disks into a <filename>vg_critical</filename> VG; the other VG, <filename>vg_normal</filename>, will also include slower elements."
msgstr "دعنا الآن نجمع هذه العناصر الفيزيائية في VG باستخدام <command>vgcreate</command>. سوف نجمع الحيزات الفيزيائية من الأقراص السريعة فقط في مجموعة اسمها <filename>vg_critical</filename>؛ أما المجموعة الأخرى، <filename>vg_normal</filename>، فسوف تحوي عناصر سريعة وأخرى بطيئة."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
#| "<computeroutput>  No volume groups found\n"
#| "# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay</userinput>\n"
#| "<computeroutput>  --- Volume group ---\n"
#| "  VG Name               vg_critical\n"
#| "  System ID             \n"
#| "  Format                lvm2\n"
#| "  Metadata Areas        2\n"
#| "  Metadata Sequence No  1\n"
#| "  VG Access             read/write\n"
#| "  VG Status             resizable\n"
#| "  MAX LV                0\n"
#| "  Cur LV                0\n"
#| "  Open LV               0\n"
#| "  Max PV                0\n"
#| "  Cur PV                2\n"
#| "  Act PV                2\n"
#| "  VG Size               8.09 GiB\n"
#| "  PE Size               4.00 MiB\n"
#| "  Total PE              2071\n"
#| "  Alloc PE / Size       0 / 0   \n"
#| "  Free  PE / Size       2071 / 8.09 GiB\n"
#| "  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
#| "\n"
#| "# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
#| "<computeroutput>  Volume group \"vg_normal\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay -C</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
#| "  vg_critical   2   0   0 wz--n-  8.09g  8.09g\n"
#| "  vg_normal     3   0   0 wz--n- 12.30g 12.30g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               7.99 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2046\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2046 / 7.99 GiB\n"
"  VG UUID               wAbBjx-d82B-q7St-0KFf-z40h-w5Mh-uAXkNZ\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize   VFree  \n"
"  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n"
"  vg_normal     3   0   0 wz--n- &lt;11.99g &lt;11.99g\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  No volume groups found\n"
"# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               8.09 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2071\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2071 / 8.09 GiB\n"
"  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
"  vg_critical   2   0   0 wz--n-  8.09g  8.09g\n"
"  vg_normal     3   0   0 wz--n- 12.30g 12.30g\n"
"</computeroutput>\n"

msgid "Here again, commands are rather straightforward (and <command>vgdisplay</command> proposes two output formats). Note that it is quite possible to use two partitions of the same physical disk into two different VGs. Note also that we used a <filename>vg_</filename> prefix to name our VGs, but it is nothing more than a convention."
msgstr "الأوامر هنا أيضًا واضحة جداً ( كما أن <command>vgdisplay</command> يوفر صيغتين للخرج). لاحظ أنه من الممكن استخدام قسمين من القرص الفيزيائي نفسه في مجموعتين مختلفتين. لاحظ أيضًا أننا استخدمنا بادئة <filename dir=\"ltr\">vg_</filename> عند تسمية VGs التي أنشأناها ولكن هذا مجرد اصطلاح."

#, fuzzy
#| msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB, respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgstr "لدينا الآن ”قرصين ظاهريين“، أحجامهما تقريبًا 8 غ.ب و 12 غ.ب على التوالي. دعنا الآن نقطعهما إلى ”أقسم ظاهرية“ (LVs). نحتاج الأمر <command>lvcreate</command>، ونحتاج أيضًا تعليمة أكثر تعقيداً بقليل:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_files\" created\n"
#| "# </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput>  --- Logical volume ---\n"
#| "  LV Path                /dev/vg_critical/lv_files\n"
#| "  LV Name                lv_files\n"
#| "  VG Name                vg_critical\n"
#| "  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
#| "  LV Write Access        read/write\n"
#| "  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
#| "  LV Status              available\n"
#| "  # open                 0\n"
#| "  LV Size                5.00 GiB\n"
#| "  Current LE             1280\n"
#| "  Segments               2\n"
#| "  Allocation             inherit\n"
#| "  Read ahead sectors     auto\n"
#| "  - currently set to     256\n"
#| "  Block device           253:0\n"
#| "\n"
#| "# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_base\" created\n"
#| "# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_backups\" created\n"
#| "# </computeroutput><userinput>lvdisplay -C</userinput>\n"
#| "<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_base    vg_critical -wi-a---  1.00g                                           \n"
#| "  lv_files   vg_critical -wi-a---  5.00g                                           \n"
#| "  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created.\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                W6XT08-iBBx-Nrw2-f8F2-r2y4-Ltds-UrKogV\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time debian, 2019-11-30 22:45:46 -0500\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           254:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created.\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal</userinput>\n"
"<computeroutput>  Rounding up size to full physical extent 11.98 GiB\n"
"  Logical volume \"lv_backups\" created.\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 11.98g</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
"<computeroutput>  Logical volume \"lv_backups\" created\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"

msgid "Two parameters are required when creating logical volumes; they must be passed to the <command>lvcreate</command> as options. The name of the LV to be created is specified with the <literal>-n</literal> option, and its size is generally given using the <literal>-L</literal> option. We also need to tell the command what VG to operate on, of course, hence the last parameter on the command line."
msgstr "يوجد معاملان مطلوبان عند إنشاء الحيزات المنطقية؛ ويجب تمريرهما إلى الأمر <command>lvcreate</command> كخيارات. الأول هو اسم LV الذي سوف ننشئه ويحدد بالخيار <literal dir=\"ltr\">-n</literal>، والثاني هو حجم LV ويعطى عمومًا بالخيار <literal dir=\"ltr\">-L</literal>. نحتاج أيضًا إعلام الأمر باسم VG التي يطبق عليها طبعًا، وهذا هو المعامل الأخير في التعليمة."

msgid "<emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> options"
msgstr "<emphasis>التعمق أكثر</emphasis> خيارات <command>lvcreate</command>"

msgid "The <command>lvcreate</command> command has several options to allow tweaking how the LV is created."
msgstr "للأمر <command>lvcreate</command> العديد من الخيارات تسمح بضبط عملية إنشاء LV."

msgid "Let's first describe the <literal>-l</literal> option, with which the LV's size can be given as a number of blocks (as opposed to the “human” units we used above). These blocks (called PEs, <emphasis>physical extents</emphasis>, in LVM terms) are contiguous units of storage space in PVs, and they can't be split across LVs. When one wants to define storage space for an LV with some precision, for instance to use the full available space, the <literal>-l</literal> option will probably be preferred over <literal>-L</literal>."
msgstr "دعنا أولاً نشرح الخيار <literal dir=\"ltr\">-l</literal>، الذي يسمح بتحديد حجم الحيز المنطقي كعدد من الكتل (بدلاً من استخدام الواحدات ”البشرية“ كما فعلنا في المثال السابق). هذه الكتل (التي تدعى PEs، أي <emphasis>physical extents</emphasis>، بحسب مصطلحات LVM) هي وحدات متجاورة من المساحة التخزينية في الحيزات الفيزيائية، ولا يمكن أن تقسم الواحدة منها بين الحيزات المنطقية. عندما يحتاج المرء لتحديد السعة التخزينية للحيز المنطقي بدقة أكبر، مثلاً لاستخدام كامل المساحة المتوفرة، سيكون الخيار <literal dir=\"ltr\">-l</literal> مفضلاً على الخيار <literal dir=\"ltr\">-L</literal> غالبًا."

#, fuzzy
#| msgid "It's also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgid "It is also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgstr "من الممكن أيضاً الإشارة إلى الموقع الفيزيائي لتخزين LV، بحيث تخزن ”استطالاته“ (extents) على PV معين (مع البقاء ضمن الحيزات الفيزيائية المخصصة للـVG طبعاً). نظراً لأننا نعلم أن <filename>sdb</filename> أسرع من <filename>sdf</filename>، ربما نريد تخزين <filename>lv_base</filename> هناك إذا أردنا منح الأفضلية لمخدم قاعدة البيانات على مخدم الملفات. يصبح الأمر كالتالي: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. لاحظ أن هذا الأمر قد يفشل إذا لم يحو الحيز الفيزيائي عدداً كافياً من الاستطالات الحرة. في هذا المثال، لعلنا سنحتاج إلى إنشاء <filename>lv_base</filename> قبل <filename>lv_files</filename> لتفادي هذا الموقف ― أو إلى تحرير بعض المساحة على <filename>sdb2</filename> باستخدام الأمر <command>pvmove</command>."

msgid "Logical volumes, once created, end up as block device files in <filename>/dev/mapper/</filename>:"
msgstr "ينتهي المطاف بالحيزات المنطقية بعد إنشائها كملفات أجهزة كتلية في <filename dir=\"ltr\">/dev/mapper/</filename>:"

msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"

#, fuzzy
#| msgid "<emphasis>NOTE</emphasis> Autodetecting LVM volumes"
msgid "<emphasis>NOTE</emphasis> Auto-detecting LVM volumes"
msgstr "<emphasis>ملاحظة</emphasis> التعرف الآلي على حيزات LVM"

msgid "When the computer boots, the <filename>lvm2-activation</filename> systemd service unit executes <command>vgchange -aay</command> to “activate” the volume groups: it scans the available devices; those that have been initialized as physical volumes for LVM are registered into the LVM subsystem, those that belong to volume groups are assembled, and the relevant logical volumes are started and made available. There is therefore no need to edit configuration files when creating or modifying LVM volumes."
msgstr "عند إقلاع الحاسب، تنفذ وحدة الخدمة <filename>lvm2-activation</filename> التابعة لنظام systemd الأمر <command>vgchange -aay</command> ”لتنشيط activate“ مجموعات الحيزات: حيث يفحص الأجهزة المتوفرة؛ وتُسجَّل الأجهزة التي تمت تهيئتها كحيزات فيزيائية ضمن نظام LVM الفرعي، وتجمَع الحيزات التي تنتمي لمجموعات في مجموعاتها، ثم تنشط الحيزات المنطقية وتصبح متوفرة. لا حاجة إذاً لتحرير أي ملفات إعداد عند إنشاء أو تعديل حيزات LVM."

msgid "Note, however, that the layout of the LVM elements (physical and logical volumes, and volume groups) is backed up in <filename>/etc/lvm/backup</filename>, which can be useful in case of a problem (or just to sneak a peek under the hood)."
msgstr "لكن لاحظ أن خريطة عناصر LVM ( الحيزات الفيزيائية والمنطقية، والمجموعات) تُنسَخ احتياطيًا إلى <filename dir=\"ltr\">/etc/lvm/backup</filename>، وهذه قد تفيد في حال حدوث مشكلة (أو لاختلاس النظر تحت الغطاء)."

msgid "To make things easier, convenience symbolic links are also created in directories matching the VGs:"
msgstr "لتسهيل الأمور، يتم إنشاء اختصارات رمزيّة أيضًا في مجلدات بأسماء VGs نفسها:"

msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"

msgid "The LVs can then be used exactly like standard partitions:"
msgstr "يمكن استخدام LVs عندها مثل أي قسم نظامي تماماً:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
#| "<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
#| "Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
#| "Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
#| "[...]\n"
#| "Creating journal (32768 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
#| "<computeroutput>[...]\n"
#| "/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
#| "/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
#| "/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 3140608 4k blocks and 786432 inodes\n"
"Filesystem UUID: b9e6ed2f-cb37-43e9-87d8-e77568446225\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   41M   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
"Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
"[...]\n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"

msgid "From the applications' point of view, the myriad small partitions have now been abstracted into one large 12 GB volume, with a friendlier name."
msgstr "من وجهة نظر التطبيقات، تم تحويل الأقسام الصغيرة العديدة إلى حيز كبير واحد بحجم 12غ.ب، وله اسم ألطف."

msgid "LVM Over Time"
msgstr "‏LVM مع الزمن"

msgid "Even though the ability to aggregate partitions or physical disks is convenient, this is not the main advantage brought by LVM. The flexibility it brings is especially noticed as time passes, when needs evolve. In our example, let's assume that new large files must be stored, and that the LV dedicated to the file server is too small to contain them. Since we haven't used the whole space available in <filename>vg_critical</filename>, we can grow <filename>lv_files</filename>. For that purpose, we'll use the <command>lvresize</command> command, then <command>resize2fs</command> to adapt the filesystem accordingly:"
msgstr "بالرغم من أن ميزة جمع الأقراص أو الأقسام الفيزيائية مفيدة، إلا أنها ليست الميزة الأساسية لاستخدام LVM. لا تبدو المرونة التي تحصل عليها من LVM واضحة إلا بعد مرور فترة من الزمن بشكل خاص، عندما تتغير الحاجات. في مثالنا السابق، دعنا نفترض أن هناك ملفات جديدة كبيرة يجب تخزينها، وأن الحيز المنطقي المخصص لمخدم الملفات صغير جداً عليها. بما أننا لم نستهلك كامل المساحة الحرة المتوفرة على <filename>vg_critical</filename>، يمكننا توسعة <filename>lv_files</filename>. سوف نستخدم الأمر <command>lvresize</command> لذلك الغرض، ثم نستخدم <command>resize2fs</command> لملائمة نظام الملفات مع الحجم الجديد:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 5.00g\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
#| "# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
#| "  Logical volume lv_files successfully resized\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 7.00g\n"
#| "# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
#| "<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
#| "Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
#| "old_desc_blocks = 1, new_desc_blocks = 1\n"
#| "The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
#| "\n"
#| "# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  4.9G  4.2G  485M  90% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 7.99g 1.99g\n"
"# </computeroutput><userinput>lvresize -L 6G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\n"
"  Logical volume vg_critical/lv_files successfully resized.\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 6.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.44.5 (15-Dec-2018)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1572864 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.9G  4.2G  1.5G  75% /srv/files</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
"# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
"  Logical volume lv_files successfully resized\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 7.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"

msgid "<emphasis>CAUTION</emphasis> Resizing filesystems"
msgstr "<emphasis>تحذير</emphasis> تحجيم نظم الملفات"

#, fuzzy
#| msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It's rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It is rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgstr "لا تدعم جميع نظم الملفات التحجيم أثناء الاتصال (online resizing)؛ بالتالي يجب فصل نظام الملفات أولاً (unmount) ثم إعادة ربطه بعد إنهاء العملية. طبعاً إذا كان هناك رغبة بتصغير المساحة المخصصة لأحد الحيزات المنطقية، فيجب تقليص نظام الملفات اولاً؛ أما في حال التكبير فيكون الترتيب معكوساً: حيث يجب تكبير الحيز المنطقي قبل توسعة نظام الملفات داخله. هذا منطقي تمامًا، فلا يمكن أن يكون حجم نظام الملفات أكبر من حجم الجهاز الكتلي الذي يحويه بأي حال من الأحوال (سواء كان الجهاز قسمًا فيزيائياً أو كان حيز تخزين منطقي)."

msgid "The ext3, ext4 and xfs filesystems can be grown online, without unmounting; shrinking requires an unmount. The reiserfs filesystem allows online resizing in both directions. The venerable ext2 allows neither, and always requires unmounting."
msgstr "يمكن توسعة نظم الملفات ext3، وext4 و xfs دون فصل الاتصال (online)؛ أما التقليص فيحتاج الفصل عن شجرة الملفات. يسمح نظام الملفات reiserfs بالتحجيم أثناء الاتصال في الاتجاهين. أما صاحب الجلالة ext2 فلا يسمح بأي منهما، ويحتاج للفصل في جميع الحالات."

msgid "We could proceed in a similar fashion to extend the volume hosting the database, only we've reached the VG's available space limit:"
msgstr "يمكننا توسعة الحيز الذي يستضيف قاعدة البيانات بنفس الأسلوب، لولا أننا وصلنا لحدود المساحة المتاحة على المجموعة:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  976M  882M   28M  97% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree   \n"
"  vg_critical   2   2   0 wz--n- 7.99g 1016.00m</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
"  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>\n"

msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb1</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgstr "لا مشكلة، حيث يسمح LVM بإضافة حيزات فيزيائية إلى المجموعات القائمة مسبقًا. مثلاً، ربما لاحظنا أن القسم <filename>sdb1</filename> الذي كان يستخدم خارج نظام LVM حتى الآن، كان يحتوي على أرشيفات يمكن نقلها إلى <filename>lv_backups</filename>. يمكننا الآن إعادة استخدام القسم ودمجه في مجموعة الحيزات الحالية، واستثمار بعض المساحة الحرة. هذه هي وظيفة الأمر <command>vgextend</command>. طبعاً يجب تهيئة القسم كحيز فيزيائي قبل ذلك. بعد توسيع المجموعة، يمكننا استخدام أوامر مشابهة للسابقة لتمديد الحيز المنطقي وتوسعة نظام الملفات بعد ذلك:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
#| "# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created.\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
"  vg_critical   3   2   0 wz--n- &lt;9.99g &lt;1.99g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  882M  994M  48% /srv/base</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"

msgid "<emphasis>GOING FURTHER</emphasis> Advanced LVM"
msgstr "<emphasis>التعمق أكثر</emphasis> LVM متقدم"

#, fuzzy
#| msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance, to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgstr "يسمح LVM باستخدامات متقدمة أكثر، حيث يمكن تحديد الكثير من التفاصيل يدوياً. مثلاً، يستطيع مدير النظام تعديل حجم الكتل التي تتركب منها الحيزات الفيزيائية والمنطقية، كما يستطيع ضبط تخطيطها الفيزيائي (physical layout). من الممكن أيضًا نقل الكتل بين الحيزات الفيزيائية، لضبط الأداء بدقة مثلاً، أو تحرير PV معين عند الحاجة لإخراج القرص الفيزيائي الموافق من المجموعة (سواء لنقله إلى VG أخرى أو إزالته من LVM بالكامل) كتيبات التعليمات التي تصف الأوامر واضحة ومفصلة بشكل عام. صفحة <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry>‎ هي نقطة بدء جيدة."

msgid "RAID or LVM?"
msgstr "‏RAID أو LVM؟"

msgid "RAID and LVM both bring indisputable advantages as soon as one leaves the simple case of a desktop computer with a single hard disk where the usage pattern doesn't change over time. However, RAID and LVM go in two different directions, with diverging goals, and it is legitimate to wonder which one should be adopted. The most appropriate answer will of course depend on current and foreseeable requirements."
msgstr "يقدم كلٌّ من RAID وLVM ميزات لا تقبل الجدل عندما يبتعد المرء عن الحالة البسيطة للحاسوب المكتبي ذي القرص الواحد حيث لا تتغير الاستخدامات مع مرور الزمن. لكن RAID وLVM يتباعدان في اتجاهين مختلفين، وتتباعد أهدافهما، ومن المقبول أن يتسائل المرء عن أي التقنيتين يجب أن يتبناها. الإجابة الأنسب ستعتمد طبعاً على الحاجات الحالية والمتوقعة."

msgid "There are a few simple cases where the question doesn't really arise. If the requirement is to safeguard data against hardware failures, then obviously RAID will be set up on a redundant array of disks, since LVM doesn't really address this problem. If, on the other hand, the need is for a flexible storage scheme where the volumes are made independent of the physical layout of the disks, RAID doesn't help much and LVM will be the natural choice."
msgstr "هناك عدة حالات بسيطة حيث لا تظهر فيها أي تساؤلات فعلية. إذا كان الهدف هو حماية البيانات من عطب العتاد، فالحل طبعاً هو إعداد RAID مع مصفوفة أقراص ذات فائض تخزيني، نظرًا لأن LVM لا يعالج هذه المشكلة أبداً. من جهة أخرى، إذا كان هناك حاجة لتصميم تخزيني مرن تستقل فيه الحيزات التخزينية عن المخطط الفيزيائي للأقراص، عندها RAID لا يساعد كثيراً وLVM هو الخيار الطبيعي."

msgid "<emphasis>NOTE</emphasis> If performance matters…"
msgstr "<emphasis>ملاحظة</emphasis> إذا كان الأداء مهمًا…"

msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgstr "إذا كانت سرعة الدخل والخرج جوهرية، خصوصًا من ناحية أزمنة الوصول، فإن استخدام LVM أو RAID أو جمعهما معاً بإحدى الطرق قد يؤثر على الأداء، وأحياناً يجب أخذ هذا بعين الاعتبار عند اختيار إحدى التقنيتين. إلا أن هذه الاختلافات في الأداء صغيرة حقاً، ولا يمكن قياسها إلا في حالات قليلة. إذا كان الأداء مهمًا، فإن أكبر زيادة يمكن الحصول عليها تكون باستخدام وسائط تخزين غير ميكانيكية (سواقات الحالة الصلبة SSD – ‏<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis>)؛ كلفة الميغابايت في هذه الوسائط أعلى من كلفته في الأقراص الصلبة العادية، كما أن سعتها أصغر عادة، لكنها تقدم أداء باهراً للوصول العشوائي. إذا كان نمط الاستخدام يشتمل على العديد من عمليات الدخل والخرج المنتشرة على أنحاء نظام الملفات، كما في حالة قواعد البيانات التي تجرى عليها استعلامات معقدة مثلاً، فإن جدوى تشغيلها على SSD أكبر بكثير من استخدام LVM بدلاً من RAID أو العكس. يجب اتخاذ القرار في هذه الحالات اعتماداً على معايير أخرى غير السرعة، نظراً لأن موضوع الأداء يمكن معالجته بسهولة باستخدام SSD."

msgid "The third notable use case is when one just wants to aggregate two disks into one volume, either for performance reasons or to have a single filesystem that is larger than any of the available disks. This case can be addressed both by a RAID-0 (or even linear-RAID) and by an LVM volume. When in this situation, and barring extra constraints (for instance, keeping in line with the rest of the computers if they only use RAID), the configuration of choice will often be LVM. The initial set up is barely more complex, and that slight increase in complexity more than makes up for the extra flexibility that LVM brings if the requirements change or if new disks need to be added."
msgstr "حالة الاستخدام الثالثة الجديرة بالاهتمام هي عندما يحتاج المرء جمع قرصين في حيز تخزيني واحد، وذلك بهدف زيادة الأداء أو للحصول على نظام ملفات أكبر من سعة الأقراص المتوفرة. يمكن معالجة هذه الحالة باستخدم RAID-0 (أو حتى linear-RAID) أو باستخدام LVM. في هذه الحالة، يقع الاختيار على LVM ما لم تكن هناك قيود إضافية (الانسجام مع بقية الحواسيب إذا كانت تعتمد على RAID مثلاً). الإعداد الأولي لنظام LVM أكثر تعقيداً بقليل، ولكن المرونة الإضافية التي يوفرها تعوض هذه الزيادة الطفيفة في التعقيدات عندما تتغير المتطلبات التخزينية أو إذا دعت الحاجة لإضافة أقراص جديدة."

msgid "Then of course, there is the really interesting use case, where the storage system needs to be made both resistant to hardware failure and flexible when it comes to volume allocation. Neither RAID nor LVM can address both requirements on their own; no matter, this is where we use both at the same time — or rather, one on top of the other. The scheme that has all but become a standard since RAID and LVM have reached maturity is to ensure data redundancy first by grouping disks in a small number of large RAID arrays, and to use these RAID arrays as LVM physical volumes; logical partitions will then be carved from these LVs for filesystems. The selling point of this setup is that when a disk fails, only a small number of RAID arrays will need to be reconstructed, thereby limiting the time spent by the administrator for recovery."
msgstr "ثم نصل طبعاً إلى حالة الاستخدام الشيقة حقاً، وهي عندما نحتاج نظاماً تخزينياً يقاوم أعطال العتاد ومرناً من ناحية توزيع الحيزات التخزينية. لا يستطيع RAID وحده ولا LVM معالجة المتطلبين معاً؛ هذه هي الحالة التي نستخدم فيها الاثنين في الوقت نفسه — أو بالأحرى، نستخدم أحدهما فوق الآخر. أكثر طريقة مستخدمة منذ وصل RAID و LVM إلى مرحلة النضج هي ضمان حماية البيانات أولاً من خلال جمع الأقراص في عدد صغير من مصفوفات RAID الكبيرة، ثم استخدام هذه المصفوفات كحيزات فيزيائية لنظام LVM؛ بعدها تقطع LVs إلى أقسام منطقية لإنشاء نظم الملفات. إن ميزة هذا الأسلوب هي أنه عندما يتعطل قرص ما، سنحتاج لإعادة بناء عدد صغير من مصفوفات RAID، وبالتالي اختصار الوقت الذي يقضيه مدير النظام في الاستعادة."

msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited videoclips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgstr "لنأخذ مثلاً حقيقياً: يحتاج قسم العلاقات العامة في شركة فلكوت محطة عمل لتحرير الفيديو، لكن ميزانية القسم لا تسمح بشراء عتاد متطور بالكامل. اتخذ القرار بتفضيل العتاد المخصص لأعمال الجرافيك (الشاشة وبطاقة الفيديو)، والاكتفاء بالعتاد العادي بالنسبة لوسائط التخزين. لكن، كما هو معلوم، يحتاج الفيديو الرقمي بعض المتطلبات الخاصة فيما يتعلق بوشائط التخزين: فكمية البيانات المخزنة كبيرة، كما أن معدل النقل عند قراءة أو كتابة هذه البيانات مهم ويؤثر على الأداء الكلي للنظام (أهميته أكبر من أهمية زمن الوصول النموذجي مثلاً). يجب تلبية هذه المتطلبات باستخدام عتاد عادي، في هذه الحالة لدينا قرصين صلبين SATA سعة كل منهما 300 غيغابايت؛ يجب أيضًا أن تقاوم بيانات النظام وبعض من بيانات المستخدم أعطال العتاد، إذ يجب أن تبقى مقاطع الفيديو المحررة بأمان، لكن اللقطات (rushes) التي تنتظر التحرير أقل أهميةً، بما أنها لا تزال متوفرة على شرائط الفيديو."

msgid "RAID-1 and LVM are combined to satisfy these constraints. The disks are attached to two different SATA controllers to optimize parallel access and reduce the risk of a simultaneous failure, and they therefore appear as <filename>sda</filename> and <filename>sdc</filename>. They are partitioned identically along the following scheme:"
msgstr "سوف نجمع RAID-1 و LVM معاً لإيفاء هذه الشروط. سوف نصل القرصين إلى متحكمي SATA مختلفين لتحسين الوصول المتوازي وتخفيف خطر الأعطال المتزامنة، بالتالي سوف يظهر القرصان باسمي <filename>sda</filename> و<filename>sdc</filename>. سوف نقطِّع القرصين وفق المخطط التالي:"

msgid ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"

msgid "The first partitions of both disks (about 1 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgstr "جمعنا القسمين الأولين من كل قرص (حوالي 1 غ.ب) في حيز RAID-1، هو <filename>md0</filename>. هذه المرآة ستستخدم مباشرة لتخزين نظام الملفات الجذر."

msgid "The <filename>sda2</filename> and <filename>sdc2</filename> partitions are used as swap partitions, providing a total 2 GB of swap space. With 1 GB of RAM, the workstation has a comfortable amount of available memory."
msgstr "استخدمنا القسمين <filename>sda2</filename> و<filename>sdc2</filename> كقسمي swap، ما منحنا مساحة تبديل سعتها الكلية 2 غ.ب. ومع 1 غ.ب من الذاكرة RAM، أصبحت كمية الذاكرة المتوفرة لمحطة العمل مريحة."

msgid "The <filename>sda5</filename> and <filename>sdc5</filename> partitions, as well as <filename>sda6</filename> and <filename>sdc6</filename>, are assembled into two new RAID-1 volumes of about 100 GB each, <filename>md1</filename> and <filename>md2</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 200 GB of safe space."
msgstr "جمعنا القسمين <filename>sda5</filename> و<filename>sdc5</filename>، كما جمعنا <filename>sda6</filename> و<filename>sdc6</filename> في حيزي RAID-1 حجم كل منهما حوالي 100 غ.ب، هما <filename>md1</filename> و<filename>md2</filename>. تمت تهيئة كل من هاتين المرآتين كحيز LVM فيزيائي، وتم تخصيصهما للمجموعة <filename>vg_raid</filename>. هذه VG تحوي تقريبًا 200 غ.ب من المساحة المؤمنة."

msgid "The remaining partitions, <filename>sda7</filename> and <filename>sdc7</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 200 GB of space."
msgstr "استخدمنا القسمين المتبقيين، <filename>sda7</filename> و<filename>sdc7</filename>، مباشرة بشكل حيزات فيزيائية، وخصصناهما لمجموعة حيزات أخرى تدعى <filename>vg_bulk</filename>، حيث أصبحت تحوي تقريبًا 200 غ.ب من المساحة."

msgid "Once the VGs are created, they can be partitioned in a very flexible way. One must keep in mind that LVs created in <filename>vg_raid</filename> will be preserved even if one of the disks fails, which will not be the case for LVs created in <filename>vg_bulk</filename>; on the other hand, the latter will be allocated in parallel on both disks, which allows higher read or write speeds for large files."
msgstr "بعد إنشاء VGs، يمكن تقطيعها بطريقة مرنة جداً. يجب أن نأخذ بعين الاعتبار أن LVs التي ننشئها في <filename>vg_raid</filename> ستبقى محفوظة حتى لو تعطل أحد القرصين، لكن هذا لا ينطبق على LVs التي ننشئها في <filename>vg_bulk</filename>؛ من ناحية أخرى، سوف تحجز الحيزات المنطقية في <filename>vg_bulk</filename> على القرصين على التوازي، ما يسمح بسرعات قراءة أو كتابة أكبر للملفات الكبيرة."

#, fuzzy
#| msgid "We will therefore create the <filename>lv_usr</filename>, <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgid "We will therefore create the <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgstr "إذن سوف ننشئ الحيزات المنطقية <filename>lv_usr</filename> و<filename>lv_var</filename> و<filename>lv_home</filename> على <filename>vg_raid</filename>، لتخزين نظم الملفات المقابلة لها؛ وسنستخدم حيز منطقي آخر كبير باسم <filename>lv_movies</filename> لتخزين النسخ النهائية من الأفلام بعد التحرير. سوف نقسم الـVG الأخرى إلى حيز كبير باسم <filename>lv_rushes</filename>، للبيانات القادمة مباشرة من كميرات الفيديو الرقمية، و<filename>lv_tmp</filename> للملفات المؤقتة. تحديد موقع مساحة العمل ليس خياراً واضحاً تماماً: في حين أن الأداء الجيد مطلوب لذلك القسم، هل يستحق هذا المخاطرة بخسارة العمل إذا تعطل أحد الأقراص أثناء جلسة التحرير؟ اعتماداً على إجابة ذلك السؤال، سوف ننشئ الحيز المنطقي المناسب على إحدى المجموعتين."

#, fuzzy
#| msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications. Should new software be installed later on (for editing audio clips, for instance), the LV hosting <filename>/usr/</filename> can be grown painlessly."
msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications."
msgstr "الآن أصبح لدينا بعض الفائض يضمن لنا حماية البيانات الهامة ومرونة كبيرة في توزيع المساحة المتوفرة بين التطبيقات. على فرض أن هناك حاجة لتثبيت برمجيات جديدة لاحقًا (لتحرير المقاطع الصوتية مثلاً)، يمكن توسيع الحيز المنطقي المقابل لنظام ملفات <filename>/usr/</filename> بسهولة."

msgid "<emphasis>NOTE</emphasis> Why three RAID-1 volumes?"
msgstr "<emphasis>ملاحظة</emphasis> لماذا ثلاثة حيزات RAID-1؟"

msgid "We could have set up one RAID-1 volume only, to serve as a physical volume for <filename>vg_raid</filename>. Why create three of them, then?"
msgstr "كان يمكن إعداد حيز RAID-1 واحد فقط ليعمل كحيز فيزيائي نضع عليه <filename>vg_raid</filename>. فلم أنشأنا ثلاثة منها إذاً؟"

msgid "The rationale for the first split (<filename>md0</filename> vs. the others) is about data safety: data written to both elements of a RAID-1 mirror are exactly the same, and it is therefore possible to bypass the RAID layer and mount one of the disks directly. In case of a kernel bug, for instance, or if the LVM metadata become corrupted, it is still possible to boot a minimal system to access critical data such as the layout of disks in the RAID and LVM volumes; the metadata can then be reconstructed and the files can be accessed again, so that the system can be brought back to its nominal state."
msgstr "السبب وراء القسم الأول (فصل <filename>md0</filename> عن البقية) هو أمان البيانات: فالبيانات التي تكتب على مرايا RAID-1 هي نفسها على جميع الأقراص، ولذلك يمكن تجاوز طبقة RAID وربط أحد أقراص المصفوفة مباشرة. في حال مواجهة علة في النواة مثلاً، أو إذا تضررت البيانات الفوقية التي تُعرّف LVM، يمكن عندها إقلاع نظام أصغري يسمح بالوصول إلى البيانات الحساسة مثل مخطط الأقراص في حيزات RAID وLVM؛ يمكن حينئذ إعادة بناء البيانات الفوقية والوصول للملفات ثانية، بحيث يعود النظام إلى حالته النظامية."

msgid "The rationale for the second split (<filename>md1</filename> vs. <filename>md2</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md2</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md2</filename> and integrate its components <filename>sda6</filename> and <filename>sdc6</filename> into the bulk VG (which grows by 200 GB instead of 100 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgstr "أما السبب وراء القسم الثاني (فصل <filename>md1</filename> عن <filename>md2</filename>) فهو أقل وضوحاً، والداعي له هو عدم ثقتنا بطبيعة التغييرات التي سنحتاجها في المستقبل. قد لا نعرف الحاجات التخزينية للمستخدمين بدقة عند تجميع محطة العمل أول مرة، كما يمكن أن تتغير هذه الحاجات مع مرور الزمن. في حالتنا، لا يمكننا معرفة الأحجام التخزينية اللازمة للقطات الخام (rushes) ومقاطع الفيديو المكتملة مسبقاً. إذا احتاج أحد المقاطع لعدد كبير من اللقطات، وكان أكثر من نصف VG المخصصة للحيزات المؤمنة فارغاً، يمكننا إعادة استخدام بعض المساحة غير اللازمة منها. يمكننا إزالة أحد الحيزات الفيزيائية، ولنقل <filename>md2</filename>، من <filename>vg_raid</filename> ثم نضيفه إلى <filename>vg_bulk</filename> مباشرة (إذا كانت المدة المتوقعة لإنهاء العملية قصيرة بحيث نستطيع قبول الانخفاض المؤقت في الأداء)، أو نلغي مصفوفة RAID على <filename>md2</filename> وندمج مكوناتها (<filename>sda6</filename> و<filename>sdc6</filename>) مع VG غير المؤمنة (التي ستكبر بمقدار 200 غ.ب بدلاً من 100 غ.ب)؛ بعدها يمكن توسعة الحيز المنطقي <filename>lv_rushes</filename> حسب الحاجة."

msgid "<primary>virtualization</primary>"
msgstr "<primary>الحوسبة الظاهرية (virtualization)</primary>"

msgid "Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security."
msgstr "الحوسبة الظاهرية (virtualization) هي إحدى أهم تطورات الحوسبة في السنوات الأخيرة. يغطي المصطلح العديد من المفاهيم والتقنيات المستخدمة لمحاكاة الحواسيب الظاهرية بدرجات متفاوتة من الاستقلال عن العتاد الفعلي. يمكن لمخدم فيزيائي واحد عندها أن يستضيف العديد من الأنظمة التي تعمل في الوقت نفسه بمعزل عن بعضها. تطبيقات هذه التقنية عديدة، وهي مشتقة غالبًا من فكرة العزل: كاختبار بيئات لها إعدادات مختلفة مثلاً، أو فصل الخدمات المقدمة عبر حواسيب ظاهرية (virtual) مختلفة لزيادة الأمن."

msgid "There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:"
msgstr "هناك الكثير من حلول الحوسبة الظاهرية، لكل منها ميزاته وعيوبه. يركز هذا الكتاب على Xen، و LXC، وKVM، لكن هناك حلول أخرى تستحق الذكر منها:"

msgid "<primary><emphasis>VMWare</emphasis></primary>"
msgstr "<primary><emphasis>VMWare</emphasis></primary>"

msgid "<primary><emphasis>Bochs</emphasis></primary>"
msgstr "<primary><emphasis>Bochs</emphasis></primary>"

msgid "<primary><emphasis>QEMU</emphasis></primary>"
msgstr "<primary><emphasis>QEMU</emphasis></primary>"

msgid "<primary><emphasis>VirtualBox</emphasis></primary>"
msgstr "<primary><emphasis>VirtualBox</emphasis></primary>"

msgid "<primary><emphasis>KVM</emphasis></primary>"
msgstr "<primary><emphasis>KVM</emphasis></primary>"

msgid "<primary><emphasis>LXC</emphasis></primary>"
msgstr "<primary><emphasis>LXC</emphasis></primary>"

#, fuzzy
#| msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"
msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"https://www.qemu.org/\" />"
msgstr "QEMU هو محاك برمجي لحاسوب كامل؛ الأداء بعيد عن السرعة التي تحصل عليها من العمل بشكل مباشر على العتاد (natively)، لكنه يسمح بتشغيل نظم تشغيل غير معدلة أو نظم تجريبية على عتاد ظاهري. كما يسمح أيضًا بمحاكاة معماريات عتادية مختلفة: مثلاً، يستطيع نظام <emphasis>amd64</emphasis> محاكاة حاسوب <emphasis>arm</emphasis>.‏ QEMU برنامج حر. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"

msgid "Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64)."
msgstr "Bochs هو نظام محاكاة حر آخر، لكنه يحاكي معماريات x86 فقط (i386 و amd64)."

#, fuzzy
#| msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"
msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"https://www.vmware.com/\" />"
msgstr "VMWare هو نظام محاكاة احتكاري (مملوك – proprietary)؛ بما أنه أقدم الحلول المتوفرة فهو أيضًا أكثرها شهرة. يعتمد على مبادئ تشبه مبادئ QEMU. يقدم VMWare ميزات متقدمة مثل أخذ لقطة (snapshot) لحالة حاسوب ظاهري قيد العمل. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"

#, fuzzy
#| msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler. While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"
msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type=\"block\" url=\"https://www.virtualbox.org/\" />"
msgstr "VirtualBox هو نظام محاكاة معظمه برمجيات حرة (رغم أن بعض المكونات الإضافية متوفرة برخص احتكارية). للأسف فهو مصنف في قسم المشتركات ”contrib“ لأنه يحوي بعض الملفات المترجمة مسبقاً والتي لا يمكن إعادة بناؤها دون استخدام مترجم مملوك. رغم أنه أقل عمراً من VMWare ومقيد بمعماريتي i386 و amd64، إلا أنه يتضمن مع ذلك ميزة snapshot وبعض الميزات المشوقة الأخرى. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"

#, fuzzy
#| msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgid "<emphasis>HARDWARE</emphasis> Virtualization support"
msgstr "<emphasis>التعمق أكثر</emphasis> المحاكاة العملاقة"

msgid "Some computers might not have hardware virtualization support; when they do, it should be enabled in the BIOS."
msgstr ""

msgid "To know if you have virtualization support enabled, you can check if the relevant flag is enabled with <command>grep</command>. If the following command for your processor returns some text, you already have virtualization support enabled:"
msgstr ""

msgid "For Intel processors you can execute <command>grep vmx /proc/cpuinfo</command>"
msgstr ""

msgid "For AMD processors you can execute <command>grep svm /proc/cpuinfo</command>"
msgstr ""

msgid "Xen <indexterm><primary>Xen</primary></indexterm> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen."
msgstr "Xen<indexterm><primary>Xen</primary></indexterm> هو حل محاكاة ”شبه ظاهرية – paravirtualization“. يقدم Xen طبقة عزل رقيقة، تدعى ”المشرف – hypervisor“، بين العتاد والأنظمة العليا؛ تعمل بمثابة مرجع يتحكم بالوصول للعتاد من الحواسيب الظاهرية. لكنها تعالج عدداً قليلاً من التعليمات، أما البقية فتنفذ مباشرة على العتاد بالنيابة عن الأنظمة الظاهرية. الميزة الأساسية هي أن مستوى الأداء لا ينخفض، والنظم تعمل بسرعات تقترب من السرعة الأصلية؛ لكن نقطة الضعف هي أن نوى نظم التشغيل التي يمكن استخدامها مع مشرف Xen يجب تعديلها لتناسب العمل على Xen."

#, fuzzy
#| msgid "Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgid "Let's spend some time on terms. The hypervisor is the lowest layer, which runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgstr "لنمض بعض الوقت في التعرف على المصطلحات. المُشرف هو أدنى طبقة، يعمل مباشرة على العتاد، بل تحت النواة حتى. يستطيع هذا المشرف تقسيم البرمجيات الأخرى إلى عدة نطاقات domains، التي يمكن اعتبارها كحواسيب ظاهرية متعددة. يدعى أحد هذه النطاقات (أول نطاق يتم تشغيله) باسم dom0، ويتمتع بدور خاص، حيث يستطيع هذا النطاق فقط التحكم بالمشرف وتنفيذ النطاقات الأخرى. تعرف هذه النطاقات الأخرى باسم domU. بكلمات أخرى، من وجهة نظر المستخدم، يقابل dom0 ”المستضيف – host“ في نظم المحاكاة الأخرى، بينما يمكن اعتبار domU على أنه ”الضيف – guest“."

msgid "<emphasis>CULTURE</emphasis> Xen and the various versions of Linux"
msgstr "<emphasis>ثقافة</emphasis> Xen والإصدارات المختلفة من لينكس"

msgid "Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <emphasis>paravirt_ops</emphasis> or <emphasis>pv_ops</emphasis> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially."
msgstr "تم تطوير Xen أساسًا كمجموعة من الترقيعات التي بقيت خارج الشجرة الرسمية، ولم تدمج في النواة لينكس. في الوقت نفسه، تطلبت عدة نظم محاكاة جديدة (بما فيها KVM) بعض الدوال العامة المتعلقة بالمحاكاة لتسهيل دمجها، وأضيفت هذه الدوال إلى النواة لينكس (التي تعرف بواجهة <emphasis>paravirt_ops</emphasis> أو <emphasis>pv_ops</emphasis>). وبما أن رقع Xen كانت تكرر بعض وظائف هذه الواجهة، لم يعد قبولها رسميًا ممكنًا."

#, fuzzy
#| msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgstr "كان على Xensource، وهي الشركة وراء تطوير Xen، نقل Xen لإطار العمل الجديد هذا، حتى يمكن دمج رقع Xen في شجرة النواة لينكس الرسمية. هذا يعني الكثير من إعادة كتابة الكود، وبالرغم من أن Xensource وصلت سريعاً إلى نسخة فعالة اعتماداً على واجهة paravirt_ops، إلا أن الرقع لم تدمج إلا تدريجيًا في النواة الرسمية. تم إكمال الدمج في لينكس 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"

#, fuzzy
#| msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgstr "بما أن <emphasis role=\"distribution\">جيسي</emphasis> تعتمد على الإصدار 3.16 من النواة لينكس، فإن الحزم النظامية <emphasis role=\"pkg\">linux-image-686-pae</emphasis> و<emphasis role=\"pkg\">linux-image-amd64</emphasis> تحوي الكود اللازم، والترقيع الخاص بالتوزيعة الذي كان لازماً مع <emphasis role=\"distribution\">سكويز</emphasis> والنسخ السابقة من دبيان لم يعد مطلوباً. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"

msgid "<emphasis>CULTURE</emphasis> Xen and non-Linux kernels"
msgstr "<emphasis>ثقافة</emphasis> Xen والنوى المختلفة عن لينكس"

#, fuzzy
#| msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgstr "يحتاج Xen لتعديل جميع نظم التشغيل التي يريد المرء تشغيلها عليه؛ لا تتمتع جميع النوى بدرجة النضج نفسها في هذا المجال. العديد من النوى تعمل بالكامل، سواء في dom0 أو domU: مثل لينكس 3.0 وما بعد، وNetBSD 4.0 وما بعد، وOpenSolaris. أما النوى الأخرى تعمل فقط في domU. يمكنك التحقق من حالة نظم التشغيل المختلفة في ويكي Xen:‏ <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"

msgid "However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows)."
msgstr "لكن إذا كان Xen يستطيع الاعتماد على تعليمات العتاد المختصة بالمحاكاة (المتوفرة فقط في المعالجات الأحدث)، فيمكن تشغيل النظم غير المعدلة أيضًا في domU (بما في ذلك Windows)."

msgid "<emphasis>NOTE</emphasis> Architectures compatible with Xen"
msgstr "<emphasis>ملاحظة</emphasis> المعماريات المتوافقة مع Xen"

msgid "Xen is currently only available for the i386, amd64, arm64 and armhf architectures."
msgstr "حالياً Xen متاح فقط لمعماريات i386 وamd64 وarm64 وarmhf."

msgid "Using Xen under Debian requires three components:"
msgstr "استخدام Xen في دبيان يحتاج ثلاثة مكونات:"

#, fuzzy
#| msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>."
msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.11-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.11-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.11-arm64</emphasis>."
msgstr "المشرف نفسه. الحزمة المناسبة هي إما <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis> أو <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis> أو <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>. حسب العتاد المستخدم."

#, fuzzy
#| msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <emphasis role=\"distribution\">Jessie</emphasis>."
msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 4.19 version present in <emphasis role=\"distribution\">Buster</emphasis>."
msgstr "نواة تعمل فوق المشرف. أي نواة أحدث من 3.0 سوف تعمل، بما في ذلك الإصدارة 3.16 المعتمدة في <emphasis role=\"distribution\">جيسي</emphasis>."

msgid "The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <emphasis role=\"pkg\">libc6-xen</emphasis> package."
msgstr "معمارية i386 تحتاج أيضًا لمكتبة قياسية مع الترقيعات المناسبة للاستفادة من Xen؛ هذه متوفرة في الحزمة <emphasis role=\"pkg\">libc6-xen</emphasis>."

#, fuzzy
#| msgid "In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <emphasis role=\"pkg\">xen-linux-system-amd64</emphasis>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.4</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:"
msgid "The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.11</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the GRUB bootloader menu, so as to start the chosen kernel in a Xen dom0. Note, however, that this entry is not usually set to be the first one in the list, but it will be selected by default."
msgstr "لتفادي عناء اختيار هذه المكونات يدويًا، تم توفير عدد من الحزم المريحة للمستخدم (مثل <emphasis role=\"pkg\">xen-linux-system-amd64</emphasis>)؛ كل من هذه الحزم تسحب تجميعة من حزم المشرف والنواة معروفة بتناسبها. يحضر المشرف معه أيضًا حزمة <emphasis role=\"pkg\">xen-utils-4.4</emphasis>، التي تحوي أدوات للتحكم بالمشرف من dom0. تحضر هذه الحزمة بدورها المكتبة القياسية المناسبة. خلال تثبيت كل هذا، تنشئ سكربتات الإعداد أيضًا مدخلة جديدة في قائمة محمل الإقلاع Grub، لبدء تشغيل النواة المختارة لنطاق dom0. لكن لاحظ أن هذه المدخلة لا تكون الأولى عادة في القائمة، ولذلك لن تحدد افتراضيًا. إذا لم يكن هذا السلوك مرغوبًا، يمكن تغييره بالأوامر التالي:"

msgid "Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps."
msgstr "بعد تثبيت هذه المتطلبات، يأتي دور اختبار سلوك dom0 نفسه؛ هذا يحتاج إعادة الإقلاع إلى المشرف ونواة Xen. يجب أن يقلع النظام بالأسلوب العادي، مع بعض الرسائل الإضافية على الشاشة خلال خطوات التهيئة المبكرة."

msgid "Now is the time to actually install useful systems on the domU systems, using the tools from <emphasis role=\"pkg\">xen-tools</emphasis>. This package provides the <command>xen-create-image</command> command, which largely automates the task. The only mandatory parameter is <literal>--hostname</literal>, giving a name to the domU; other options are important, but they can be stored in the <filename>/etc/xen-tools/xen-tools.conf</filename> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <command>xen-create-image</command> invocation. Important parameters of note include the following:"
msgstr "الآن حان وقت تثبيت أنظمة مفيدة على نطاقات domU، باستخدام الأدوات من حزمة <emphasis role=\"pkg\">xen-tools</emphasis>. توفر هذه الحزمة الأمر <command>xen-create-image</command>، الذي يؤتمت معظم المهمة. البارامتر الإجباري الوحيد هو <literal dir=\"ltr\">--hostname</literal>، لإعطاء اسم للنطاق domU؛ الخيارات الأخرى هامة، لكن يمكن تخزينها في ملف الضبط <filename dir=\"ltr\">/etc/xen-tools/xen-tools.conf</filename>، وغيابها من سطر الأوامر لا يسبب خطأ. من المهم إذاً التحقق من محتويات هذا الملف قبل إنشاء الصور، أو استخدام بارامترات إضافية عند استدعاء <command>xen-create-image</command>. نذكر من البارامترات الهامة:"

msgid "<literal>--memory</literal>, to specify the amount of RAM dedicated to the newly created system;"
msgstr "<literal dir=\"ltr\">--memory</literal>، لتحديد كمية RAM المخصصة للنظام الجديد؛"

msgid "<literal>--size</literal> and <literal>--swap</literal>, to define the size of the “virtual disks” available to the domU;"
msgstr "<literal dir=\"ltr\">--size</literal> و <literal dir=\"ltr\">--swap</literal>، لتحديد حجم ”الأقراص الظاهرية“ المتاحة للـ domU؛"

#, fuzzy
#| msgid "<literal>--debootstrap</literal>, to cause the new system to be installed with <command>debootstrap</command>; in that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">jessie</emphasis>)."
msgid "<literal>--debootstrap-cmd</literal>, to specify the which debootstrap command is used. The default is <command>debootstrap</command> if debootstrap and cdebootstrap are installed. In that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">buster</emphasis>)."
msgstr "<literal dir=\"ltr\">--debootstrap</literal>، لتثبيت النظام الجديد مع <command>debootstrap</command>؛ في تلك الحالة، يستخدم خيار <literal dir=\"ltr\">--dist</literal> أيضًا أغلب الأحيان (مع اسم توزيعة ما مثل <emphasis role=\"distribution\">jessie</emphasis>)."

msgid "<emphasis>GOING FURTHER</emphasis> Installing a non-Debian system in a domU"
msgstr "<emphasis>التعمق أكثر</emphasis> تثبيت نظام آخر غير دبيان في domU"

msgid "In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <literal>--kernel</literal> option."
msgstr "في حال تثبيت نظام تشغيل لا يعتمد على نواة لينكس، يجب الانتباه لتحديد النواة التي يجب أن يستخدمها domU، عبر استخدام الخيار <literal dir=\"ltr\">--kernel</literal>."

msgid "<literal>--dhcp</literal> states that the domU's network configuration should be obtained by DHCP while <literal>--ip</literal> allows defining a static IP address."
msgstr "يبين <literal dir=\"ltr\">--dhcp</literal> أن الحصول على إعدادات الشبكة في domU يتم من خلال DHCP بينما يسمح <literal dir=\"ltr\">--ip</literal> بتحديد عنوان IP ستاتيكي (ثابت)."

msgid "Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <literal>--dir</literal> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <literal>--lvm</literal> option, followed by the name of a volume group; <command>xen-create-image</command> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive."
msgstr "أخيراً، يجب اختيار طريقة التخزين للصور المنشأة (التي سيراها domU على أنها أقراص صلبة). أبسط طريقة، التي تقابل الخيار <literal dir=\"ltr\">--dir</literal>، هي إنشاء ملف على dom0 لكل جهاز يجب تقديمه للـ domU. هناك بديل للأنظمة التي تستخدم LVM، وهو استخدام الخيار <literal dir=\"ltr\">--lvm</literal>، متبوعاً باسم مجموعة حيزات (VG)؛ عندئذ سينشئ <command>xen-create-image</command> حيزاً منطقيًا جديداً داخل تلك المجموعة، وسيكون هذا الحيز الجديد متاحاً للـ domU بشكل قرص صلب."

msgid "<emphasis>NOTE</emphasis> Storage in the domU"
msgstr "<emphasis>ملاحظة</emphasis> التخزين في domU"

msgid "Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <command>xen-create-image</command>, however, so editing the Xen image's configuration file is in order after its initial creation with <command>xen-create-image</command>."
msgstr "يمكن تصدير أقراص صلبة كاملة إلى domU، كما يمكن تصدير أقسام الأقراص، أو مصفوفات RAID أو حيزات LVM منطقية موجودة مسبقًا. لكن هذه العمليات لا يديرها الأمر <command>xen-create-image</command>، لذلك يجب تحرير ملف إعداد صورة Xen بعد إنشاءه أولاً باستخدام <command>xen-create-image</command>."

msgid "Once these choices are made, we can create the image for our future Xen domU:"
msgstr "بعد تحديد هذه الخيارات، يمكننا إنشاء صورة domU:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
#| "<computeroutput>\n"
#| "[...]\n"
#| "General Information\n"
#| "--------------------\n"
#| "Hostname       :  testxen\n"
#| "Distribution   :  jessie\n"
#| "Mirror         :  http://ftp.debian.org/debian/\n"
#| "Partitions     :  swap            128Mb (swap)\n"
#| "                  /               2G    (ext3)\n"
#| "Image type     :  sparse\n"
#| "Memory size    :  128Mb\n"
#| "Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
#| "Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
#| "[...]\n"
#| "Logfile produced at:\n"
#| "         /var/log/xen-tools/testxen.log\n"
#| "\n"
#| "Installation Summary\n"
#| "---------------------\n"
#| "Hostname        :  testxen\n"
#| "Distribution    :  jessie\n"
#| "MAC Address     :  00:16:3E:8E:67:5C\n"
#| "IP-Address(es)  :  dynamic\n"
#| "RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
#| "Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=buster --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  buster\n"
"Mirror         :  http://deb.debian.org/debian\n"
"Partitions     :  swap            512M  (swap)\n"
"                  /               2G    (ext4)\n"
"Image type     :  sparse\n"
"Memory size    :  256M\n"
"Kernel path    :  /boot/vmlinuz-4.19.0-5-amd64\n"
"Initrd path    :  /boot/initrd.img-4.19.0-5-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  buster\n"
"MAC Address     :  00:16:3E:0C:74:2F\n"
"IP Address(es)  :  dynamic\n"
"SSH Fingerprint :  SHA256:PuAGX4/4S07Xzh1u0Cl2tL04EL5udf9ajvvbufBrfvU (DSA)\n"
"SSH Fingerprint :  SHA256:ajFTX54eakzolyzmZku/ihq/BK6KYsz5MewJ98BM5co (ECDSA)\n"
"SSH Fingerprint :  SHA256:/sFov86b+rD/bRSJoHKbiMqzGFiwgZulEwpzsiw6aSc (ED25519)\n"
"SSH Fingerprint :  SHA256:/NJg/CcoVj+OLE/cL3yyJINStnla7YkHKe3/xEdVGqc (RSA)\n"
"Root Password   :  EwmQMHtywY9zsRBpqQuxZTb\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  jessie\n"
"Mirror         :  http://ftp.debian.org/debian/\n"
"Partitions     :  swap            128Mb (swap)\n"
"                  /               2G    (ext3)\n"
"Image type     :  sparse\n"
"Memory size    :  128Mb\n"
"Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
"Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  jessie\n"
"MAC Address     :  00:16:3E:8E:67:5C\n"
"IP-Address(es)  :  dynamic\n"
"RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
"Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
"</computeroutput>"

msgid "We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters."
msgstr "لدينا الآن حاسوب ظاهري، لكنه لا يعمل حاليًا (وبالتالي فهو لا يشغل سوى المساحة على القرص الصلب في dom0). طبعاً يمكننا إنشاء المزيد من الصور، وربما استخدمنا بارامترات أخرى."

#, fuzzy
#| msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgstr "قبل تشغيل هذه الحواسيب الظاهرية، علينا تحديد طريقة الوصول إليها. يمكن طبعاً اعتبارها حواسيب منفصلة، ونصل إليها فقط من خلال سطر أوامر النظام، لكن هذا نادراً ما يناسب نموذج الاستخدام. في معظم الأحيان، يعتبر domU كمخدم بعيد، ويتم الوصول إليه عبر الشبكة فقط. لكن من الصعب جداً إضافة بطاقة شبكة من أجل كل domU؛ ولذلك يسمح Xen بإنشاء واجهات شبكة ظاهرية، يستطيع كل نطاق أن يراها ويستعملها بالطريقة القياسية. لاحظ أن هذه البطاقات، بالرغم من أنها ظاهرية، إلا أنها غير مفيدة ما لم تتصل بأي شبكة، حتى لو كانت شبكة ظاهرية. لدى Xen عدة نماذج شبكية لهذا الغرض:"

msgid "The simplest model is the <emphasis>bridge</emphasis> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch."
msgstr "أبسط نموذج هو النموذج الجسري <emphasis>bridge</emphasis> model؛ وفيه تعمل جميع بطاقات eth0 (في أنظمة dom0 وdomU على حد سواء) كما لو كانت موصولة مباشرة مع تحويلة إيثرنت Ethernet switch."

msgid "Then comes the <emphasis>routing</emphasis> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network."
msgstr "بعدها يأتي نموذج التوجيه <emphasis>routing</emphasis> model، حيث يعمل dom0 كموجه (راوتر) ما بين أنظمة domU والشبكة الخارجية (الفيزيائية)."

msgid "Finally, in the <emphasis>NAT</emphasis> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0."
msgstr "أخيراً، نموذج <emphasis>NAT</emphasis>، وفيه يصل dom0 ثانية بين أنظمة domU وباقي عناصر الشبكة، لكن لا يمكن الوصول مباشرة من الخارج إلى أنظمة domU، وتمر البيانات عبر dom0 باستخدام network address translation (ترجمة عنوان الشبكة)."

msgid "These three networking nodes involve a number of interfaces with unusual names, such as <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> and <filename>xenbr0</filename>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model."
msgstr "هذه الأنماط الثلاثة تحتاج عدداً من الواجهات ذات المسميات الغريبة، مثل <filename dir=\"ltr\">vif*</filename>، و<filename dir=\"ltr\">veth*</filename>، ‏<filename dir=\"ltr\">peth*</filename> وأيضًا <filename>xenbr0</filename>. يرتب مشرف Xen هذه الواجهات في التخطيط الذي يعرفه المستخدم، حيث يتم التحكم بأدوات من فضاء المستخدم (user-space tools). سوف نقتصر على شرح النموذج الجسري، بما أن نموذج NAT ونموذج التوجيه يناسبان بعض الحالات الخاصة فقط."

#, fuzzy
#| msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.4</emphasis> package recommends it) to replace the existing eth0 entry:"
msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.11</emphasis> package recommends it) to replace the existing eth0 entry:"
msgstr "الإعداد القياسي لحزم Xen لا يؤثر على إعداد الشبكة للنظام. لكن خدمة <command>xend</command> معدّة لدمج الواجهات الشبكية الظاهرية في أي جسر شبكة سابق (يأخذ <filename>xenbr0</filename> الأولوية إذا كان هناك أكثر من جسر واحد). علينا إذاً إعداد جسر في <filename dir=\"ltr\">/etc/network/interfaces</filename> (وهذا يحتاج تثبيت حزمة <emphasis role=\"pkg\">bridge-utils</emphasis>، ولهذا السبب توصي بها حزمة <emphasis role=\"pkg\">xen-utils-4.4</emphasis>) لاستبدال المدخلة السابقة eth0:"

msgid ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

#, fuzzy
#| msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them."
msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them. You might need to increase the default memory by editing the variable memory from configuration file (in this case, <filename>/etc/xen/testxen.cfg</filename>). Here we have set it to 1024 (megabytes)."
msgstr "بعد إعادة التشغيل للتأكد أن الجسر يُنشَأ آليَّاً، يمكننا الآن تشغيل domU باستخدام أدوات التحكم بـXen، بالأخص الأمر <command>xl</command>. يسمح هذا الأمر بإجراء العديد من التعديلات على النطاقات، مثل سردها أو تشغيلها وإيقافها."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   463     1     r-----      9.8\n"
#| "# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
#| "<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
#| "# </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   366     1     r-----     11.4\n"
#| "testxen                                      1   128     1     -b----      1.1</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  1894     2     r-----      63.5\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  1505     2     r-----     100.0\n"
"testxen                                     13  1024     0     --p---       0.0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   463     1     r-----      9.8\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   366     1     r-----     11.4\n"
"testxen                                      1   128     1     -b----      1.1</computeroutput>"

msgid "<emphasis>TOOL</emphasis> Choice of toolstacks to manage Xen VM"
msgstr "<emphasis>أدوات</emphasis> اختيار أدوات إدارة أجهزة Xen الظاهرية"

msgid "<primary><command>xm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "<primary><command>xe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of libvirt and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgstr "في دبيان 7 والإصدارات الأقدم، كانت <command>xm</command> هي الأداة النصية المعيارية لإدارة أجهزة Xen الظاهرية. لكنها استبدلت الآن بالأداة <command>xl</command> المتوافقة مع الأداة القديمة بشكل كبير. لكن هناك أدوات أخرى متاحة: من الأدوات البديلة هناك <command>virsh</command> التابعة لمشروع libvirt و<command>xe</command> من XAPI (النسخة التجارية من Xen التي تقدمها XenServer)."

msgid "<emphasis>CAUTION</emphasis> Only one domU per image!"
msgstr "<emphasis>تحذير</emphasis> domU واحد فقط لكل صورة!"

#, fuzzy
#| msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is, however, quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgstr "مع أنه من الممكن طبعاً تشغيل أكثر من domU معاً على التوازي، إلا أن كل واحد منهم يحتاج استخدام صورة خاصة به، بما أن كل واحد من domU يعتقد أنه يعمل على عتاد خاص به (بغض النظر عن الجزء الصغير من النواة الذي يتخاطب مع المشرف). على الأخص، لا يمكن لنظامي domU يعملان في الوقت نفسه أن يتشاركا المساحة التخزينية. على أية حال، إذا كانت أنظمة domU لن تعمل في الوقت نفسه، فمن الممكن إعادة استخدام قسم swap ذاته، أو القسم الذي يحوي نظام الملفات <filename dir=\"ltr\">/home</filename>."

msgid "Note that the <filename>testxen</filename> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly."
msgstr "لاحظ أن النطاق <filename>testxen</filename> يستهلك ذاكرة حقيقية من الـRAM المتاحة للنطاق dom0، وليست ذاكرة ظاهرية. يجب أخذ الحيطة إذن عند بناء مخدم لاستضافة نسخ Xen، وتزويده بذاكرة فيزيائية مناسبة."

msgid "Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <filename>hvc0</filename> console, with the <command>xl console</command> command:"
msgstr "ڤوالا! آلتنا الظاهرية قيد الإقلاع. يمكننا الوصول إليها بإحدى طريقتين. الطريقة المعتادة هي الاتصال بها ”عن بعد“ عبر الشبكة، كما كنا سنتصل بأي حاسب حقيقي؛ هذا يحتاج عادة مخدم DHCP أو بعض إعدادات DNS. الطريقة الأخرى، ولعلها الطريقة الوحيدة إذا كانت إعدادات الشبكة غير صحيحة، هي استخدام طرفية <filename>hvc0</filename>، باستخدام الأمر <command>xl console</command>:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
#| "<computeroutput>[...]\n"
#| "\n"
#| "Debian GNU/Linux 8 testxen hvc0\n"
#| "\n"
#| "testxen login: </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 10 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 8 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"

msgid "One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> key combination."
msgstr "بعدها يمكنك بدء جلسة، كما لو كنت تجلس وراء لوحة مفاتيح الحاسب الظاهري. يتم الانفصال عن هذه الطرفية بالمفتاحين <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo>‎."

msgid "<emphasis>TIP</emphasis> Getting the console straight away"
msgstr "<emphasis>تلميح</emphasis> الوصول للطرفية مباشرة"

msgid "Sometimes one wishes to start a domU system and get to its console straight away; this is why the <command>xl create</command> command takes a <literal>-c</literal> switch. Starting a domU with this switch will display all the messages as the system boots."
msgstr "أحياناً يرغب المرء بتشغيل domU والوصول إلى طرفية النظام فوراً؛ ولذلك يقبل الأمر <command>xl create</command> الخيار <literal dir=\"ltr\">-c</literal>. تشغيل domU مع هذا الخيار سوف يعرض كل الرسائل مع إقلاع النظام."

msgid "<emphasis>TOOL</emphasis> OpenXenManager"
msgstr "<emphasis>أدوات</emphasis> OpenXenManager"

msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <command>xl</command> command."
msgstr "OpenXenManager (من الحزمة <emphasis role=\"pkg\">openxenmanager</emphasis>) هي واجهة رسومية تسمح بإدارة نطاقات Xen عن بعد بالاستفادة من Xen API. تستطيع هذه الواجهة إذن التحكم بنطاقات Xen عن بعد، وهي توفر معظم مزايا الأمر <command>xl</command>."

msgid "Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <command>xl pause</command> and <command>xl unpause</command> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <command>xl save</command> and <command>xl restore</command> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system."
msgstr "بعد أن يعمل domU، يمكن استخدامه مثل أي مخدم آخر (بما أنه نظام غنو/لينكس في النهاية). لكن بما أنه حاسب ظاهري فهذه الحالة تسمح ببعض المزايا الإضافية. مثلاً، يمكن إيقاف عمل domU مؤقتاً ثم استكماله، بالأمرين <command>xl pause</command> و<command>xl unpause</command>. لاحظ أن الذاكرة المخصصة للنطاق domU تبقى محجوزة أثناء الإيقاف المؤقت، رغم أنه لا يستهلك أي طاقة حسابية من المعالج. الأمران <command>xl save</command> و<command>xl restore</command> جديران بالاهتمام أيضاً: حفظ domU يحرر الموارد التي كان يستهلكها، بما في ذلك ذاكرة RAM. لا يلاحظ domU عند استعادته (أو استكمال عمله) أي شيء إلا مرور الزمن. إذا كان domU يعمل عند إيقاف تشغيل dom0، فسوف تحفظ سكربتات الحزمة حالة domU آلياً، وتستعيدها عند الإقلاع التالي. هذا يؤدي طبعاً للمتاعب التي تظهر عادة عند إسبات الحاسب المحمول. على سبيل المثال؛ إذا تعلق domU لفترة طويلة، فقد تلغى اتصالاته الشبكية. لاحظ أيضاً أن Xen حتى الآن غير متوافق مع شريحة واسعة من واجهة ACPI لإدارة الطاقة، ما يحول دون إمكانية إسبات النظام المستضيف (dom0)."

msgid "<emphasis>DOCUMENTATION</emphasis> <command>xl</command> options"
msgstr "<emphasis>توثيق</emphasis> خيارات <command>xl</command>"

msgid "Most of the <command>xl</command> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> manual page."
msgstr "تحتاج معظم أوامر <command>xl</command> الفرعية إلى متغير واحد أو أكثر، غالباً هي اسم domU. هذه المتغيرات مشروحة بشكل جيد في صفحة التعليمات <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry>‎."

msgid "Halting or rebooting a domU can be done either from within the domU (with the <command>shutdown</command> command) or from the dom0, with <command>xl shutdown</command> or <command>xl reboot</command>."
msgstr "يمكن إيقاف أو إعادة تشغيل domU إما من داخل domU نفسه (بالأمر <command>shutdown</command>) أو من dom0، بالأمر <command>xl shutdown</command> أو <command>xl reboot</command>."

msgid "<emphasis>GOING FURTHER</emphasis> Advanced Xen"
msgstr "<emphasis>التعمق أكثر</emphasis> خيارات Xen المتقدمة"

#, fuzzy
#| msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"
msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"https://xenproject.org/help/documentation/\" />"
msgstr "يملك Xen ميزات أكثر بكثير مما يمكننا شرحه في هذه المقاطع القليلة. على وجه الخصوص، النظام ديناميكي جداً، ويمكن تعديل العديد من بارامترات النطاق (مثل كمية الذاكرة المخصصة، الأقراص الصلبة المرئية، سلوك جدولة المهام، وغيرها) أثناء عمل النطاق. بل يمكن أيضًا تهجير domU بين المخدمات دون إيقاف تشغيله، ودون انقطاع اتصاله عن الشبكة! المصدر الرئيسي للمعلومات لجميع هذه المزايا المتقدمة هو توثيق Xen الرسمي. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"

msgid "<primary>LXC</primary>"
msgstr "<primary>LXC</primary>"

msgid "Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <emphasis>control groups</emphasis>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system."
msgstr "بالرغم من أن LXC يستخدم لبناء ”حواسيب ظاهرية“، إلا أن LXC –إذا تحرينا الدقة– ليس نظام محاكاة، بل هو نظام لعزل مجموعات من العمليات عن بعضها مع أنها تعمل على نفس الحاسب المستضيف. يستفيد هذا النظام من مجموعة من التطورات الحديثة في النواة لينكس، التي تعرف باسم مجموعات التحكم—<emphasis>control groups</emphasis>، التي تسمح لعدة زمر مختلفة من العمليات التي تدعى ”المجموعات“ برؤية بعض مظاهر النظام الكلي بشكل مختلف. من أبرز هذه المظاهر هي أرقام تعريف العمليات PIDs، وإعدادات الشبكة، ونقاط الربط في نظام الملفات. لا تستطيع أي مجموعة عمليات معزولة مثل هذه الوصول بأي شكل إلى العمليات الأخرى في النظام، كما يمكن تقييد وصولها إلى نظام الملفات بجزء فرعي محدد. يمكن لها أن تملك واجهة شبكية وجدول توجيه خاصين بها، ويمكن ضبطها حتى ترى مجموعة جزئية فقط من الأجهزة المتاحة المتصلة بالنظام."

#, fuzzy
#| msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there is no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgstr "يمكن جمع هذه المزايا لعزل عائلة كاملة من العمليات بدءاً من العملية <command>init</command>، وستشبه المجموعة الناتجة حاسوباً ظاهرياً. الاسم الرسمي لهذا الوضع هو ”حاوية—container“ (ومن هنا جاء اسم LXC: ‏<emphasis>LinuX Containers</emphasis>)، لكن الفرق الهام بينها وبين الحواسيب الظاهرية ”الحقيقية“ التي يقدمها Xen أو KVM هو عدم وجود نواة ثانية؛ فالحاوية تستخدم نواة النظام نفسها تماماً. ينطوي هذا على محاسن ومساوئ: من المزايا الأداء الممتاز لعدم وجود عبئ حقيقي، والواقع أن النواة ترى جميع العمليات الجارية في النظام، وبالتالي فإن جدولة المهام ستكون أكثر فعالية مما لو استخدمنا نواتين مستقلتين وكل منهما ستجدول مجموعة مختلفة من المهام. أول العيوب هو استحالة استخدام نواة مختلفة في الحاوية (سواء نسخة مختلفة من لينكس أو نظام تشغيل مختلف بالكامل)."

msgid "<emphasis>NOTE</emphasis> LXC isolation limits"
msgstr "<emphasis>ملاحظة</emphasis> حدود العزل في LXC"

msgid "LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:"
msgstr "حاويات LXC لا توفر درجة العزل التي تحصل عليها عند استخدام محاكيات أو حلول حوسبة ظاهرية أثقل. على وجه الخصوص:"

msgid "since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;"
msgstr "بما أن النواة مشتركة بين النظام المستضيف والحاويات، فإن العمليات المحجوزة في الحاويات ستبقى تصل لرسائل النواة، ما قد يؤدي لتسرب المعلومات إذا بثت الحاوية الرسائل؛"

msgid "for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;"
msgstr "للأسباب ذاتها، إذا تم اختراق حاوية وتم استغلال ثغرة في النواة، فقد تتأثر الحاويات الأخرى أيضًا؛"

msgid "on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers."
msgstr "في نظام الملفات، تتحقق النواة من الصلاحيات وفقاً للمعرفات العددية للمستخدمين والمجموعات؛ وربما كانت هذه المعرفات تشير لمستخدمين ومجموعات مختلفة حسب الحاوية، ويجب أخذ هذا بعين الاعتبار عند مشاركة أجزاء قابلة للكتابة من نظام الملفات بين عدد من الحاويات."

msgid "Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container."
msgstr "بما أننا نتعامل مع تقنية عزل وليست محاكاة وحسب، فإن إعداد حاويات LXC أعقد من تشغيل مثبت دبيان على جهاز ظاهري. سوف نشرح بعض المتطلبات الأولية، ثم نتجه إلى إعداد الشبكة؛ وبعدها سوف نتمكن من إنشاء النظام الذي سيعمل ضمن الحاوية."

msgid "Preliminary Steps"
msgstr "الخطوات الأولية"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains the tools required to run LXC, and must therefore be installed."
msgstr "تحوي الحزمة <emphasis role=\"pkg\">lxc</emphasis> الأدوات اللازمة لتشغيل LXC، ويجب تثبيتها إذن."

msgid "LXC also requires the <emphasis>control groups</emphasis> configuration system, which is a virtual filesystem to be mounted on <filename>/sys/fs/cgroup</filename>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration."
msgstr "يحتاج LXC أيضاً لنظام مجموعات التحكم <emphasis>control groups</emphasis> للإعداد، وهو نظام ملفات ظاهري يتم ربطه على <filename dir=\"ltr\">/sys/fs/cgroup</filename>. بما أن دبيان 8 قد انتقلت إلى systemd، الذي يعتمد أيضاً على مجموعات التحكم، فهذا يتم تلقائياً أثناء الإقلاع دون الحاجة لأي عمليات إضافية."

msgid "Network Configuration"
msgstr "إعداد الشبكة"

#, fuzzy
#| msgid "The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgid "The goal of installing LXC is to set up virtual machines; while we could, of course, keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgstr "الهدف من تثبيت LXC هو إعداد أجهزة ظاهرية؛ وفي حين أننا نستطيع تركها معزولة عن الشبكة طبعاً، والتخاطب معها عبر نظام الملفات فقط، إلا أن معظم حالات الاستخدام تحتاج إعطاء الحاويات وصولاً محدوداً للشبكة على الأقل. في الحالة النموذجية، كل حاوية سيكون لها واجهة شبكية ظاهرية، تتصل بالشبكة الحقيقية عبر جسر. يمكن وصل هذه الواجهة الظاهرية إما مباشرة مع الواجهة الشبكية الفيزيائية للمستضيف (وفي تلك الحالة تتصل الحاوية مباشرة بالشبكة)، أو مع واجهة ظاهرية أخرى معرفة لدى المستضيف (ويمكن للمستضيف بعدها توجيه حركة الشبكة أو حجبها). في كلا الحالتين، سوف نحتاج للحزمة <emphasis role=\"pkg\">bridge-utils</emphasis>."

#, fuzzy
#| msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance, <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgstr "أبسط حالة تتلخص بتحرير <filename dir=\"ltr\">/etc/network/interfaces</filename>، ونقل إعدادات الواجهة الفيزيائية (<literal>eth0</literal> مثلاً) إلى واجهة جسرية (عادة <literal>br0</literal>)، وإعداد الوصلة بينهما. على سبيل المثال، إذا كان ملف إعداد الواجهة الشبكية في البداية يحوي مدخلات تشبه ما يلي:"

msgid ""
"auto eth0\n"
"iface eth0 inet dhcp"
msgstr ""
"auto eth0\n"
"iface eth0 inet dhcp"

msgid "They should be disabled and replaced with the following:"
msgstr "فيجب تعطيلها واستبدالها بالتالي:"

msgid ""
"#auto eth0\n"
"#iface eth0 inet dhcp\n"
"\n"
"auto br0\n"
"iface br0 inet dhcp\n"
"  bridge-ports eth0"
msgstr ""
"#auto eth0\n"
"#iface eth0 inet dhcp\n"
"\n"
"auto br0\n"
"iface br0 inet dhcp\n"
"  bridge-ports eth0"

msgid "The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <literal>eth0</literal> as well as the interfaces defined for the containers."
msgstr "إن نتيجة هذا الإعداد ستشبه ما نحصل عليه لو كانت الحاويات أجهزة تتصل بشبكة المستضيف الفيزيائية نفسها. يدير الإعداد ”الجسري“ حركة إطارات الإيثرنت بين جميع الواجهات المجسَّرة، بما فيها الواجهة الفيزيائية <literal>eth0</literal> بالإضافة للواجهات الظاهرية المعرفة في الحاويات."

#, fuzzy
#| msgid "In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgid "In cases where this configuration cannot be used (for instance, if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgstr "في الحالات التي لا يمكن فيها استخدام هذا الإعداد (مثلاً إذا لم يكن هناك مجال لتعيين عناوين IP عامة للحاويات)، سيتم إنشاء واجهة <emphasis>tap</emphasis> ظاهرية ووصلها مع الجسر. عندها يصبح مخطط الشبكة الموافق لهذا الإعداد هو كأن المستضيف له بطاقة شبكة إضافية متصلة بتحويلة (switch) منفصلة، والحاويات تتصل أيضًا بتلك التحويلة. على المستضيف عندها العمل كبوابة للحاويات إذا كانت تريد التواصل مع العالم الخارجي."

msgid "In addition to <emphasis role=\"pkg\">bridge-utils</emphasis>, this “rich” configuration requires the <emphasis role=\"pkg\">vde2</emphasis> package; the <filename>/etc/network/interfaces</filename> file then becomes:"
msgstr "هذا الإعداد ”الغني“ يحتاج –بالإضافة إلى حزمة <emphasis role=\"pkg\">bridge-utils</emphasis>– إلى الحزمة <emphasis role=\"pkg\">vde2</emphasis>؛ عندئذ يصبح ملف <filename dir=\"ltr\">/etc/network/interfaces</filename> كما يلي:"

msgid ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"
msgstr ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0\n"

msgid "The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <literal>br0</literal> interface."
msgstr "بعدها يمكن إعداد الشبكة إما ستاتيكيًا في الحاويات، أو ديناميكيًا باستخدام مخدم DHCP يعمل على المستضيف. إذا استخدم مخدم DHCP فيجب إعداده لإجابة الطلبات على الواجهة <literal>br0</literal>."

msgid "Setting Up the System"
msgstr "إعداد النظام"

msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgstr "دعنا الآن نضبط نظام الملفات الذي ستستخدمه الحاوية. بما أن هذا ”الجهاز الظاهري“ لن يعمل على العتاد مباشرة، فيجب إجراء بعض التعديلات على نظام الملفات حتى يتناسب مع تنظيم أنظمة الملفات القياسية، خصوصاً بالنسبة للنواة والأجهزة والطرفيات. لحسن الحظ، تحوي <emphasis role=\"pkg\">lxc</emphasis> سكربتات تؤتمت معظم عملية الضبط هذه. مثلاً، يمكن استخدام الأوامر التالية (التي تحتاج الحزمتين <emphasis role=\"pkg\">debootstrap</emphasis> و <emphasis role=\"pkg\">rsync</emphasis>) لتثبيت حاوية دبيان:"

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
#| "</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
#| "Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
#| "Downloading debian minimal ...\n"
#| "I: Retrieving Release \n"
#| "I: Retrieving Release.gpg \n"
#| "[...]\n"
#| "Download complete.\n"
#| "Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
#| "[...]\n"
#| "Root password is 'sSiKhMzI', please change !\n"
#| "root@mirwiz:~# </computeroutput>\n"
#| "        "
msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"root@mirwiz:~# </computeroutput>\n"
"        "
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"Root password is 'sSiKhMzI', please change !\n"
"root@mirwiz:~# </computeroutput>\n"
"        "

msgid "Note that the filesystem is initially created in <filename>/var/cache/lxc</filename>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required."
msgstr "لاحظ أن إنشاء نظام الملفات يتم أولاً في <filename dir=\"ltr\">/var/cache/lxc</filename>، ثم ينقل إلى المجلد الوجهة. هذا يسمح بإنشاء حاويات متطابقة أسرع بكثير، نظراً لأنك تحتاج للنسخ فقط لا أكثر."

#, fuzzy
#| msgid "Note that the debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgid "Note that the Debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgstr "لاحظ أيضًا أن سكربت إنشاء قالب دبيان يقبل خيار <option dir=\"ltr\">--arch</option> لتحديد معمارية النظام الذي سيتم تثبيته وخيار <option dir=\"ltr\">--release</option> إذا كنت تريد تثبيت إصدار آخر غير الإصدار المستقر الحالي من دبيان. يمكنك أيضًا ضبط متغير البيئة <literal>MIRROR</literal> ليشير إلى مرآة دبيان محلية."

msgid "The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<filename>/var/lib/lxc/testlxc/config</filename>) and add a few <literal>lxc.network.*</literal> entries:"
msgstr "يحوي نظام الملفات المنشأ حديثاً نظام دبيان أصغري، ولا تملك الحاوية افتراضياً أي واجهة شبكية (عدا واجهة loopback). بما أن هذا غير مرغوب، سوف نعدل ملف إعداد الحاوية (<filename dir=\"ltr\">/var/lib/lxc/testlxc/config</filename>) ونضيف بضعة مدخلات <literal dir=\"ltr\">lxc.network.*</literal>:"

#, fuzzy
#| msgid ""
#| "lxc.network.type = veth\n"
#| "lxc.network.flags = up\n"
#| "lxc.network.link = br0\n"
#| "lxc.network.hwaddr = 4a:49:43:49:79:20"
msgid ""
"lxc.net.0.type = veth\n"
"lxc.net.0.flags = up\n"
"lxc.net.0.link = br0\n"
"lxc.net.0.hwaddr = 4a:49:43:49:79:20"
msgstr ""
"lxc.network.type = veth\n"
"lxc.network.flags = up\n"
"lxc.network.link = br0\n"
"lxc.network.hwaddr = 4a:49:43:49:79:20\n"

msgid "These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <literal>br0</literal> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated."
msgstr "هذه المدخلات تعني، على الترتيب، أنه سيتم إنشاء واجهة شبكية ظاهرية في الحاوية؛ وسيتم تنشيطها آليًا كلما تم تشغيل تلك الحاوية؛ وأنها ستتصل تلقائياً بالجسر <literal>br0</literal> على المستضيف؛ وأن عنوان MAC الخاص بها سيكون كما هو محدد. إذا كانت هذه المدخلة الأخيرة ناقصة أو معطلة، سيتم توليد عنوان MAC عشوائي."

msgid "Another useful entry in that file is the setting of the hostname:"
msgstr "من المدخلات المفيدة أيضًا التي يمكن إضافتها لهذا الملف هي تعيين اسم المستضيف hostname:"

#, fuzzy
#| msgid "lxc.utsname = testlxc"
msgid "lxc.uts.name = testlxc"
msgstr "lxc.utsname = testlxc\n"

msgid "Starting the Container"
msgstr "تشغيل الحاوية"

#, fuzzy
#| msgid "Now that our virtual machine image is ready, let's start the container:"
msgid "Now that our virtual machine image is ready, let's start the container with <command>lxc-start --daemon --name=testlxc</command>."
msgstr "الآن وبعد أن أصبحت صورة الجهاز الظاهري جاهزة، دعنا نشغل الحاوية:"

msgid "In LXC releases following 2.0.8, root passwords are not set by default. We can set one running <command>lxc-attach -n testlxc <replaceable>passwd</replaceable>.</command> Now we can login:"
msgstr ""

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
#| "</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
#| "</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
#| "\n"
#| "testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
#| "Password: \n"
#| "Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
#| "\n"
#| "The programs included with the Debian GNU/Linux system are free software;\n"
#| "the exact distribution terms for each program are described in the\n"
#| "individual files in /usr/share/doc/*/copyright.\n"
#| "\n"
#| "Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
#| "permitted by applicable law.\n"
#| "root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
#| "<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
#| "root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
#| "root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
#| "root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
#| "root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
#| "root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
#| "root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
#| "root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
#| "root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
#| "root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
#| "root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
#| "root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
#| "root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
#| "root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
#| "root@testlxc:~# </computeroutput>"
msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 9 testlxc console\t\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 4.19.0-5-amd64 #1 SMP Debian 4.19.37-5 (2019-06-19) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  56736  6608 ?        Ss   09:28   0:00 /sbin/init\n"
"root        32  0.0  0.1  46096  4680 ?        Ss   09:28   0:00 /lib/systemd/systemd-journald\n"
"root        75  0.0  0.1  67068  3328 console  Ss   09:28   0:00 /bin/login --\n"
"root        82  0.0  0.1  19812  3664 console  S    09:30   0:00  \\_ -bash\n"
"root        88  0.0  0.1  38308  3176 console  R+   09:31   0:00      \\_ ps auxwf\n"
"root        76  0.0  0.1  69956  5636 ?        Ss   09:28   0:00 /usr/sbin/sshd -D\n"
"root@testlxc:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
"</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
"root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
"root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
"root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
"root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
"root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
"root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
"root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
"root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
"root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
"root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
"root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
"root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
"root@testlxc:~# </computeroutput>"

msgid "We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<filename>/var/lib/lxc/testlxc/rootfs</filename>). We can exit the console with <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."
msgstr "نحن الآن داخل الحاوية؛ ووصولنا إلى العمليات مقيد بالعمليات التي بدأت من داخل الحاوية نفسها، كما أن وصولنا إلى نظام الملفات مقيد إلى المجموعة الجزئية المعينة لهذه الحاوية من نظام الملفات الكامل (<filename dir=\"ltr\">/var/lib/lxc/testlxc/rootfs</filename>). يمكننا الخروج من الطرفية باستخدام <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."

msgid "Note that we ran the container as a background process, thanks to the <option>--daemon</option> option of <command>lxc-start</command>. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgstr "لاحظ أننا بدأنا الحاوية كعملية في الخلفية، بفضل الخيار <option dir=\"ltr\">--daemon</option> للأمر <command>lxc-start</command>. يمكننا مقاطعة الحاوية بالأمر <command>lxc-stop --name=testlxc</command>."

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <command>lxc-autostart</command> which starts containers whose <literal>lxc.start.auto</literal> option is set to 1). Finer-grained control of the startup order is possible with <literal>lxc.start.order</literal> and <literal>lxc.group</literal>: by default, the initialization script first starts containers which are part of the <literal>onboot</literal> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <literal>lxc.start.order</literal> option."
msgstr "تحوي الحزمة <emphasis role=\"pkg\">lxc</emphasis> سكربت تهيئة يستطيع تشغيل حاوية واحدة أو أكثر آلياً عند إقلاع المستضيف (يعتمد السكربت على أمر <command>lxc-autostart</command> الذي يشغل كل الحاويات التي يكون خيار <literal>lxc.start.auto</literal> فيها مضبوطاً على القيمة 1). يمكن التحكم بدقة أكبر بترتيب التشغيل من خلال <literal>lxc.start.order</literal> و<literal>lxc.group</literal>: افتراضياً، يبدأ السكربت أولاً بتشغيل الحاويات التي تنتمي للمجموعة <literal>onboot</literal> ثم الحاويات التي لا تنتمي لأي مجموعة. وفي كلا الحالتين، يتحدد الترتيب فيما بين أعضاء المجموعة الواحدة من خلال الخيار <literal>lxc.start.order</literal>."

msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgstr "<emphasis>التعمق أكثر</emphasis> المحاكاة العملاقة"

msgid "Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <literal>tap</literal> and <literal>veth</literal> interfaces should be enough in many cases."
msgstr "بما أن LXC هو نظام عزل خفيف جداً، يمكن تكييفه للاستضافة الكبيرة للعديد من المخدمات الظاهرية. لعل إعداد الشبكة سيكون أعقد بقليل مما شرحناه هنا، لكن الإعداد ”الغني“ باستخدام واجهات <literal>tap</literal> و<literal>veth</literal> يجب أن يكون كافيًا في العديد من الحالات."

msgid "It may also make sense to share part of the filesystem, such as the <filename>/usr</filename> and <filename>/lib</filename> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <literal>lxc.mount.entry</literal> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage."
msgstr "ربما كان مناسباً أيضًا مشاركة أجزاء من نظام الملفات، مثل <filename dir=\"ltr\">/usr</filename> و<filename dir=\"ltr\">/lib</filename>، لتفادي تكرار البرمجيات المشتركة بين عدة حاويات. هذا يحقق عادة باستخدام مدخلات <literal>lxc.mount.entry</literal> في ملف إعداد الحاويات. هناك أثر جانبي جميل هنا وهو أن العمليات ستستهلك ذاكرة أقل في هذه الحالة، لأن النواة تقدر على اكتشاف البرامج المشتركة. عندئذ يمكن تخفيض الكلفة الهامشية لإضافة حاوية جديدة للمساحة التخزينية المخصصة لبياناتها، والعمليات القليلة الإضافية التي ستديرها النواة وتجدولها."

msgid "We haven't described all the available options, of course; more comprehensive information can be obtained from the <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> and <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual pages and the ones they reference."
msgstr "لم نشرح كافة الخيارات المتاحة، بالطبع؛ يمكنك الحصول على معلومات أوسع من صفحات الكتيبات <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry>‎ و<citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry>‎ والصفحات التي تشيران إليها."

msgid "Virtualization with KVM"
msgstr "المحاكاة في KVM"

msgid "<primary>KVM</primary>"
msgstr "<primary>KVM</primary>"

msgid "KVM, which stands for <emphasis>Kernel-based Virtual Machine</emphasis>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <command>qemu-*</command> commands: it is still about KVM."
msgstr "KVM، التي ترمز إلى <emphasis>Kernel-based Virtual Machine</emphasis>، هي أولاً وأخيراً وحدة من وحدات النواة توفر معظم البنية التحتية التي يمكن أن يستفيد منها برنامج المحاكاة، لكنها ليست محاكيًا. التحكم الفعلي بالمحاكاة يتم من خلال تطبيق مبني على QEMU. لا تقلق إذا كان هذا القسم يذكر أوامر تبدأ ب <command dir=\"ltr\">qemu-*</command>: نحن لا نزال نتحدث عن KVM."

msgid "Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <filename>/proc/cpuinfo</filename>."
msgstr "لقد دمجت KVM منذ البداية في النواة لينكس، بخلاف نظم المحاكاة الأخرى. اختر مطوروها استغلال مجموعات تعليمات المعالج المخصصة للمحاكاة (Intel-VT و AMD-V)، ما جعل KVM خفيفة الوزن، وأنيقة وغير شرهة للموارد. الجانب السلبي، طبعاً، هو أن KVM لا تعمل إلا على الحواسيب التي تملك معالجات مناسبة. بالنسبة للحواسيب ذات معمارية x86، يمكنك التأكد أن المعالج مناسب عن طريق البحث عن ”vmx“ أو ”svm“ في أعلام المعالج المذكورة في <filename dir=\"ltr\">/proc/cpuinfo</filename>."

msgid "With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization."
msgstr "مع دعم Red Hat النشط لتطوير KVM، فقد أصبحت بشكل أو بآخر المرجع في الحوسبة الظاهرية في لينكس."

msgid "<primary><command>virt-install</command></primary>"
msgstr "<primary><command>virt-install</command></primary>"

msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgstr "بعكس الأدوات الأخرى مثل VirtualBox، لا تقدم KVM نفسها أي واجهة للمستخدم لإنشاء وإدارة الحواسيب الظاهرية. تقدم حزمة <emphasis role=\"pkg\">qemu-kvm</emphasis> برنامجاً تنفيذيًا قادراً على تشغيل حاسوب ظاهري، بالإضافة إلى سكربت تهيئة يحمل وحدات النواة المناسبة."

msgid "<primary>libvirt</primary>"
msgstr "<primary>libvirt</primary>"

msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <command>virtual-manager</command> is a graphical interface that uses libvirt to create and manage virtual machines."
msgstr "لحسن الحظ، توفر Red Hat أيضًا مجموعة أخرى من الأدوات لمعالجة هذه المشكلة، من خلال تطوير المكتبة <emphasis>libvirt</emphasis> وأدوات <emphasis>virtual machine manager</emphasis> المقترنة بها. تسمح libvirt بإدارة الحواسيب الظاهرية بأسلوب قياسي، بغض النظر عن نظام المحاكاة المستخدم وراء الكواليس (حاليًا هناك دعم لنظم QEMU، وKVM، وXen، وLXC، وOpenVZ، وVirtualBox، وVMWare وأيضاً UML). ‏<command>virtual-manager</command> هي واجهة رسومية تعتمد على libvirt لإنشاء وإدارة الحواسيب الظاهرية."

msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "We first install the required packages, with <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-bin</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgid "We first install the required packages, with <command>apt-get install libvirt-clients libvirt-daemon-system qemu-kvm virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-daemon-system</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. <emphasis role=\"pkg\">libvirt-clients</emphasis> provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgstr "سوف نثبت الحزم المطلوبة أولاً، بالأمر <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. تقدم الحزمة <emphasis role=\"pkg\">libvirt-bin</emphasis> الخدمة <command>libvirtd</command>، التي تسمح بالإدارة (البعيدة ربما) للحواسيب الظاهرية التي تعمل على المستضيف، وتشغيل الحواسيب الظاهرية المناسبة عند إقلاع المستضيف. بالإضافة لذلك، توفر هذه الحزمة أداة <command>virsh</command> ذات الواجهة النصية، التي تسمح بالتحكم بالإجهزة التي تديرها خدمة <command>libvirtd</command>."

msgid "The <emphasis role=\"pkg\">virtinst</emphasis> package provides <command>virt-install</command>, which allows creating virtual machines from the command line. Finally, <emphasis role=\"pkg\">virt-viewer</emphasis> allows accessing a VM's graphical console."
msgstr "تقدم الحزمة <emphasis role=\"pkg\">virtinst</emphasis> الأداة <command>virt-install</command>، التي تسمح بإنشاء الحواسيب الظاهرية من سطر الأوامر. أخيراً، يسمح <emphasis role=\"pkg\">virt-viewer</emphasis> بالوصول إلى الطرفية الرسومية للحاسب الظاهري."

msgid "Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <xref linkend=\"sect.lxc.network\" />)."
msgstr "كما في Xen و LXC، أكثر الخيارات شيوعاً عند إعداد الشبكة هو استخدام جسر يجمع الواجهات الشبكية لعدة حواسيب ظاهرية (انظر <xref linkend=\"sect.lxc.network\" />)."

msgid "Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network."
msgstr "أو يمكن، كما هو الإعداد الافتراضي الذي تقدمه KVM، إعطاء الحاسب الظاهري عنوناً داخلياً (ضمن المجال 192.168.122.0/24)، وإعداد NAT حتى يتمكن الجهاز الظاهري من الوصول إلى الشبكة الخارجية."

msgid "The rest of this section assumes that the host has an <literal>eth0</literal> physical interface and a <literal>br0</literal> bridge, and that the former is connected to the latter."
msgstr "سنفترض في تتمة هذا القسم أن المستضيف لديه واجهة فيزيائية <literal>eth0</literal> وجسر <literal>br0</literal>، وأن الأولى متصلة مع الأخير."

msgid "Installation with <command>virt-install</command>"
msgstr "التثبيت باستخدام <command>virt-install</command>"

msgid "Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line."
msgstr "يشبه إنشاء حاسب ظاهري تثبيت النظم العادية كثيراً، عدا أن مواصفات الحواسب الظاهري تُحدَّد في أمر طويل جداً."

msgid "Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <xref linkend=\"sect.remote-desktops\" /> for details), which will allow us to control the installation process."
msgstr "عملياً، هذا يعني أننا سنستخدم برنامج تثبيت دبيان، من خلال إقلاع الحاسب الظاهري من سواقة DVD-ROM ظاهرية ترتبط مع صورة DVD دبيان مخزنة على النظام المستضيف. سوف يُصدِّر الجهاز الظاهري طرفيته الرسومية عبر بروتوكول VNC (انظر <xref linkend=\"sect.remote-desktops\" /> للتفاصيل)، ما يسمح لنا بالتحكم بعملية التثبيت."

msgid "We first need to tell libvirtd where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgstr "نحتاج أولاً إخبار libvirtd عن موقع تخزين صور الأقراص، ما لم يكن الموقع الافتراضي (<filename>/var/lib/libvirt/images/</filename>) مناسباً."

msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"

msgid "<emphasis>TIP</emphasis> Add your user to the libvirt group"
msgstr "<emphasis>تلميح</emphasis> أضف مستخدمك إلى المجموعة libvirt"

#, fuzzy
#| msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yourself to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgstr "تفترض كافة الأمثلة في هذا القسم أنك تنفذ الأوامر بصلاحيات الجذر. وبالتالي، إذا أردت التحكم بخدمة libvirt محلية، عليك إما أن تكون الجذر أو أن تكون عضواً في المجموعة <literal>libvirt</literal> (الحالة الافتراضية هي أنك لا تنتمي لهذه المجموعة). فإذا أردت تفادي استخدام صلاحيات الحذر كثيراً، يمكنك إضافة نفسك إلى المجموعة <literal>libvirt</literal> وتنفيذ الأوامر المختلفة بصلاحيات مستخدمك العادي."

msgid "Let us now start the installation process for the virtual machine, and have a closer look at <command>virt-install</command>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed."
msgstr "دعنا نبدأ الآن عملية تثبيت الحاسب الظاهري، وإلقاء نظرة قريبة على أهم خيارات <command>virt-install</command>. هذا الأمر يسجل الجهاز الظاهري وبارامتراته عند libvirtd، ثم يشغله حتى نتابع عملية التثبيت."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
#| "               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
#| "               --name testkvm            <co id=\"virtinst.name\"></co>\n"
#| "               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
#| "               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
#| "               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
#| "               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
#| "               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
#| "               --os-type linux           <co id=\"virtinst.os\"></co>\n"
#| "               --os-variant debianwheezy\n"
#| "</userinput><computeroutput>\n"
#| "Starting install...\n"
#| "Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
#| "Creating domain...                    |    0 B     00:00\n"
#| "Guest installation complete... restarting guest.\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --memory 1024             <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-10.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n"
"               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debian10\n"
"</userinput><computeroutput>\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
"               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debianwheezy\n"
"</userinput><computeroutput>\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
"Creating domain...                    |    0 B     00:00\n"
"Guest installation complete... restarting guest.\n"
"</computeroutput>"

msgid "The <literal>--connect</literal> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<literal>/system</literal>) from others (<literal>/session</literal>)."
msgstr "يحدد خيار <literal dir=\"ltr\">--connect</literal> ”المشرف“ المستخدم. شكله هو شكل URL يحوي اسم نظام المحاكاة (<literal dir=\"ltr\">xen://</literal>‏، <literal dir=\"ltr\">qemu://</literal>‏، <literal dir=\"ltr\">lxc://</literal>‏، <literal dir=\"ltr\">openvz://</literal>‏، <literal dir=\"ltr\">vbox://</literal>، وهكذا) والحاسب الذي يجب أن يستضيف الجهاز الظاهري (يمكن ترك هذا فارغًا في حالة  الاستضافة المحلية). لالإضافة لذلك، في حالة استخدام QEMU/KVM، يستطيع كل مستخدم إدارة الحواسيب الظاهرية ولكن بصلاحيات مقيدة، ويسمح مسار URL بتمييز حواسيب ”النظام“ (<literal dir=\"ltr\">/system</literal>) من الحواسيب الظاهرية (<literal dir=\"ltr\">/session</literal>)."

msgid "Since KVM is managed the same way as QEMU, the <literal>--virt-type kvm</literal> allows specifying the use of KVM even though the URL looks like QEMU."
msgstr "بما أن طريقة إدارة KVM تطابق طريقة إدارة QEMU، فإن الخيار <literal dir=\"ltr\">--virt-type kvm</literal> يسمح بتحديد استخدام KVM بالرغم من أن URL يبدو وكأنه QEMU."

msgid "The <literal>--name</literal> option defines a (unique) name for the virtual machine."
msgstr "خيار <literal dir=\"ltr\">--name</literal> يحدد اسمًا (فريداً) للجهاز الظاهري."

#, fuzzy
#| msgid "The <literal>--ram</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgid "The <literal>--memory</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgstr "يسمح خيار <literal dir=\"ltr\">--ram</literal> بتحديد كمية الذاكرة (بالميغابايت) المخصصة للجهاز الظاهري."

#, fuzzy
#| msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>raw</literal>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space."
msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>qcow2</literal>) allows starting with a small file that only grows when the virtual machine starts actually using space."
msgstr "يحدد <literal dir=\"ltr\">--disk</literal> موقع ملف الصورة التي تمثل القرص الصلب لجهازنا الظاهري؛ سوف يتم إنشاء ذلك الملف –ما لم يكن موجوداً مسبقاً– بالحجم المحدد بالبارامتر <literal>size</literal> (بالغيغابايت). يسمح المتغير <literal>format</literal> باختيار إحدى الصيغ المتعددة لتخزين ملفات الصور. الصيغة الافتراضية (<literal>raw</literal>) هي ملف وحيد يطابق القرص بالحجم والمحتويات تماماً. لقد اخترنا صيغة متقدمة أكثر هنا، هذه الصيغة خاصة بـ QEMU وهي تسمح بالبدء مع ملف صغير يكبر فقط عندما يبدأ الجهاز الظاهري باستهلاك المساحة فعلاً."

msgid "The <literal>--cdrom</literal> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <literal>/dev/cdrom</literal>)."
msgstr "يستخدم خيار <literal dir=\"ltr\">--cdrom</literal> للإشارة إلى موقع القرص الضوئي المستخدم للتثبيت. يمكن أن يكون المسار مساراً محلياً لصورة ISO، أو URL يمكن الحصول منه على الملف، أو ملف جهاز يمثل سواقة CD-ROM فيزيائية (مثل <literal dir=\"ltr\">/dev/cdrom</literal>)."

msgid "The <literal>--network</literal> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24)."
msgstr "يحدد <literal dir=\"ltr\">--network</literal> طريقة دمج بطاقة الشبكة الظاهرية في إعدادات الشبكة في المستضيف. السلوك الافتراضي (الذي  حددنا استخدامه صراحة في مثالنا) هو دمجها في أي جسر شبكي سابق. إذا لم يكن هناك أي جسر من قبل، فلن يستطيع الجهاز الظاهري الوصول إلى الشبكة الفيزيائية إلا من خلال NAT، لذلك يأخذ عنواناً ضمن مجال شبكة فرعية داخلية (192.168.122.0/24)."

#, fuzzy
#| msgid "<literal>--vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, the <literal>--vnclisten=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgid "<literal>--graphics vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, <literal>--graphics vnc,listen=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgstr "يصرح <literal dir=\"ltr\">--vnc</literal> أن الطرفية الرسومية يجب أن تكون متاحة عبر استخدام VNC. السلوك الافتراضي لمخدم VNC المرفق هو الإنصات إلى الواجهة المحلية فقط؛ إذا كان عميل VNC سيعمل على حاسب آخر، فإن الاتصال يحتاج لإعداد نفق SSH (انظر <xref linkend=\"sect.ssh-port-forwarding\" />). أو يمكن استخدام <literal dir=\"ltr\">--vnclisten=0.0.0.0</literal> حتى يصبح الوصول لمخدم VNC ممكناً من جميع الواجهات؛ لكن انتبه إلى أنك إذا استخدمت هذا الخيار، فعليك تصميم الجدار الناري بما يتناسب معه."

msgid "The <literal>--os-type</literal> and <literal>--os-variant</literal> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there."
msgstr "يسمح الخياران <literal dir=\"ltr\">--os-type</literal> و<literal dir=\"ltr\">--os-variant</literal> بتحسين بعض متغيرات الجهاز الظاهري، اعتماداً على بعض المزايا المعروفة لنظام التشغيل المذكور هنا."

msgid "At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <command>virt-viewer</command> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):"
msgstr "عند هذه النقطة، بدأ الجهاز الظاهري يعمل، ونحتاج الاتصال بالطرفية الرسومية  لمتابعة عملية التثبيت. إذا تم تنفيذ العملية السابقة من بيئة سطح مكتب رسومية، فيجب أن يبدأ هذا الاتصال آلياً. إذا لم يحدث هذا، أو إذا كنا نعمل عن بعد، يمكن تشغيل <command>virt-viewer</command> من أي بيئة رسومية لفتح الطرفية الرسومية (لاحظ أن كلمة سر الجذر للنظام البعيد ستطلب مرتين لأن العملية تحتاج لاتصالي SSH):"

msgid ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>\n"

msgid "When the installation process ends, the virtual machine is restarted, now ready for use."
msgstr "عند انتهاء عملية التثبيت، تتم إعادة تشغيل الجهاز الظاهري، ويصبح جاهزاً عند ذلك للاستخدام."

msgid "Managing Machines with <command>virsh</command>"
msgstr "إدارة الأجهزة باستخدام <command>virsh</command>"

msgid "<primary><command>virsh</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <command>libvirtd</command> for the list of the virtual machines it manages:"
msgstr "بعد أن انتهينا من التثبيت، دعنا نرى كيف ندير الأجهزة الظاهرية المتوفرة. أول شيئ سنجربه هو طلب قائمة بالأجهزة التي تديرها <command>libvirtd</command>:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
#| " Id Name                 State\n"
#| "----------------------------------\n"
#| "  - testkvm              shut off\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  8 testkvm              shut off\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all</userinput>\n"
" Id Name                 State\n"
"----------------------------------\n"
"  - testkvm              shut off\n"
"\n"

msgid "Let's start our test virtual machine:"
msgstr "دعنا نبدأ تشغيل جهازنا التجريبي:"

msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>\n"

msgid "We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <command>vncviewer</command>):"
msgstr "يمكننا الآن الحصول على تعليمات الاتصال بالطرفية الرسومية (يمكن تمرير لوحة عرض VNC المعادة كمتغير للبرنامج <command>vncviewer</command>):"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
#| "</userinput><computeroutput>:0</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>127.0.0.1:0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>:0</computeroutput>\n"

msgid "Other available <command>virsh</command> subcommands include:"
msgstr "من أوامر <command>virsh</command> الفرعية المتاحة أيضاً:"

msgid "<literal>reboot</literal> to restart a virtual machine;"
msgstr "<literal>reboot</literal> لإعادة إقلاع الجهاز الظاهري؛"

msgid "<literal>shutdown</literal> to trigger a clean shutdown;"
msgstr "<literal>shutdown</literal> لبدء عملية إيقاف تشغيل نظيفة؛"

msgid "<literal>destroy</literal>, to stop it brutally;"
msgstr "<literal>destroy</literal>، لإيقاف عمل الجهاز الظاهري قسراً؛"

msgid "<literal>suspend</literal> to pause it;"
msgstr "<literal>suspend</literal> لإيقاف عمله مؤقتاً؛"

msgid "<literal>resume</literal> to unpause it;"
msgstr "<literal>resume</literal> لاستكمال عمله؛"

msgid "<literal>autostart</literal> to enable (or disable, with the <literal>--disable</literal> option) starting the virtual machine automatically when the host starts;"
msgstr "<literal>autostart</literal> لتفعيل (أو تعطيل، إذا استخدم الخيار <literal dir=\"ltr\">--disable</literal>) تشغيل الجهاز الظاهري تلقائياً عند إقلاع المستضيف؛"

msgid "<literal>undefine</literal> to remove all traces of the virtual machine from <command>libvirtd</command>."
msgstr "<literal>undefine</literal> لإزالة كافة آثار الجهاز الظاهري من <command>libvirtd</command>."

msgid "All these subcommands take a virtual machine identifier as a parameter."
msgstr "جميع هذه الأوامر الفرعية تأخذ الاسم المُعِّرف للجهاز الظاهري كمتغير لها."

msgid "Installing an RPM based system in Debian with yum"
msgstr "تثبيت نظام مبني على RPM في دبيان باستخدام yum"

msgid "If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility (available in the package of the same name)."
msgstr "إذا كان الجهاز الظاهري سيعمل بنظام دبيان (أو أحد مشتقاته)، يمكن تهيئة النظام باستخدام <command>debootstrap</command>، كما شرحناه سابقاً. أما إذا كان الجهاز الظاهري سيعمل بنظام مبني على RPM (مثل فيدورا، أو CentOS أو Scientific Linux)، يجب إتمام التثبيت باستخدام أداة <command>yum</command> (المتوفرة في الحزمة ذات الاسم نفسه)."

msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum</command> to extract the remaining set of packages. But since we call <command>yum</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgstr "تحتاج العملية لاستخدام <command>rpm</command> لاستخراج مجموعة من الملفات، من أهمها ملفات إعداد <command>yum</command>، ثم استدعاء <command>yum</command> لفك الضغط عن بقية الحزم. لكن بما أننا سوف نستدعي <command>yum</command> من خارج chroot، علينا إجراء بعض التغييرات المؤقتة. في المثال التالي، كان chroot الهدف هو <filename dir=\"ltr\">/srv/centos</filename>."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
#| "</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
#| "</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
#| "rpm: However assuming you know what you are doing...\n"
#| "warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
#| "</userinput><computeroutput>[...]\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-6.1810.2.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"

msgid "Automated Installation"
msgstr "التثبيت المؤتمت"

msgid "<primary>deployment</primary>"
msgstr "<primary>تنصيب</primary>"

msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgstr "<primary>تثبيت</primary><secondary>التثبيت المؤتمت</secondary>"

msgid "The Falcot Corp administrators, like many administrators of large IT services, need tools to install (or reinstall) quickly, and automatically if possible, their new machines."
msgstr "يحتاج مدراء النظم في شركة فلكوت، كما هو حال مدراء النظم في العديد من شركات الخدمات التقنية الكبيرة، لأدوات تساعدهم على تثبيت (أو إعادة تثبيت) النظام على الأجهزة الجديدة بسرعة، وبصورة آلية إذا أمكن."

msgid "These requirements can be met by a wide range of solutions. On the one hand, generic tools such as SystemImager handle this by creating an image based on a template machine, then deploy that image to the target systems; at the other end of the spectrum, the standard Debian installer can be preseeded with a configuration file giving the answers to the questions asked during the installation process. As a sort of middle ground, a hybrid tool such as FAI (<emphasis>Fully Automatic Installer</emphasis>) installs machines using the packaging system, but it also uses its own infrastructure for tasks that are more specific to massive deployments (such as starting, partitioning, configuration and so on)."
msgstr "يمكن تلبية هذه الحاجة بطيف واسع من الحلول. فالأدوات العامة مثل SystemImager تعالج هذه القضية بإنشاء صورة بالاعتماد على جهاز نموذجي، ثم نشر تلك الصورة على الأجهزة المستهدفة، وعلى النهاية الأخرى من الطيف، هناك برنامج تثبيت دبيان القياسي الذي يمكن تغذيته بملف إعداد يجيب على الأسئلة المطروحة أثناء عملية التثبيت. وكنوع من الحلول الوسط، يمكن استخدام أداة هجينة مثل FAI (‏<emphasis>Fully Automatic Installer</emphasis>) لتثبيت النظام على الأجهزة باستخدام نظام إدارة الحزم، لكنها تستخدم بنية تحتية خاصة بها للمهام المتعلقة بالنشر واسع النطاق massive deployment (مثل الإقلاع، وتقطيع الأقراص، وإعداد النظام وما شابه)."

msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an i386 image won't fit on a powerpc machine, and so on)."
msgstr "لكل من هذه الأدوات محاسن ومساوئ. يعمل SystemImager بشكل مستقل عن أي نظام حزم معين، وهذا يسمح له بإدارة مجموعات كبيرة من الأجهزة باستخدام عدة توزيعات لينكس مختلفة. كما أنه يتضمن نظام تحديث لا يحتاج إعادة تثبيت النظام، لكن لا يمكن الاعتماد على نظام التحديث هذا إلا إذا لم تعدّل الأجهزة بشكل مستقل؛ أي يجب ألا يحدث المستخدمون وحدهم أي برمجية، كما لا يجب أن يثبتوا برمجيات إضافية. كما يجب عدم أتمتة التحديثات الأمنية، بل يجب أن تمر عبر الصورة المركزية التي يديرها SystemImager. هذا الحل يتطلب أيضًا أن تكون الأجهزة المستهدفة متجانسة، وإلا يجب الاحتفاظ بعدد من الصور المختلفة وإدارتها (صورة i386 لن تتناسب مع جهاز powerpc، وهكذا)."

msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly simple-CDD (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even the simple-CDD solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgstr "أما التثبيت المؤتمت باستخدام مثبت دبيان  فيستطيع التكيف مع خصائص كل جهاز؛ إذ أن المثبت سيجلب النواة والحزم البرمجية المناسبة من المستودعات الموافقة، وسيتعرف على العتاد المتوفر، ويقطع كامل القرص الصلب للاستفادة من كل المساحة التخزينية المتاحة، ثم يثبت نظام دبيان ويعد محمل إقلاع ملائم. لكن المثبت القياسي لا يثبت إلا نسخ دبيان ”القياسية“، التي تحوي النظام الأساسي مع مجموعة من ”المهام“ المحددة مسبقًا؛ وهذا يمنع تثبيت نظام مخصص مع تطبيقات غير محزمة. لتلبية هذا المتطلب بالذات يجب تخصيص المثبت… لحسن الحظ، المثبت تجزيئي كثيراً (modular)، وهناك أدوات لأتمتة معظم العمل المطلوب لهذا التخصيص، أهمها simple-CDD (حيث CDD هي اختصار <emphasis>Custom Debian Derivative</emphasis>—مشتق مخصص من دبيان). وحتى simple-CDD يعالج التثبيت الأولي فقط؛ لكن هذه ليست مشكلة عادة بما أن أدوات APT تسمح بالنشر الفعال للتحديثات لاحقاً."

msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian), in order to focus more intently on debian-installer and simple-CDD, which are more interesting in a Debian-only context."
msgstr "سوف نقدم شرحاً مقتضبًا فقط عن FAI، وسنتجاوز SystemImager بالكامل (الذي لم يعد متوفراً في دبيان)، وذلك للتركيز أكثر على مثبت دبيان وsimple-CDD، وهي الحلول الأكثر جاذبية عند العمل مع نظم دبيان."

msgid "Fully Automatic Installer (FAI)"
msgstr "‏Fully Automatic Installer (FAI)‎"

msgid "<primary>Fully Automatic Installer (FAI)</primary>"
msgstr "<primary>Fully Automatic Installer (FAI)</primary>"

msgid "<foreignphrase>Fully Automatic Installer</foreignphrase> is probably the oldest automated deployment system for Debian, which explains its status as a reference; but its very flexible nature only just compensates for the complexity it involves."
msgstr "لعل <foreignphrase>Fully Automatic Installer</foreignphrase> أقدم نظم النشر المؤتمت لأنظمة دبيان، وهذا ما يفسر ذكر هذه الأداة كثيراً؛ إلا أن طبيعته فائقة المرونة بالكاد تغطي تعقيد استخدامه."

msgid "FAI requires a server system to store deployment information and allow target machines to boot from the network. This server requires the <emphasis role=\"pkg\">fai-server</emphasis> package (or <emphasis role=\"pkg\">fai-quickstart</emphasis>, which also brings the required elements for a standard configuration)."
msgstr "يحتاج FAI لنظام يعمل كمخدم لتخزين معلومات النشر ويسمح للأجهزة المستهدفة بالإقلاع عبر الشبكة. يحتاج هذا المخدم حزمة <emphasis role=\"pkg\">fai-server</emphasis> (أو <emphasis role=\"pkg\">fai-quickstart</emphasis> التي تثبت أيضًا العناصر المطلوبة للإعداد القياسي)."

msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgstr "يستخدمُ FAI أسلوباً خاصاً لتعريف البروفايلات المتنوعة التي يمكن تثبيتها. بدلاً من النسخ البسيط للنظام المرجعي، يوفر FAI مثبتاً متكاملاً يمكن تخصيصه بالكامل عبر مجموعة من الملفات والسكربتات المخزنة على المخدم؛ لا يتم إنشاء الموقع الافتراضي <filename>/srv/fai/config/</filename> آليًا، لذلك يجب أن ينشئه مدير النظام بالإضافة لجميع الملفات اللازمة. في معظم الأحيان تكون هذه الملفات نسخاً مخصصة عن ملفات الأمثلة المتوفرة في الحزمة <emphasis role=\"pkg\">fai-doc</emphasis> وبالأخص في المجلد <filename dir=\"ltr\">/usr/share/doc/fai-doc/examples/simple/</filename>."

#, fuzzy
#| msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start a FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgstr "بعد تعريف البروفايلات، يجب تنفيذ الأمر <command>fai-setup</command> لتوليد العناصر المطلوبة لبدء التثبيت باستخدام FAI؛ هذا يعني تحضير أو تحديث نظام أصغري (NFS-root) يستخدم خلال التثبيت. أو يمكن توليد CD إقلاعي للتثبيت باستخدام <command>fai-cd</command>."

msgid "Creating all these configuration files requires some understanding of the way FAI works. A typical installation process is made of the following steps:"
msgstr "لإنشاء كل ملفات الضبط هذه يجب فهم طريقة عمل FAI. تتألف عملية التثبيت النموذجية من الخطوات التالية:"

msgid "fetching a kernel from the network, and booting it;"
msgstr "إحضار النواة عبر الشبكة، وإقلاعها؛"

msgid "mounting the root filesystem from NFS;"
msgstr "ربط نظام الملفات الجذر عبر NFS‏ (nfsroot المذكور سابقاً)؛"

msgid "executing <command>/usr/sbin/fai</command>, which controls the rest of the process (the next steps are therefore initiated by this script);"
msgstr "تنفيذ <command dir=\"ltr\">/usr/sbin/fai</command> الذ يتحكم بتتمة العملية (أي أن الخطوات التالية سينفذها هذا السكربت)؛"

msgid "copying the configuration space from the server into <filename>/fai/</filename>;"
msgstr "نسخ مساحة الإعداد من المخدم إلى <filename>/fai/</filename>؛"

msgid "running <command>fai-class</command>. The <filename>/fai/class/[0-9][0-9]*</filename> scripts are executed in turn, and return names of “classes” that apply to the machine being installed; this information will serve as a base for the following steps. This allows for some flexibility in defining the services to be installed and configured."
msgstr "استدعاء <command>fai-class</command>. سوف تُنفَّذ السكربتات <filename dir=\"ltr\">/fai/class/[0-9][0-9]*</filename> بالدور، وتعيد أسماء ”الفئات“ (classes) التي يجب تطبيقها على الجهاز الذي تجري عليه عملية التثبيت؛ سوف تعمل هذه المعلومات كأساس للخطوات التالية. هذا يسمح ببعض المرونة في تعريف الخدمات التي سوف تُثبَّت وتُضبَط."

msgid "fetching a number of configuration variables, depending on the relevant classes;"
msgstr "قراءة عدد من متغيرات الضبط، وذلك تبعاً للفئات (classes) المحددة؛"

msgid "partitioning the disks and formatting the partitions, based on information provided in <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;"
msgstr "تقسيم الأقراص وتهيئة الأقسام الناتجة، حسب المعلومات المتوفرة في <filename dir=\"ltr\">/fai/disk_config/<replaceable>class</replaceable></filename>؛"

msgid "mounting said partitions;"
msgstr "ربط الأقسام السابقة؛"

msgid "installing the base system;"
msgstr "تثبيت أساس النظام؛"

msgid "preseeding the Debconf database with <command>fai-debconf</command>;"
msgstr "تغذية قاعدة بيانات Debconf باستخدام <command>fai-debconf</command>؛"

msgid "fetching the list of available packages for APT;"
msgstr "الحصول على قائمة الحزم المتاحة لأداة APT؛"

msgid "installing the packages listed in <filename>/fai/package_config/<replaceable>class</replaceable></filename>;"
msgstr "تثبيت الحزم المذكورة في <filename dir=\"ltr\">/fai/package_config/<replaceable>class</replaceable></filename>؛"

msgid "executing the post-configuration scripts, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;"
msgstr "تنفيذ السكربتات التالية للإعداد، <filename dir=\"ltr\">/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>؛"

msgid "recording the installation logs, unmounting the partitions, and rebooting."
msgstr "حفظ سجلات التثبيت، فصل أقسام الأقراص الصلبة، ثم إعادة الإقلاع؛"

msgid "Preseeding Debian-Installer"
msgstr "تغذية مثبت دبيان"

msgid "<primary>preseed</primary>"
msgstr "<primary>تغذية</primary>"

msgid "<primary>preconfiguration</primary>"
msgstr "<primary>إعداد مسبق</primary>"

msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <emphasis>preseeding</emphasis>."
msgstr "في النهاية، يجب –منطقياً– أن يبقى مُثبِّت دبيان الرسمي أفضل أداة لتثبيت أنظمة دبيان. ولهذا السبب تم تصميم مثبت دبيان منذ البداية للاستخدام المؤتمت، بالاستفادة من مزايا البنية التحتية التي تقدمها <emphasis role=\"pkg\">debconf</emphasis>. تسمح الأخيرة بتقليل عدد الأسئلة المطروحة من جهة (تأخذ الأسئلة المخفية الإجابات الافتراضية آلياً)، ومن جهة أخرى، توفير الإجابات الافتراضية بشكل مستقل، حتى تتاح إمكانية التثبيت غير التفاعلي. هذه الميزة الأخيرة تعرف باسم <emphasis>preseeding</emphasis>—التغذية، التي تعني ”الإعداد المسبق“ ببساطة."

msgid "<emphasis>GOING FURTHER</emphasis> Debconf with a centralized database"
msgstr "<emphasis>التعمق أكثر</emphasis> Debconf مع قاعدة بيانات مركزية"

msgid "<primary><command>debconf</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "Preseeding allows to provide a set of answers to Debconf questions at installation time, but these answers are static and do not evolve as time passes. Since already-installed machines may need upgrading, and new answers may become required, the <filename>/etc/debconf.conf</filename> configuration file can be set up so that Debconf uses external data sources (such as an LDAP directory server, or a remote file accessed via NFS or Samba). Several external data sources can be defined at the same time, and they complement one another. The local database is still used (for read-write access), but the remote databases are usually restricted to reading. The <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual page describes all the possibilities in detail (you need the <emphasis role=\"pkg\">debconf-doc</emphasis> package)."
msgstr "تسمح التغذية بالإجابة على أسئلة Debconf التي تطرحها أثناء التثبيت، لكن هذه الأجوبة ثابتة ولا تتطور بمرور الزمن. بما أن النظم المثبتة مسبقًا قد تحتاج للترقية، وقد تطرح أسئلة جديدة أثناء العملية، فيمكن ضبط ملف الإعداد <filename dir=\"ltr\">/etc/debconf.conf</filename> بحيث تستخدم Debconf مصادر بيانات خارجية (مثل مخدم LDAP directory، أو ملف بعيد تصل إليه عبر NFS أو Samba). يمكن تعريف عدة مصادر خارجية للبيانات في الوقت نفسه، وسوف تكمل هذه المصادر بعضها البعض. ستبقى قاعدة البيانات المحلية قيد الاستخدام (لاستخدامها للقراءة والكتابة)، أما قواعد البيانات الخارجية فتقتصر الصلاحيات فيها على القراءة فقط عادة. تشرح صفحة التعليمات <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry>‎ كافة الاحتمالات بالتفصيل (ستحتاج للحزمة <emphasis role=\"pkg\">debconf-doc</emphasis>)."

msgid "Using a Preseed File"
msgstr "استخدام ملف تغذية"

msgid "There are several places where the installer can get a preseeding file:"
msgstr "يستطيع المُثبّت الحصول على ملف التغذية من العديد من الأماكن:"

msgid "in the initrd used to start the machine; in this case, preseeding happens at the very beginning of the installation, and all questions can be avoided. The file just needs to be called <filename>preseed.cfg</filename> and stored in the initrd root."
msgstr "من initrd المستخدمة لإقلاع الجهاز، في هذه الحالة، تتم التغذية منذ بداية التثبيت الأولية، وسوف يتم تجاوز جميع الأسئلة. يجب فقط تسمية الملف preseed.cfg وتخزينه في جذر initrd."

msgid "on the boot media (CD or USB key); preseeding then happens as soon as the media is mounted, which means right after the questions about language and keyboard layout. The <literal>preseed/file</literal> boot parameter can be used to indicate the location of the preseeding file (for instance, <filename>/cdrom/preseed.cfg</filename> when the installation is done off a CD-ROM, or <filename>/hd-media/preseed.cfg</filename> in the USB-key case)."
msgstr "من وسيط الإقلاع (CD أو مفتاح USB)؛ وتحدث التغذية فور ربط الوسيط التخزيني، أي  مباشرة بعد السؤال عن اللغة وتخطيط لوحة المفاتيح. يمكن استخدام متغير الإقلاع <literal>preseed/file</literal> للإشارة إلى موقع ملف التغذية (مثلا، <filename dir=\"ltr\">/cdrom/preseed.cfg</filename> عند التثبيت من قرص CD-ROM، أو <filename dir=\"ltr\">/hd-media/preseed.cfg</filename> في حال استخدام مفتاح USB)."

msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."
msgstr "من الشبكة؛ عندها لا تتم التغذية إلا بعد إعداد الشبكة (الأوتوماتيكي)؛ عندها يجب استخدام متغير الإقلاع <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."

msgid "At a glance, including the preseeding file in the initrd looks like the most interesting solution; however, it is rarely used in practice, because generating an installer initrd is rather complex. The other two solutions are much more common, especially since boot parameters provide another way to preseed the answers to the first questions of the installation process. The usual way to save the bother of typing these boot parameters by hand at each installation is to save them into the configuration for <command>isolinux</command> (in the CD-ROM case) or <command>syslinux</command> (USB key)."
msgstr "كنظرة أولية، يبدو تضمين ملف التغذية في initrd أنه الحل الأكثر جاذبية؛ لكنه نادراً ما يستخدم عملياً، لأن توليد initrd للمثبت معقد جداً. الحلين الآخرين أكثر انتشاراً بكثير، خصوصًا أنك تستطيع استخدام المتغيرات الإقلاعية كطريق بديل لتغذية الأسئلة الأولى لعملية التثبيت. جرت العادة أن تحفظ هذه المتغيرات في إعدادات <command>isolinux</command> (في حال استخدام CD-ROM) أو <command>syslinux</command> (ذاكرة USB) بدلاً من كتابتها يدوياً عند كل عملية تثبيت."

msgid "Creating a Preseed File"
msgstr "إنشاء ملف التغذية"

msgid "A preseed file is a plain text file, where each line contains the answer to one Debconf question. A line is split across four fields separated by whitespace (spaces or tabs), as in, for instance, <literal>d-i mirror/suite string stable</literal>:"
msgstr "ملف التغذية هو ملف نصي عادي، كل سطر منه يحوي إجابة لسؤال واحد من أسئلة Debconf. يفصل السطر إلى أربعة أقسام تفصلها مسافات بيضاء (علامة مسافة space أو علامة جدولة tab)، فمثلاً <literal>d-i mirror/suite string stable</literal>:"

msgid "the first field is the “owner” of the question; “d-i” is used for questions relevant to the installer, but it can also be a package name for questions coming from Debian packages;"
msgstr "الحقل الأول هو ”صاحب“ السؤال؛ تستخدم ”d-i“ للأسئلة المتعلقة بالمثبت، لكن يمكن أن تكتب اسم حزمة للأسئلة التي تطرحها حزم دبيان؛"

msgid "the second field is an identifier for the question;"
msgstr "الحقل الثاني هو معرف للسؤال؛"

msgid "third, the type of question;"
msgstr "الثالث، نوع السؤال؛"

msgid "the fourth and last field contains the value for the answer. Note that it must be separated from the third field with a single space; if there are more than one, the following space characters are considered part of the value."
msgstr "الحقل الرابع والأخير يحوي قيمة الإجابة. لاحظ أن هذا الحقل يجب فصله عن سابقه بمسافة واحدة؛ وإذا كان هناك أكثر من واحدة ستعتبر المسافات اللاحقة جزءاً من الإجابة."

msgid "The simplest way to write a preseed file is to install a system by hand. Then <command>debconf-get-selections --installer</command> will provide the answers concerning the installer. Answers about other packages can be obtained with <command>debconf-get-selections</command>. However, a cleaner solution is to write the preseed file by hand, starting from an example and the reference documentation: with such an approach, only questions where the default answer needs to be overridden can be preseeded; using the <literal>priority=critical</literal> boot parameter will instruct Debconf to only ask critical questions, and use the default answer for others."
msgstr "أبسط طريقة لكتابة ملف تغذية هي تثبيت النظام يدوياً. ثم يعطيك الأمر <command>debconf-get-selections --installer</command> الإجابات المتعلقة بالمثبت. يمكن الحصول على الإجابات المتعلقة بالحزم الأخرى بالأمر <command>debconf-get-selections</command>. لكن الحل الأفضل هو أن تكتب ملف التغذية يدوياً، بالاعتماد على مثال وعلى الوثائق: بهذا الشكل يمكن تغذية الأسئلة التي تحتاج تغيير إجاباتها الافتراضية فقط؛ واستخدام متغير الإقلاع <literal>priority=critical</literal> سوف يفرض على Debconf أن تطرح الأسئلة الحرجة فقط، وأن تستخدم الإجابات الافتراضية لبقية الأسئلة."

msgid "<emphasis>DOCUMENTATION</emphasis> Installation guide appendix"
msgstr "<emphasis>توثيق</emphasis> الملحق في دليل التثبيت"

#, fuzzy
#| msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"
msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/apb\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/example-preseed.txt\" />"
msgstr "يتضمن دليل التثبيت، المتاح على شبكة الإنترنت، توثيقاً مفصلاً عن استخدام ملفات التغذية في ملحق خاص. كما يتضمن مثالاً عن ملف تغذية مفصلاً ومزوداً بالتعليقات، يمكن الاستفادة منه كأساس للتخصيصات المحلية. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"

msgid "Creating a Customized Boot Media"
msgstr "إنشاء وسيط إقلاعي مخصص"

msgid "Knowing where to store the preseed file is all very well, but the location isn't everything: one must, one way or another, alter the installation boot media to change the boot parameters and add the preseed file."
msgstr "من الجيد أن يعرف المرء مكان تخزين ملف التغذية، لكن مكان التخزين ليس كل شيء: يجب تعديل وسيط الإقلاع –بشكل أو بآخر– لتغيير متغيرات الإقلاع وإضافة ملف التغذية."

msgid "Booting From the Network"
msgstr "الإقلاع من الشبكة"

#, fuzzy
#| msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"
msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/ch04s05\" />"
msgstr "عند إقلاع الحاسب من الشبكة، يعرف المخدم الذي يرسل عناصر التهيئة متغيرات الإقلاع أيضاً. أي يجب أن يتم التعديل على إعداد PXE لمخدم الإقلاع؛ وبالتحديد أكثر، في ملف الإعداد <filename dir=\"ltr\">/tftpboot/pxelinux.cfg/default</filename>. إن إعداد الإقلاع عبر الشبكة هو متطلب أساسي؛ انظر دليل التثبيت لمزيد من التفاصيل. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"

msgid "Preparing a Bootable USB Key"
msgstr "تحضير ذاكرة USB إقلاعية"

msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>:"
msgstr "بعد تجهيز الذاكرة الإقلاعية (انظر <xref linkend=\"sect.install-usb\" />)، يجب تنفيذ بعض العمليات الإضافية. على فرض أن محتويات الذاكرة متاحة في <filename>/media/usbdisk/</filename>:"

msgid "copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>"
msgstr "انسخ ملف التغذية إلى <filename dir=\"ltr\">/media/usbdisk/preseed.cfg</filename>"

msgid "edit <filename>/media/usbdisk/syslinux.cfg</filename> and add required boot parameters (see example below)."
msgstr "حرر الملف <filename dir=\"ltr\">/media/usbdisk/syslinux.cfg</filename> وأضف المتغيرات الإقلاعية اللازمة (انظر المثال التالي)."

msgid "syslinux.cfg file and preseeding parameters"
msgstr "ملف syslinux.cfg وبارامترات التغذية"

msgid ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

msgid "Creating a CD-ROM Image"
msgstr "إنشاء صورة CD-ROM"

msgid "<primary>debian-cd</primary>"
msgstr "<primary>debian-cd</primary>"

msgid "A USB key is a read-write media, so it was easy for us to add a file there and change a few parameters. In the CD-ROM case, the operation is more complex, since we need to regenerate a full ISO image. This task is handled by <emphasis role=\"pkg\">debian-cd</emphasis>, but this tool is rather awkward to use: it needs a local mirror, and it requires an understanding of all the options provided by <filename>/usr/share/debian-cd/CONF.sh</filename>; even then, <command>make</command> must be invoked several times. <filename>/usr/share/debian-cd/README</filename> is therefore a very recommended read."
msgstr "ذاكرة USB هي وسيط تخزين يقبل القراءة والكتابة، لذلك كانت إضافة الملف إليها وتعديل بعض المتغيرات فيها عملية سهلة. لكن في حالة استخدام CD-ROM، فالعملية معقدة أكثر، لأننا نحتاج توليد صورة ISO كاملة. هذه المهمة تحتاج الأداة <emphasis role=\"pkg\">debian-cd</emphasis>، لكن استخدام هذه الأداة مزعج نوعًا ما: تحتاج الأداة لمرآة محلية، كما تحتاج لفهم جميع الخيارات في <filename dir=\"ltr\">/usr/share/debian-cd/CONF.sh</filename>؛ وحتى بعد ذلك، يجب استدعاء <command>make</command> عدة مرات. عليك إذن قراءة <filename dir=\"ltr\">/usr/share/debian-cd/README</filename>."

msgid "Having said that, debian-cd always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific file is <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgstr "تعمل debian-cd دائماً بنفس الأسلوب: يتم توليد مجلد ”صورة“ فيه محتويات القرص الليزري نفسها، ثم يحوّل إلى ملف ISO بأداة مثل <command>genisoimage</command> أو <command>mkisofs</command> أو <command>xorriso</command>. يُختَم المجلد بعد الخطوة <command>make image-trees</command> التابعة لحزمة debian-cd. عند هذه النقطة، سوف نزرع ملف التغذية في المجلد المناسب (عادة <filename dir=\"ltr\">$TDIR/$CODENAME/CD1/</filename>، حيث ‎$TDIR و$CODENAME هما متغيران يعرفهما ملف الإعداد <filename>CONF.sh</filename>). تستخدم الأقراص الليزرية <command>isolinux</command> كمحمل للإقلاع، ويجب ضبط ملف الإعداد ليتناسب مع ما ولدته debian-cd، وإدخال متغيرات الإقلاع المطلوبة (الملف المقصود هو <filename dir=\"ltr\">$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). بعدها يمكن متابعة العملية ”الاعتيادية“، ويمكننا توليد صورة ISO بالأمر <command>make image CD=1</command> (أو <command>make images</command> إذا كنا سنولد عدة CD-ROMs)."

msgid "Simple-CDD: The All-In-One Solution"
msgstr "‏Simple-CDD: كل الحلول في حل واحد"

msgid "<primary>simple-cdd</primary>"
msgstr "<primary>simple-cdd</primary>"

msgid "Simply using a preseed file is not enough to fulfill all the requirements that may appear for large deployments. Even though it is possible to execute a few scripts at the end of the normal installation process, the selection of the set of packages to install is still not quite flexible (basically, only “tasks” can be selected); more important, this only allows installing official Debian packages, and precludes locally-generated ones."
msgstr "ببساطة إن استخدام ملف التغذية لا يكفي لتلبية كافة المطالب التي قد تظهر عند النشر واسع النطاق. وبالرغم أنه يمكن تنفيذ بضعة سكربتات عند نهاية عملية التثبيت العادية، إلا أن مجموعة الحزم التي ستثبت ليست مرنة بما يكفي (أساساً لا يمكن إلا اختيار ”المهام“)؛ وأهم من هذا، لا يمكن إلا تثبيت حزم دبيان الرسمية، ولا يسمح بالحزم المولدة محليًا."

msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what Simple-CDD (in the <emphasis role=\"pkg\">simple-cdd</emphasis> package) does."
msgstr "وعلى صعيد آخر، تستطيع debian-cd دمج الحزم الخارجية، كما يمكن توسيع مثبت دبيان بإدخال خطوات جديدة في عملية التثبيت. بجمع هذه الإمكانيات، يفترض أن نستطيع إنشاء مثبت مخصص يلبي حاجاتنا؛ بل يفترض أن يتمكن أيضًا من ضبط بعض الخدمات بعد تثبيت الحزم المطلوبة. لحسن الحظ، هذه ليست فرضية بلا برهان، بل هي وظيفة Simple-CDD (في الحزمة <emphasis role=\"pkg\">simple-cdd</emphasis>) تماماً."

msgid "The purpose of Simple-CDD is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgstr "الهدف من Simple-CDD هو السماح لأي شخص بإنشاء توزيعة مشتقة من دبيان بسهولة، بتحديد مجموعة جزئية من الحزم المتوفرة، وإعدادها مسبقاً باستخدام Debconf، وإضافة برمجيات معينة، وتنفيذ سكربتات مخصصة عند نهاية عملية التثبيت. هذا يوافق فلسفة ”نظام التشغيل العالمي“، حيث يستطيع أي شخص تعديله ليناسب حاجاته الشخصية."

msgid "Creating Profiles"
msgstr "تعريف البروفايلات"

msgid "Simple-CDD defines “profiles” that match the FAI “classes” concept, and a machine can have several profiles (determined at installation time). A profile is defined by a set of <filename>profiles/<replaceable>profile</replaceable>.*</filename> files:"
msgstr "يعرف Simple-CDD ”بروفايلات“ تقابل مفهوم ”الفئات – classes“ في FAI، ويمكن إعطاء الجهاز عدة بروفايلات (تُحدَّد أثناء التثبيت). يعرف البروفايل بمجموعة من ملفات <filename dir=\"ltr\">profiles/<replaceable>profile</replaceable>.*</filename>:"

msgid "the <filename>.description</filename> file contains a one-line description for the profile;"
msgstr "ملف <filename dir=\"ltr\">.description</filename> يحوي سطراً واحداً يصف البروفايل؛"

msgid "the <filename>.packages</filename> file lists packages that will automatically be installed if the profile is selected;"
msgstr "ملف <filename dir=\"ltr\">.packages</filename> يسرد أسماء الحزم التي ستثبت تلقائيًا عند تحديد هذا البروفايل؛"

msgid "the <filename>.downloads</filename> file lists packages that will be stored onto the installation media, but not necessarily installed;"
msgstr "ملف <filename dir=\"ltr\">.downloads</filename> يسرد أسماء الحزم التي ستخزن على وسيط التثبيت، لكن لا يشترط تثبيتها؛"

msgid "the <filename>.preseed</filename> file contains preseeding information for Debconf questions (for the installer and/or for packages);"
msgstr "ملف <filename dir=\"ltr\">.preseed</filename> يحوي معلومات التغذية لأسئلة Debconf (للمثبت أو للحزم)؛"

msgid "the <filename>.postinst</filename> file contains a script that will be run at the end of the installation process;"
msgstr "ملف <filename dir=\"ltr\">.postinst</filename> يحوي سكربتًا يعمل عند نهاية التثبيت؛"

msgid "lastly, the <filename>.conf</filename> file allows changing some Simple-CDD parameters based on the profiles to be included in an image."
msgstr "أخيراً، ملف <filename dir=\"ltr\">.conf</filename> يسمح بتعديل بعض متغيرات Simple-CDD اعتماداً على البروفايلات التي ستضمَّن في الصورة."

msgid "The <literal>default</literal> profile has a particular role, since it is always selected; it contains the bare minimum required for Simple-CDD to work. The only thing that is usually customized in this profile is the <literal>simple-cdd/profiles</literal> preseed parameter: this allows avoiding the question, introduced by Simple-CDD, about what profiles to install."
msgstr "البروفايل <literal>default</literal> له دور خاص، لأنه محدد دوماً؛ ولذلك يحوي الحد الأدنى المطلوب لعمل Simple-CDD. الشيء الوحيد الذي يخصص عادة في هذا البروفايل هو متغير التغذية <literal>simple-cdd/profiles</literal>: هذا يسمح بتفادي طلب Simple-CDD تحديد البروفايل الذي يريد تثبيته من المستخدم."

msgid "Note also that the commands will need to be invoked from the parent directory of the <filename>profiles</filename> directory."
msgstr "لاحظ أيضًا أنه يجب استدعاء الأوامر من المجلد الأب للمجلد <filename>profiles</filename>."

msgid "Configuring and Using <command>build-simple-cdd</command>"
msgstr "إعداد واستخدام <command>build-simple-cdd</command>"

msgid "<primary><command>build-simple-cdd</command></primary>"
msgstr "<primary><command>build-simple-cdd</command></primary>"

msgid "<emphasis>QUICK LOOK</emphasis> Detailed configuration file"
msgstr "<emphasis>نظرة سريعة</emphasis> ملف إعداد مفصل"

msgid "An example of a Simple-CDD configuration file, with all possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). This can be used as a starting point when creating a custom configuration file."
msgstr "هناك مثال عن ملف إعداد Simple-CDD فيه كل المتغيرات الممكنة، مضمن في الحزمة (<filename dir =\"ltr\">/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). يمكن استخدام هذا الملف كنقطة انطلاق عند إنشاء ملفات إعداد مخصصة."

msgid "Simple-CDD requires many parameters to operate fully. They will most often be gathered in a configuration file, which <command>build-simple-cdd</command> can be pointed at with the <literal>--conf</literal> option, but they can also be specified via dedicated parameters given to <command>build-simple-cdd</command>. Here is an overview of how this command behaves, and how its parameters are used:"
msgstr "يحتاج Simple-CDD للكثير من المتغيرات ليعمل بشكل كامل. غالبًا ما تجمع هذه المتغيرات في ملف إعداد، وبعدها نمرره للأمر <command>build-simple-cdd</command> بالخيار <literal dir=\"ltr\">--conf</literal>، لكن يمكن أيضاً تحديد قيم هذه المتغيرات باستخدام بارمترات خاصة تعطى للأمر <command>build-simple-cdd</command>. إليك نظرة عامة عن عمل هذا الأمر، وعن تأثير متغيراته المختلفة:"

msgid "the <literal>profiles</literal> parameter lists the profiles that will be included on the generated CD-ROM image;"
msgstr "يحدد المتغير <literal>profiles</literal> البروفايلات التي ستضمن في صورة CD-ROM المولدة؛"

msgid "based on the list of required packages, Simple-CDD downloads the appropriate files from the server mentioned in <literal>server</literal>, and gathers them into a partial mirror (which will later be given to debian-cd);"
msgstr "اعتماداً على قائمة الحزم المطلوبة سوف ينزل Simple-CDD الملفات المناسبة من المخدم المذكور في <literal>server</literal>، ويجمعها في مرآة جزئية (التي ستعطى لاحقًا إلى debian-cd)."

msgid "the custom packages mentioned in <literal>local_packages</literal> are also integrated into this local mirror;"
msgstr "تدمج الحزم المخصصة المذكورة في <literal>local_packages</literal> أيضًا في هذه المرآة المحلية؛"

msgid "debian-cd is then executed (within a default location that can be configured with the <literal>debian_cd_dir</literal> variable), with the list of packages to integrate;"
msgstr "بعدها تستدعى debian-cd (ويستخدم موقع افتراضي يمكن تعديله بالمتغير <literal>debian_cd_dir</literal>)، وتعطى قائمة بالحزم المراد دمجها؛"

msgid "once debian-cd has prepared its directory, Simple-CDD applies some changes to this directory:"
msgstr "بعدما جهزت debian-cd المجلد، تطبق Simple-CDD بعض التعديلات عليه:"

msgid "files containing the profiles are added in a <filename>simple-cdd</filename> subdirectory (that will end up on the CD-ROM);"
msgstr "تضاف الملفات التي تحوي البروفايلات إلى مجلد فرعي باسم <filename>simple-cdd</filename> (وسوف يظهر في القرص النهائي)؛"

msgid "other files listed in the <literal>all_extras</literal> parameter are also added;"
msgstr "تضاف الملفات الأخرى المذكورة في المتغير <literal>all_extras</literal> أيضًا؛"

msgid "the boot parameters are adjusted so as to enable the preseeding. Questions concerning language and country can be avoided if the required information is stored in the <literal>language</literal> and <literal>country</literal> variables."
msgstr "تضبط متغيرات الإقلاع لتفعيل التغذية. يتم تفادي الأسئلة عن اللغة والبلد إذا كانت المعلومات المطلوبة مخزنة في المتغيرين <literal>language</literal> و<literal>country</literal>."

msgid "debian-cd then generates the final ISO image."
msgstr "تولد debian-cd صورة ISO النهائية."

msgid "Generating an ISO Image"
msgstr "توليد صورة ISO"

#, fuzzy
#| msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-8.0-amd64-CD-1.iso</filename>."
msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-10-amd64-CD-1.iso</filename>."
msgstr "بعدما كتبنا ملف الإعداد وعرفنا البروفايلات، تبقى خطوة استدعاء <command>build-simple-cdd --conf simple-cdd.conf</command>. بعد عدة دقائق، نحصل على الصورة المطلوبة في <filename>images/debian-8.0-amd64-CD-1.iso</filename>."

msgid "Monitoring is a generic term, and the various involved activities have several goals: on the one hand, following usage of the resources provided by a machine allows anticipating saturation and the subsequent required upgrades; on the other hand, alerting the administrator as soon as a service is unavailable or not working properly means that the problems that do happen can be fixed sooner."
msgstr "المراقبة هي مصطلح عام، ونشاطات المراقبة المتنوعة لها أهداف عدة: فمن ناحية أولى، تسمح متابعة استهلاك موارد الحاسب بتوقع الإشباع والتطويرات اللاحقة له؛ ومن ناحية أخرى، فإن تنبيه مدير النظام فور خروج إحدى الخدمات عن العمل أو عدم عملها بشكل صحيح يعني أن إصلاح المشاكل التي تحدث قد يتم أبكر."

msgid "<emphasis>Munin</emphasis> covers the first area, by displaying graphical charts for historical values of a number of parameters (used RAM, occupied disk space, processor load, network traffic, Apache/MySQL load, and so on). <emphasis>Nagios</emphasis> covers the second area, by regularly checking that the services are working and available, and sending alerts through the appropriate channels (e-mails, text messages, and so on). Both have a modular design, which makes it easy to create new plug-ins to monitor specific parameters or services."
msgstr "يغطي <emphasis>Munin</emphasis> الناحية الأولى، من خلال عرض مخططات بيانية للقيم التاريخية لعدد من المتغيرات (الذاكرة المستخدمة، مساحة القرص المحجوزة، حمل المعالج، نشاط الشبكة، حمل Apache/MySQL، وهكذا). أما <emphasis>Nagios</emphasis> فيغطي الناحية الأخرى، من خلال التحقق المنتظم من عمل الخدمات وتوفرها، وإرسال تنبيهات عبر القنوات المناسبة (بريد إلكتروني، رسائل نصية، وهكذا). لكل منهما تصميم تجزيئي يسهل إنشاء إضافات جديدة لمراقبة متغيرات أو خدمات محددة."

msgid "<emphasis>ALTERNATIVE</emphasis> Zabbix, an integrated monitoring tool"
msgstr "<emphasis>بدائل</emphasis> Zabbix، أداة مراقبة متكاملة"

msgid "<primary>Zabbix</primary>"
msgstr "<primary>Zabbix</primary>"

#, fuzzy
#| msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"http://www.zabbix.com/\" />"
msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"https://www.zabbix.com/\" />"
msgstr "رغم أن استخدام Munin و Nagios شائع جداً، إلا أنهما ليسا اللاعبين الوحيدين في مجال المراقبة، كما أن كل منهما يعالج نصف المهمة فقط (الأول يتولى الرسوم البيانية، والثاني التنبيهات). أما Zabbix فيجمع بين الاثنتين؛ كما أن له واجهة وب لضبط النواحي الأكثر استخداماً. لقد تطور Zabbix في قفزات كبيرة خلال السنوات القليلة الماضية، ويمكن اعتباره منافساً حقيقياً. لاستخدامه عليك تثبيت <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (أو <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>) على مخدم المراقبة، وربما أيضاً <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> للحصول على واجهة وب. أما على الأجهزة التي ستراقبها فعليك تثبيت <emphasis role=\"pkg\">zabbix-agent</emphasis> الذي يرسل البيانات إلى المخدم. <ulink type=\"block\" url=\"http://www.zabbix.com/\" />"

msgid "<emphasis>ALTERNATIVE</emphasis> Icinga, a Nagios fork"
msgstr "<emphasis>بدائل</emphasis> Icinga، مشتق من Nagios"

msgid "<primary>Icinga</primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"
msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"https://www.icinga.org/\" />"
msgstr "اشتق عدد من المطورين Nagios نتيجة تباين الآراء بخصوص نموذج تطوير Nagios (الذي تتحكم به شركة)، واختاروا Icinga كاسم لهم. لا يزال Icinga متوافقاً مع إعدادات Nagios وإضافاته —حتى الآن— إلا أنه يضيف بعض المزايا الخاصة أيضاً. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"

msgid "Setting Up Munin"
msgstr "إعداد Munin"

msgid "<primary>Munin</primary>"
msgstr "<primary>Munin</primary>"

msgid "The purpose of Munin is to monitor many machines; therefore, it quite naturally uses a client/server architecture. The central host — the grapher — collects data from all the monitored hosts, and generates historical graphs."
msgstr "يهدف Munin لمراقبة العديد من الأجهزة؛ وبالتالي، من الطبيعي أن يعتمد بنية مخدم/عميل. يجمع المستضيف المركزي –راسم البيانات (the grapher)– المعطيات من جميع حواسيب المراقبة، ويولد المخططات البيانية الزمنية."

msgid "Configuring Hosts To Monitor"
msgstr "إعداد الأجهزة للمراقبة"

msgid "The first step is to install the <emphasis role=\"pkg\">munin-node</emphasis> package. The daemon installed by this package listens on port 4949 and sends back the data collected by all the active plugins. Each plugin is a simple program returning a description of the collected data as well as the latest measured value. Plugins are stored in <filename>/usr/share/munin/plugins/</filename>, but only those with a symbolic link in <filename>/etc/munin/plugins/</filename> are really used."
msgstr "الخطوة الأولى هي تثبيت الحزمة <emphasis role=\"pkg\">munin-node</emphasis>. تنصت الخدمة التي تثبتها هذه الحزمة إلى المنفذ 4949 وترد بإرسال البيانات التي تجمعها كافة الملحقات الفعالة. كل ملحق هو برنامج بسيط يعيد وصفاً للبيانات التي يجمعها بالإضافة إلى آخر قيمة مقاسة. تخزن الملحقات في <filename>/usr/share/munin/plugins/</filename>، لكن لا تستخدم منها إلا التي لها رابط رمزي في المجلد <filename>/etc/munin/plugins/</filename>."

#, fuzzy
#| msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the <ulink url=\"http://gallery.munin-monitoring.org\">Plugin Gallery</ulink> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the Plugin Gallery<footnote><para><ulink type=\"block\" url=\"http://gallery.munin-monitoring.org\" /></para></footnote> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgstr "عند تثبيت الحزمة، تعرف مجموعة من الملحقات الفعالة اعتماداً على البرمجيات المتوفرة والإعداد الحالي للمستضيف. لكن هذا الإعداد الآلي يعتمد على ميزة يجب أن يوفرها كل ملحق، ولذلك كان من المستحسن مراجعة وتعديل النتائج يدوياً. قد يفيد تصفح <ulink url=\"http://gallery.munin-monitoring.org\">معرض الملحقات</ulink> ولو لم يكن هناك توثيقاً شاملاً لجميع الملحقات. على أي حال، جميع الملحقات هي سكربتات ومعظمها بسيط جداً وفيه تعليقات توضيحية جيدة. إن تصفح <filename>/etc/munin/plugins/</filename> إذن هو طريق جيدة لأخذ فكرة عن مهمة كل ملحق وتحديد الملحقات التي يجب إزالتها. كما أن تفعيل ملحق مفيد تجده في <filename>/usr/share/munin/plugins/</filename> لا يحتاج إلا إنشاء رابط رمزي بالأمر <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>‎. لاحظ أنه عندما ينتهي اسم الملحق بشرطة منخفضة ”_“ (underscore)، فهذا يعني أن الملحق يحتاج متغيراً حتى يعمل. يجب تخزين قيمة هذا المتغير في اسم الرابط الرمزي؛ مثلاً، يجب تفعيل الملحق ”if_‎“ بالرابط <filename>if_eth0</filename>، وعندها سيراقب نشاط الشبكة على الواجهة الشبكية eth0."

#, fuzzy
#| msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>service munin-node restart</command>."
msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>systemctl restart munin-node</command>."
msgstr "بعد إعداد جميع الملحقات بشكل صحيح، يجب تغيير إعدادات الخدمة لتحديد صلاحيات الوصول للبيانات المجموعة. يتم هذا من خلال استخدام تعليمة التوجيه <literal>allow</literal> في الملف <filename>/etc/munin/munin-node.conf</filename> الإعداد الافتراضي هو <literal dir=\"ltr\">allow ^127\\.0\\.0\\.1$</literal>، وهو يسمح بالوصول فقط للمستضيف المحلي. في العادة سيضيف مدير النظام سطراً مشابهاً يحوي عنوان IP للمستضيف راسم البيانات، وبعدها يعيد تشغيل الخدمة بالأمر <command>service munin-node restart</command>."

msgid "<emphasis>GOING FURTHER</emphasis> Creating local plugins"
msgstr "<emphasis>التعمق أكثر</emphasis> إنشاء ملحقات محلية"

#, fuzzy
#| msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"
msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://guide.munin-monitoring.org/en/latest/plugin/writing.html\" />"
msgstr "يوفر Munin توثيقاً مفصلاً عن أسلوب عمل الملحقات، وكيفية تطوير الملحقات الجديدة. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"

msgid "A plugin is best tested when run in the same conditions as it would be when triggered by munin-node; this can be simulated by running <command>munin-run <replaceable>plugin</replaceable></command> as root. A potential second parameter given to this command (such as <literal>config</literal>) is passed to the plugin as a parameter."
msgstr "أفضل اختبار للملحق هو عند تشغيله في الظروف نفسها التي يعمل فيها عندما تستدعيه الخدمة munin-node؛ ويمكن محاكاة هذا باستدعاء الأمر <command>munin-run <replaceable>plugin</replaceable></command> بصلاحيات الجذر. إذا تم تمرير متغير ثان لهذا الأمر (مثل <literal>config</literal>) فسوف يعطى للملحق كمتغير."

msgid "When a plugin is invoked with the <literal>config</literal> parameter, it must describe itself by returning a set of fields:"
msgstr "عند استدعاء الملحق مع المتغير <literal>config</literal>، عليه توصيف نفسه عبر إعادة زمرة من الحقول:"

msgid ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>\n"

#, fuzzy
#| msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"https://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgstr "يشرح ”مرجع الملحقات Plugin reference“ مختلف الحقول المتاحة، هذا الدليل متوفر ضمن ”Munin guide“.‏ <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"

msgid "When invoked without a parameter, the plugin simply returns the last measured values; for instance, executing <command>sudo munin-run load</command> could return <literal>load.value 0.12</literal>."
msgstr "عند استدعاء الملحلق دون أي متغيرات، سوف يعيد آخر قيمة مقاسة ببساطة؛ مثلاً، تنفيذ <command>sudo munin-run load</command> سوف يعيد القيمة <literal>load.value 0.12</literal>."

msgid "Finally, when a plugin is invoked with the <literal>autoconf</literal> parameter, it should return “yes” (and a 0 exit status) or “no” (with a 1 exit status) according to whether the plugin should be enabled on this host."
msgstr "أخيراً، عند استدعاء الملحق مع المتغير <literal>autoconf</literal>، عليه أن يعيد ”yes“ (مع حالة الخروج—exit status ‏0) إذا كان تفعيل الملحق واجباً على هذا المستضيف، أو ”no“ (مع حالة الخروج 1) في الحالة المعاكسة."

msgid "Configuring the Grapher"
msgstr "إعداد راسم البيانات"

msgid "The “grapher” is simply the computer that aggregates the data and generates the corresponding graphs. The required software is in the <emphasis role=\"pkg\">munin</emphasis> package. The standard configuration runs <command>munin-cron</command> (once every 5 minutes), which gathers data from all the hosts listed in <filename>/etc/munin/munin.conf</filename> (only the local host is listed by default), saves the historical data in RRD files (<emphasis>Round Robin Database</emphasis>, a file format designed to store data varying in time) stored under <filename>/var/lib/munin/</filename> and generates an HTML page with the graphs in <filename>/var/cache/munin/www/</filename>."
msgstr "”راسم البيانات“ هو ببساطة حاسوب يجمع البيانات ويولد الرسوم البيانية الموافقة. البرنامج المطلوب متوفر في الحزمة <emphasis role=\"pkg\">munin</emphasis>. يشغل الإعداد الافتراضي <command>munin-cron</command> (مرة كل 5 دقائق)، الذي يجمع البيانات من كافة الأجهزة المذكورة في <filename dir=\"ltr\">/etc/munin/munin.conf</filename> (المستضيف المحلي هو الوحيد المذكور افتراضيًا)، ويحفظ البيانات التاريخية في ملفات RRD (‏<emphasis>Round Robin Database</emphasis>، وهي صيغة ملفات مصممة لحفظ البيانات التي تتغير مع الزمن) محفوظة في <filename>/var/lib/munin/</filename> ويولد صفحة HTML تحوي المخططات البيانية في المجلد <filename>/var/cache/munin/www/</filename>."

msgid "All monitored machines must therefore be listed in the <filename>/etc/munin/munin.conf</filename> configuration file. Each machine is listed as a full section with a name matching the machine and at least an <literal>address</literal> entry giving the corresponding IP address."
msgstr "يجب إذن ذكر جميع الأجهزة المراقبة في ملف الضبط <filename dir=\"ltr\">/etc/munin/munin.conf</filename>. كل جهاز يذكر في قسم كامل مع اسم يقابل الجهاز ومدخلة <literal>address</literal> واحدة على الأقل هي مدخلة العنوان التي تعطي عنوان IP المناسب."

msgid ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"
msgstr ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes\n"

msgid "Sections can be more complex, and describe extra graphs that could be created by combining data coming from several machines. The samples provided in the configuration file are good starting points for customization."
msgstr "يمكن أن تصبح الأقسام معقدة أكثر وتضاف إليها معلومات وصف مخططات بيانية إضافية لتوليدها بجمع البيانات من عدة أجهزة. العينات الموفرة في ملف الضبط هي نقاط بدء جيدة للتخصيص."

msgid "The last step is to publish the generated pages; this involves configuring a web server so that the contents of <filename>/var/cache/munin/www/</filename> are made available on a website. Access to this website will often be restricted, using either an authentication mechanism or IP-based access control. See <xref linkend=\"sect.http-web-server\" /> for the relevant details."
msgstr "آخر خطوة هي نشر الصفحات المولدة؛ وهذا يحتاج إعداد مخدم وب حتى تتاح محتويات <filename>/var/cache/munin/www/</filename> على موقع وب. سيكون الوصول لهذا الموقع مقيَّداً غالباً، إما باستخدام نظام مصادقة أو بتقييد الوصول حسب عناوين IP. انظر <xref linkend=\"sect.http-web-server\" /> لمزيد من التفاصيل."

msgid "Setting Up Nagios"
msgstr "إعداد Nagios"

msgid "<primary>Nagios</primary>"
msgstr "<primary>Nagios</primary>"

msgid "Unlike Munin, Nagios does not necessarily require installing anything on the monitored hosts; most of the time, Nagios is used to check the availability of network services. For instance, Nagios can connect to a web server and check that a given web page can be obtained within a given time."
msgstr "لا يشترط Nagios تثبيت أي شيء على الأجهزة المراقبة بخلاف Munin؛ بل يستخدم Nagios –معظم الأحيان– للتحقق من توفر الخدمات الشبكية. مثلاً، يمكن أن يتصل Nagios بمخدم الوب ويتحقق أنه يستطيع الحصول على صفحة وب معينة خلال مدة زمنية محددة."

msgid "Installing"
msgstr "التثبيت"

msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios4</emphasis> and <emphasis role=\"pkg\">monitoring-plugins</emphasis> packages. Installing the packages configures the web interface and the Apache server. The <literal>authz_groupfile</literal> and <literal>auth_digest</literal> Apache modules must be enabled, for that execute:"
msgstr ""

msgid ""
"<computeroutput># </computeroutput><userinput>a2enmod authz_groupfile</userinput>\n"
"<computeroutput>Considering dependency authz_core for authz_groupfile:\n"
"Module authz_core already enabled\n"
"Enabling module authz_groupfile.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>a2enmod auth_digest\n"
"Considering dependency authn_core for auth_digest:\n"
"Module authn_core already enabled\n"
"Enabling module auth_digest.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl restart apache2\n"
"</userinput>"
msgstr ""

msgid "Adding other users is a simple matter of inserting them in the <filename>/etc/nagios4/hdigest.users</filename> file."
msgstr ""

#, fuzzy
#| msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios3/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios4/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgstr "تفتح واجهة الوب بتوجيه مستعرض الوب إلى العنوان <literal dir=\"ltr\">http://<replaceable>server</replaceable>/nagios3/</literal>؛ لاحظ أن Nagios يراقب وحده بعض المتغيرات للجهاز الذي يعمل عليه. لكن لا تعمل بعض المزايا التفاعلية مثل إضافة التعليقات إلى المستضيف. هذه المزايا معطلة في إعدادات Nagios الافتراضية، إذ أن هذه الإعدادات مقيدة جداً لأسباب أمنية."

#, fuzzy
#| msgid "As documented in <filename>/usr/share/doc/nagios3/README.Debian</filename>, enabling some features involves editing <filename>/etc/nagios3/nagios.cfg</filename> and setting its <literal>check_external_commands</literal> parameter to “1”. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgid "Enabling some features involves editing <filename>/etc/nagios4/nagios.cfg</filename>. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgstr "كما هو موثق في <filename dir=\"ltr\">/usr/share/doc/nagios3/README.Debian</filename>، لتفعيل بعض المزايا يجب تعديل <filename dir=\"ltr\">/etc/nagios3/nagios.cfg</filename> وتغيير  قيمة المتغير <literal>check_external_commands</literal> إلى ”1“. كما نحتاج ضبط صلاحيات الكتابة للمجلدات التي يستخدمها Nagios، بأوامر تشبه ما يلي:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
#| "</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
#| "</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
#| "<computeroutput>[...]</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>systemctl stop nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios4/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl start nagios4\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
"</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
"<computeroutput>[...]</computeroutput>"

msgid "Configuring"
msgstr "الضبط"

#, fuzzy
#| msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios3/nagios.cfg</filename>."
msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios4/nagios.cfg</filename>."
msgstr "واجهة الوب في Nagios جميلة نسبيًا، لكنها لا تسمح بتغيير الإعدادات، ولا يمكن استخدامها لإضافة أجهزة أو خدمات لمراقبتها. كل الإعداد تديره ملفات يشير إليها ملف الإعداد المركزي، وهو <filename dir=\"ltr\">/etc/nagios3/nagios.cfg</filename>."

msgid "These files should not be dived into without some understanding of the Nagios concepts. The configuration lists objects of the following types:"
msgstr "قبل الغوص في هذه الملفات، يجب فهم بعض مفاهيم Nagios. يشمل الإعداد مجموعة من الأنواع المختلفة من الكائنات:"

msgid "a <emphasis>host</emphasis> is a machine to be monitored;"
msgstr "<emphasis>host</emphasis> (المستضيف) هو الجهاز الذي ستتم مراقبته؛"

msgid "a <emphasis>hostgroup</emphasis> is a set of hosts that should be grouped together for display, or to factor some common configuration elements;"
msgstr "<emphasis>hostgroup</emphasis> هي مجموعة من المستضيفين يجب تجميعهم معاً عند العرض، أو لتجميع بعض الإعدادات المشتركة؛"

msgid "a <emphasis>service</emphasis> is a testable element related to a host or a host group. It will most often be a check for a network service, but it can also involve checking that some parameters are within an acceptable range (for instance, free disk space or processor load);"
msgstr "<emphasis>service</emphasis> (الخدمة) هي عنصر قابل للقياس متعلق بمستضيف أو بمجموعة من المستضيفين. الغالب أنها فحص لخدمة شبكية ما، لكن يمكن أن تشمل اختبار متغيرات أخرى أيضًا والتحقق أن قيمها ضمن مجال مقبول (مثلاً، مساحة القرص الحرة أو حمل المعالج)؛"

msgid "a <emphasis>servicegroup</emphasis> is a set of services that should be grouped together for display;"
msgstr "<emphasis>servicegroup</emphasis> هي مجموعة من الخدمات التي يجب تجميعها معاً عند العرض؛"

msgid "a <emphasis>contact</emphasis> is a person who can receive alerts;"
msgstr "<emphasis>contact</emphasis> هو شخص يتلقى التنبيهات؛"

msgid "a <emphasis>contactgroup</emphasis> is a set of such contacts;"
msgstr "<emphasis>contactgroup</emphasis> مجموعة من الأشخاص الذين يتلقون التنبيهات؛"

msgid "a <emphasis>timeperiod</emphasis> is a range of time during which some services have to be checked;"
msgstr "<emphasis>timeperiod</emphasis> الفاصل الزمني بين كل عملية تحقق من بعض الخدمات؛"

msgid "a <emphasis>command</emphasis> is the command line invoked to check a given service."
msgstr "<emphasis>command</emphasis> هو سطر من الأوامر يستدعى للتحقق من خدمة معينة."

msgid "According to its type, each object has a number of properties that can be customized. A full list would be too long to include, but the most important properties are the relations between the objects."
msgstr "لكل كائن عدد من الخصائص (تختلف حسب نوعه) التي يمكن تعديلها. لا يمكن أن نضع قائمة كاملة بها لكثرتها، لكن أهم الخصائص هي العلاقات بين الكائنات."

msgid "A <emphasis>service</emphasis> uses a <emphasis>command</emphasis> to check the state of a feature on a <emphasis>host</emphasis> (or a <emphasis>hostgroup</emphasis>) within a <emphasis>timeperiod</emphasis>. In case of a problem, Nagios sends an alert to all members of the <emphasis>contactgroup</emphasis> linked to the service. Each member is sent the alert according to the channel described in the matching <emphasis>contact</emphasis> object."
msgstr "تستخدم الخدمة (<emphasis>service</emphasis>) أمراً (<emphasis>command</emphasis>) للتحقق من حالة ميزة على مستضيف (<emphasis>host</emphasis>) معين (أو مجموعة <emphasis>hostgroup</emphasis>) خلال فاصل زمني (<emphasis>timeperiod</emphasis>). في حال حدوث مشكلة، يرسل Nagios تنبيهاً لجميع أعضاء <emphasis>contactgroup</emphasis> المرتبطة بتلك الخدمة. يرسل التنبيه لكل عضو وفقاً لقناة الاتصال المحددة في كائن <emphasis>contact</emphasis> المقابل له."

#, fuzzy
#| msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios3/conf.d/</filename> are a good source of information on how they work."
msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios4/conf.d/</filename> are a good source of information on how they work."
msgstr "يسمح نظام الوراثة بتشارك مجموعة من الخصائص بين العديد من الكائنات دون تكرار المعلومات. كما يتضمن الإعداد الأولي عدد من الكائنات القياسية؛ إن تعريف مستضيف جديد أو خدمة أو جهة اتصال في معظم الأحيان هو مجرد اشتقاق للكائنات العامة المعرفة مسبقًا. الملفات في <filename>/etc/nagios3/conf.d/</filename> هي مصدر جيد لتعلم طريقة عمل هذه الكائنات."

msgid "The Falcot Corp administrators use the following configuration:"
msgstr "يستخدم مديرو النظم في شركة فلكوت الإعداد التالي:"

#, fuzzy
#| msgid "<filename>/etc/nagios3/conf.d/falcot.cfg</filename> file"
msgid "<filename>/etc/nagios4/conf.d/falcot.cfg</filename> file"
msgstr "الملف <filename dir=\"ltr\">/etc/nagios3/conf.d/falcot.cfg</filename>"

msgid ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"
msgstr ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}\n"

#, fuzzy
#| msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>."
msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios4/conf.d/services_nagios2.cfg</filename>."
msgstr "يُعرِّف ملف الإعداد هذا مستضيفين لمراقبتهما. الأول مخدم وب، وتجرى عليه فحوصات على منفذ HTTP‏ (80) ومنفذ HTTPS‏ (443). يختبر Nagios أيضاً مخدم SMTP يعمل على المنفذ 25. المستضيف الثاني هو مخدم FTP، ويتضمن الاختبار التحقق أن الرد يتم خلال 20 ثانية. بعد هذا التأخير يولد <emphasis>warning</emphasis>؛ أما بعد 30 ثانية، فيصدر إنذار حرج. تظهر واجهة الوب الخاصة بـNagios أن خدمة SSH مراقبة أيضاً: هذه المراقبة ناتجة عن انضمام المستضيفون لمجموعة <literal>ssh-servers</literal>. الخدمة القياسية المقابلة معرفة في <filename dir=\"ltr\">/etc/nagios3/conf.d/services_nagios2.cfg</filename>."

msgid "Note the use of inheritance: an object is made to inherit from another object with the “use <replaceable>parent-name</replaceable>”. The parent object must be identifiable, which requires giving it a “name <replaceable>identifier</replaceable>” property. If the parent object is not meant to be a real object, but only to serve as a parent, giving it a “register 0” property tells Nagios not to consider it, and therefore to ignore the lack of some parameters that would otherwise be required."
msgstr "لاحظ استخدام الوراثة: يرث الكائن من كائن آخر باستخدام ”use <replaceable>parent-name</replaceable>“. يجب أن يكون الكائن الأب قابلاً للتعرف، عبر إسناد اسم له في خاصية ”name <replaceable>identifier</replaceable>“. إذا كان الهدف من الكائن الأب أن يستعمل في الوراثة فقط دون أن يكون كائناً حقيقياً، عندها يعطى الخاصية ”register 0“ حتى لا يأخذه Nagios بعين الاعتبار، وبالتالي يتجاهل نقصان بعض المتغيرات المطلوبة في الحالة الطبيعية."

msgid "<emphasis>DOCUMENTATION</emphasis> List of object properties"
msgstr "<emphasis>توثيق</emphasis> قائمة بخصائص الكائنات"

#, fuzzy
#| msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation provided by the <emphasis role=\"pkg\">nagios3-doc</emphasis> package. This documentation is directly accessible from the web interface, with the “Documentation” link in the top left corner. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation hosted on <ulink url=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/index.html\" />. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgstr "يمكن التعمق في فهم الطرق المختلفة لإعداد Nagios من الوثائق المتاحة في حزمة <emphasis role=\"pkg\">nagios3-doc</emphasis>. يمكن الوصول لهذه الوثائق مباشرة عبر واجهة الوب، من خلال وصلة ”Documentation“ في الزاوية اليسرى العليا. يتضمن التوثيق قائمة بأنواع الكائنات، مع جميع الخصائص التي يمكن أن تملكها. كما يشرح كيفية إنشاء ملحقات جديدة."

msgid "<emphasis>GOING FURTHER</emphasis> Remote tests with NRPE"
msgstr "<emphasis>التعمق أكثر</emphasis> الاختبار عن بعد باستخدام NRPE"

msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <emphasis>check_nrpe</emphasis> command."
msgstr "العديد من ملحقات Nagios تسمح باختبار بعض المتغيرات المحلية على المستضيف؛ إذا كانت هناك حاجة لإجراء مثل هذه الاختبارات مع تجميع نتائجها في مكان واحد، فيجب نشر الملحق NPRE‏ (<emphasis>Nagios Remote Plugin Executor</emphasis>). يجب تثبيت الحزمة <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> على مخدم Nagios، والحزمة <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> على الأجهزة التي يراد إجراء الاختبارات عليها. تأخذ الأخيرة إعداداتها من <filename dir=\"ltr\">/etc/nagios/nrpe.cfg</filename>. يجب أن يحدد هذا الملف الاختبارات التي يمكن تنفيذها عن بعد، وعناوين IP للأجهزة التي يسمح لها بطلب هذه الاختبارات. تفعيل هذه الاختبارات البعيدة على طرف Nagios يتم ببساطة بإضافة الخدمات المقابلة لها باستخدام الأمر الجديد <emphasis>check_nrpe</emphasis>."

#~ msgid ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"
#~ msgstr ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>\n"

#~ msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> and <emphasis role=\"pkg\">nagios3-doc</emphasis> packages. Installing the packages configures the web interface and creates a first <literal>nagiosadmin</literal> user (for which it asks for a password). Adding other users is a simple matter of inserting them in the <filename>/etc/nagios3/htpasswd.users</filename> file with Apache's <command>htpasswd</command> command. If no Debconf question was displayed during installation, <command>dpkg-reconfigure nagios3-cgi</command> can be used to define the <literal>nagiosadmin</literal> password."
#~ msgstr "أول خطوة في إعداد Nagios هي تثبيت الحزم <emphasis role=\"pkg\">nagios3</emphasis>، و<emphasis role=\"pkg\">nagios-plugins</emphasis>، و<emphasis role=\"pkg\">nagios3-doc</emphasis>. عملية التثبيت لهذه الحزم تتضمن إعداد واجهة وب وإنشاء مستخدم أولي باسم <literal>nagiosadmin</literal> (ويطلب منك تحديد كلمة السر لهذا الحساب). يمكن إضافة مستخدمين آخرين بسهولة بإضافتهم إلى ملف <filename dir=\"ltr\">/etc/nagios3/htpasswd.users</filename> بالأمر <command>htpasswd</command> الذي يوفره مخدم الوب أباتشي. إذا لم يظهر سؤال Debconf عن كلمة السر أثناء التثبيت، فيمكن استخدام <command>dpkg-reconfigure nagios3-cgi</command> لتعريف كلمة السر لحساب <literal>nagiosadmin</literal>."

# AUTHOR <EMAIL@ADDRESS>, YEAR.
msgid ""
msgstr "Project-Id-Version: 0\nPOT-Creation-Date: 2022-07-30 18:23+0200\nPO-Revision-Date: 2022-10-20 19:23+0000\nLast-Translator: eulalio <eulalio@disroot.org>\nLanguage-Team: Spanish <https://hosted.weblate.org/projects/debian-handbook/12_advanced-administration/es/>\nLanguage: es-ES\nMIME-Version: 1.0\nContent-Type: application/x-publican; charset=UTF-8\nContent-Transfer-Encoding: 8bit\nPlural-Forms: nplurals=2; plural=n != 1;\nX-Generator: Weblate 4.14.2-dev\n"

msgid "RAID"
msgstr "RAID"

msgid "LVM"
msgstr "LVM"

msgid "FAI"
msgstr "FAI"

msgid "Preseeding"
msgstr "Presembrado"

msgid "Monitoring"
msgstr "Monitorización"

msgid "Virtualization"
msgstr "Virtualización"

msgid "Xen"
msgstr "Xen"

msgid "LXC"
msgstr "LXC"

msgid "Advanced Administration"
msgstr "Administración avanzada"

msgid "This chapter revisits some aspects we already described, with a different perspective: instead of installing one single computer, we will study mass-deployment systems; instead of creating RAID or LVM volumes at install time, we'll learn to do it by hand so we can later revise our initial choices. Finally, we will discuss monitoring tools and virtualization techniques. As a consequence, this chapter is more particularly targeting professional administrators, and focuses somewhat less on individuals responsible for their home network."
msgstr "Este capítulo vuelve sobre algunos aspectos que ya se han descripto anteriormente con una perspectiva diferente: en lugar de instalar un único equipo vamos a estudiar sistemas de despliegue masivo; en lugar de crear volúmenes RAID o LVM durante la instalación, vamos a aprender a hacerlo a mano para que posteriormente podamos revisar nuestras elecciones iniciales. Por último veremos herramientas de monitorización y técnicas de virtualización. Como consecuencia de lo anterior, este capítulo se dirige más a administradores profesionales y no tanto a personas responsables únicamente de su red doméstica."

msgid "RAID and LVM"
msgstr "RAID y LVM"

msgid "<primary>RAID</primary>"
msgstr "<primary>RAID</primary>"

msgid "<primary>LVM</primary>"
msgstr "<primary>LVM</primary>"

msgid "<primary>Logical Volume Manager</primary><see>LVM</see>"
msgstr "<primary>Administrador de volúmenes lógicos</primary><see>LVM</see>"

msgid "<primary>volume</primary><secondary>logical volume</secondary>"
msgstr "<primary>volumen</primary><secondary>volumen lógico</secondary>"

msgid "<primary>volume</primary><secondary>raid volume</secondary>"
msgstr "<primary>volumen</primary><secondary>volumen raid</secondary>"

msgid "<primary>filesystem</primary><secondary>redundancy</secondary>"
msgstr "<primary>sistema de ficheros</primary><secondary>redundancia</secondary>"

msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive re-installation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgstr "El <xref linkend=\"installation\" /> presentaba estas tecnologías desde el punto de vista del instalador y cómo éste las integra para hacer sencillo su despliegue desde el comienzo. Después de la instalación inicial, un administrador debe ser capaz de gestionar las cambiantes necesidades de espacio sin tener que recurrir a una reinstalación. Por lo tanto necesita dominar las herramientas necesarias para manipular volúmenes RAID y LVM."

msgid "<primary>volume</primary><secondary>management</secondary>"
msgstr "<primary>volumen</primary><secondary>gestión</secondary>"

msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former ensures the security and availability of the data in case of hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgstr "Tanto RAID como LVM son técnicas para abstraer los volúmenes montados de sus correspondientes dispositivos físicos (discos duros reales o particiones de los mismos). El primero garantiza la seguridad y disponibilidad de los datos en caso de fallos de hardware agregando redundancia mientras que el segundo hace más flexible la gestión de los volúmenes y los independiza del tamaño real de los discos subyacentes. En ambos casos se crean nuevos dispositivos de bloques en el sistema que pueden ser utilizados tanto para crear sistemas de archivos como espacios de intercambio sin necesidad de que se asocien a un disco físico concreto. RAID y LVM tienen orígenes bastante diferentes pero su funcionalidad a veces se solapa, por lo que a menudo se mencionan juntos."

msgid "<emphasis>PERSPECTIVE</emphasis> Btrfs combines LVM and RAID"
msgstr "<emphasis>PERSPECTIVA</emphasis> Btrfs combina LVM y RAID"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>mount</command></primary><secondary>Btrfs</secondary>"
msgstr "<primary><command>mount</command></primary><secondary>Btrfs</secondary>"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary>Btrfs</primary>"
msgstr "<primary>Btrfs</primary>"

msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a filesystem, initially developed at Oracle, that purports to combine the feature sets of LVM and RAID and much more. <ulink type=\"block\" url=\"https://btrfs.wiki.kernel.org/\" />"
msgstr "Mientras que LVM y RAID son dos subsistemas diferenciados del núcleo que se interponen entre los dispositivos de bloques de disco y sus sistemas de archivos, <emphasis>btrfs</emphasis> es un sistema de archivos, desarrollado originalmente por Oracle, que combina las características de LVM, RAID y muchas más. <ulink type=\"block\" url=\"https://btrfs.wiki.kernel.org/\" />"

msgid "Among the noteworthy features are the ability to take a snapshot of a filesystem tree at any point in time. This snapshot copy doesn't initially use any disk space, the data only being duplicated when one of the copies is modified. The filesystem also handles transparent compression of files, and checksums ensure the integrity of all stored data."
msgstr "Entre las características más notables está el poder tomar una instantánea del sistema de archivos en cualquier momento. Esta copia instantánea no utiliza inicialmente espacio en el disco, y sólo se dupica aquella información que es modificada en alguna de las copias. Este sistema de archivos también gestiona de forma transparente la compresión de archivos y hace sumas de verificación para garantizar la integridad de toda la información almacenada."

msgid "<primary>filesystem</primary><secondary>snapshot</secondary>"
msgstr "<primary>sistema de ficheros</primary><secondary>snapshot</secondary>"

msgid "In both the RAID and LVM cases, the kernel provides a block device file, similar to the ones corresponding to a hard disk drive or a partition. When an application, or another part of the kernel, requires access to a block of such a device, the appropriate subsystem routes the block to the relevant physical layer. Depending on the configuration, this block can be stored on one or several physical disks, and its physical location may not be directly correlated to the location of the block in the logical device."
msgstr "Tanto en el caso de RAID como en el de LVM, el núcleo proporciona un archivo de dispositivo de bloques similar a los que corresponden a un disco duro o una partición. Cuando una aplicación u otra parte del núcleo necesita acceder a un bloque de estos dispositivos, el subsistema apropiado canaliza el bloque a la capa física apropiada. Dependiendo de la configuración este bloque podría estar almacenado en uno o varios discos, y su localización puede no estar directamente relacionada con la ubicación del bloque en el dispositivo lógico."

msgid "Software RAID"
msgstr "RAID por software"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>Software RAID</secondary>"
msgstr "<primary>RAID</primary><secondary>Software RAID</secondary>"

#, fuzzy
msgid "<primary>Redundant Array of Independent Disks</primary><see>RAID</see>"
msgstr "<primary>Redundant Array of Independent Disks</primary><see>RAID</see>"

msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss and ensure availability in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgstr "RAID significa <emphasis>colección redundante de discos independientes</emphasis> («Redundant Array of Independent Disks»). El objetivo de este sistema es evitar pérdida de datos y asegurar su disponibilidad en caso que falle un disco duro. El principio general es bastante simple: se almacenan los datos en varios discos físicos en lugar de sólo uno, con un nivel de redundancia configurable. Dependiendo de esta cantidad de redundancia, y aún en caso de fallo inesperado del disco, se puede reconstruir los datos sin pérdida desde los discos restantes."

msgid "<emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?"
msgstr "<emphasis>CULTURA</emphasis> ¿<foreignphrase>Independiente</foreignphrase> o <foreignphrase>económico</foreignphrase>?"

msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavor of cheapness."
msgstr "La letra I en RAID era originalmente inicial de <emphasis>económico</emphasis> («inexpensive») debido a que RAID permitía un aumento drástico en la seguridad de los datos sin la necesidad de invertir en costosos discos de alta gama. Sin embargo, probablemente debido a preocupaciones de imagen, ahora se suele considerar que es inicial de <emphasis>independiente</emphasis>, lo que no tiene el sabor amargo de implicar bajo coste."

msgid "<primary>RAID</primary><secondary>Hardware RAID</secondary>"
msgstr "<primary>RAID</primary><secondary>Raid para hardware</secondary>"

msgid "<primary>RAID</primary><secondary>degraded</secondary>"
msgstr "<primary>RAID</primary><secondary>degradaedo</secondary>"

msgid "<primary>RAID</primary><secondary>reconstruction</secondary>"
msgstr "<primary>RAID</primary><secondary>reconstrucción</secondary>"

msgid "RAID can be implemented either by dedicated hardware (RAID modules integrated into SCSI or SATA controller cards) or by software abstraction (the kernel). Whether hardware or software, a RAID system with enough redundancy can transparently stay operational when a disk fails; the upper layers of the stack (applications) can even keep accessing the data in spite of the failure. Of course, this “degraded mode” can have an impact on performance, and redundancy is reduced, so a further disk failure can lead to data loss. In practice, therefore, one will strive to only stay in this degraded mode for as long as it takes to replace the failed disk. Once the new disk is in place, the RAID system can reconstruct the required data so as to return to a safe mode. The applications won't notice anything, apart from potentially reduced access speed, while the array is in degraded mode or during the reconstruction phase."
msgstr "Se puede implementar RAID tanto con hardware dedicado (módulos RAID integrados en las tarjetas controladoras SCSI o SATA) o por abstracción de software (el núcleo). Ya sea por hardware o software, un sistema RAID con suficiente redundancia puede mantenerse operativo de forma transparente cuando falle un disco; las capas superiores (las aplicaciones) inclusive pueden seguir accediendo a los datos a pesar del fallo. Por supuesto, este «modo degradado» puede tener un impacto en el rendimiento y se reduce la reduncancia, por lo que otro fallo de disco puede llevar a la pérdida de datos. En la práctica por lo tanto, uno intentará estar en este modo degradado sólo el tiempo que tome reemplazar el disco fallado. Una vez que instale el nuevo disco, el sistema RAID puede reconstruir los datos necesarios para volver a un modo seguro. Las aplicaciones no notarán cambio alguno, además de la posible disminución en la velocidad de acceso, mientras que el array esté en modo degradado o durante la fase de reconstrucción."

msgid "When RAID is implemented by hardware, its configuration generally happens within the BIOS setup tool, and the kernel will consider a RAID array as a single disk, which will work as a standard physical disk, although the device name may be different (depending on the driver)."
msgstr "Cuando se implementa RAID con hardware, generalmente se configura desde la herramienta de gestión del BIOS y el núcleo tratará el array RAID como un solo disco que funcionará como un disco físico estándar, aunque el nombre del dispositivo podría ser diferente."

msgid "We only focus on software RAID in this book."
msgstr "En este libro sólo nos enfocaremos en RAID por software."

msgid "Different RAID Levels"
msgstr "Diferentes niveles de RAID"

msgid "<primary>RAID</primary><secondary>level</secondary>"
msgstr "<primary>RAID</primary><secondary>nivel</secondary>"

msgid "RAID is actually not a single system, but a range of systems identified by their levels; the levels differ by their layout and the amount of redundancy they provide. The more redundant, the more failure-proof, since the system will be able to keep working with more failed disks. The counterpart is that the usable space shrinks for a given set of disks; seen the other way, more disks will be needed to store a given amount of data."
msgstr "RAID no es sólo un sistema sino un rango de sistemas identificados por sus niveles, los cuales se diferencian por su disposición y la cantidad de redundancia que proveen. Mientras más redundantes, más a prueba de fallos serán ya que el sistema podrá seguir funcionando con más discos fallados. Por el otro lado, el espacio utilizable disminuye dado un conjunto de discos; visto de otra forma, necesitará más discos para almacenar una cantidad de datos particular."

msgid "Linear RAID"
msgstr "RAID lineal"

msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That is about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgstr "Aún cuando el subsistema RAID del núcleo permite crear «RAID lineal», esto no es RAID propiamente ya que esta configuración no provee redundancia alguna. El núcleo simplemente agrupa varios discos de punta a punta y provee el volúmen agrupado como un solo disco virtual (un dispositivo de bloque). Esa es toda su función. Rara vez se utiliza únicamente esta configuración (revise más adelante las excepciones), especialmente debido a que la falta de redundancia significa que el fallo de un disco hará que todo el grupo, y por lo tanto todos los datos, no estén disponibles."

msgid "<primary>RAID</primary><secondary>linear</secondary>"
msgstr "<primary>RAID</primary><secondary>lineal</secondary>"

msgid "RAID-0"
msgstr "RAID-0"

msgid "This level doesn't provide any redundancy either, but disks aren't simply stuck on end one after another: they are divided in <emphasis>stripes</emphasis>, and the blocks on the virtual device are stored on stripes on alternating physical disks. In a two-disk RAID-0 setup, for instance, even-numbered blocks of the virtual device will be stored on the first physical disk, while odd-numbered blocks will end up on the second physical disk."
msgstr "Este nivel tampoco provee redundancia, pero los discos no están simplemente agrupados uno después del otro: están divididos en <emphasis>tiras</emphasis> («stripes»), y los bloques en el dispositivo virtual son almacenados en tiras de discos físicos alternados. En una configuración RAID-0 de dos discos, por ejemplo, los bloques pares del dispositivo virtual serán almacenados en el primer disco físico mientras que los bloques impares estarán en el segundo disco físico."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>stripes</secondary>"
msgstr "<primary>RAID</primary><secondary>stripes</secondary>"

msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. The disks are utilized entirely by the RAID device, so they should have the same size not to lose performance."
msgstr "Este sistema no intenta aumentar la confiabilidad ya que (como en el caso lineal) se compromete la disponibilidad de todos los datos tan pronto como falle un disco, pero sí aumenta el rendimiento: durante el acceso secuencial a grandes cantidades de datos contiguos, el núcleo podrá leer de (o escribir a) ambos discos en paralelo, lo que aumentará la tasa de transferencia de datos. Los discos son utilizados en su totalidad por el dispositivo RAID, así que deberían tener el mismo tamaño para no perder rendimiento."

msgid "RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgstr "Está disminuyendo el uso de RAID-0, su nicho está siendo ocupado por LVM (vea más adelante)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>0</secondary>"
msgstr "<primary>RAID</primary><secondary>0</secondary>"

msgid "RAID-1"
msgstr "RAID-1"

msgid "This level, also known as “RAID mirroring”, is both the simplest and the most widely used setup. In its standard form, it uses two physical disks of the same size, and provides a logical volume of the same size again. Data are stored identically on both disks, hence the “mirror” nickname. When one disk fails, the data is still available on the other. For really critical data, RAID-1 can of course be set up on more than two disks, with a direct impact on the ratio of hardware cost versus available payload space."
msgstr "Este nivel, también conocido como «espejado RAID» («mirroring») es la configuración más simple y la más utilizada. En su forma estándar, utiliza dos discos físicos del mismo tamaño y provee un volúmen lógico nuevamente del mismo tamaño. Se almacenan los datos de forma idéntica en ambos discos, de ahí el apodo «espejo» («mirror»). Cuando falla un disco, los datos continúan disponibles en el otro. Para datos realmente críticos, obviamente, RAID-1 puede configurarse con más de dos discos, con un impacto directo en la relación entre el costo del hardware y el espacio disponible para datos útiles."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>1</secondary>"
msgstr "<primary>RAID</primary><secondary>1</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>mirror</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "<emphasis>NOTE</emphasis> Disks and cluster sizes"
msgstr "<emphasis>NOTA</emphasis> Discos y tamaños de «cluster»"

msgid "If two disks of different sizes are set up in a mirror, the bigger one will not be fully used, since it will contain the same data as the smallest one and nothing more. The useful available space provided by a RAID-1 volume therefore matches the size of the smallest disk in the array. This still holds for RAID volumes with a higher RAID level, even though redundancy is stored differently."
msgstr "Si configura en espejo dos discos de diferentes tamaños, el más grande no será completamente utilizado ya que contendrá los mismos datos que el más paqueño y nada más. Por lo tanto, el espacio útil que provee un volúmen RAID-1 es el tamaño del menor de los discos en el array. Esto también aplica a volúmenes RAID de mayor nivel RAID, aún cuando la redundancia se almacene de forma diferente."

msgid "It is therefore important, when setting up RAID arrays (except for RAID-0 and “linear RAID”), to only assemble disks of identical, or very close, sizes, to avoid wasting resources."
msgstr "Por lo tanto es importante, cuando configure arrays RAID (a excepción de RAID-0 y «RAID lineal») sólo agrupar discos de tamaño idéntico, o muy similares, para evitar desperdiciar recursos."

msgid "<emphasis>NOTE</emphasis> Spare disks"
msgstr "<emphasis>NOTA</emphasis> Discos libres"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary>spare disk</primary>"
msgstr "<primary>presembrar</primary>"

msgid "RAID levels that include redundancy allow assigning more disks than required to an array. The extra disks are used as spares when one of the main disks fails. For instance, in a mirror of two disks plus one spare, if one of the first two disks fails, the kernel will automatically (and immediately) reconstruct the mirror using the spare disk, so that redundancy stays assured after the reconstruction time. This can be used as another kind of safeguard for critical data."
msgstr "Los niveles RAID que incluyen redundancia permiten asignar a un array más discos que los necesarios. Los discos adicionales son utilizados como repuestos cuando falla alguno de los discos principales. Por ejemplo, en un espejo de dos discos más uno libre, si falla uno de los primeros discos el núcleo automáticamente (e inmediatamente) reconstruirá el espejo utilizando el disco libre para continuar asegurando la redundancia luego del tiempo de reconstrucción. Puede utilizar esta característica como otra barrera de seguridad para datos críticos."

msgid "One would be forgiven for wondering how this is better than simply mirroring on three disks to start with. The advantage of the “spare disk” configuration is that the spare disk can be shared across several RAID volumes. For instance, one can have three mirrored volumes, with redundancy assured even in the event of one disk failure, with only seven disks (three pairs, plus one shared spare), instead of the nine disks that would be required by three triplets."
msgstr "Es normal preguntarse porqué esto es mejor que simplemente configurar el espejo con tres discos desde el comienzo. La ventaja de la configuración con un «disco libre» es que puede compartir este último entre varios volúmenes RAID. Por ejemplo, uno puede tener tres volúmenes en espejo asegurando redundancia en caso que falle un disco con sólo siete discos (tres pares más un disco libre compartido), en lugar de los nueve discos que necesitaría para configurar tres tríos de discos."

msgid "This RAID level, although expensive (since only half of the physical storage space, at best, is useful), is widely used in practice. It is simple to understand, and it allows very simple backups: since both disks have identical contents, one of them can be temporarily extracted with no impact on the working system. Read performance is often increased since the kernel can read half of the data on each disk in parallel, while write performance isn't too severely degraded. In case of a RAID-1 array of N disks, the data stays available even with N-1 disk failures."
msgstr "Este nivel de RAID, aunque costoso (debido a que sólo es útil la mitad del espacio de almacenamiento en el mejor de los casos) es muy utilizado en la práctica. Es simple de entender y permite respaldos muy simples, como ambos discos tienen el mismo contenido puede extraer temporalmente uno de ellos sin impactar el funcionamiento del sistema. Usualmente aumenta el rendimiento de lectura ya que el núcleo puede leer la mitad de los datos de cada disco en paralelo, mientras que el rendimiento de escritura no se ve afectado muy seriamente. En el caso de un array RAID-1 de N discos, los datos continuarán disponibles en caso que fallen N-1 discos."

#, fuzzy
#| msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgid "<emphasis>CAUTION</emphasis> RAID is not Backup"
msgstr "<emphasis>SUGERENCIA</emphasis> RAID, discos y particiones"

#, fuzzy
#| msgid "<primary>debian-cd</primary>"
msgid "<primary>backup</primary>"
msgstr "<primary>debian-cd</primary>"

msgid "RAID systems are not backup mechanisms. While RAID increases the redundancy - and therefore the availability of a system - and protects against disk failures, backups are done to protect data from being altered, deleted, getting corrupted, etc., and to be able to restore them if necessary. To demonstrate this: If you remove one or all files by accident, a RAID will mirror this change, but it will not provide the means to restore the file(s). So while there is clearly an overlap, they are not the same and should be used in conjunction with each other."
msgstr ""

msgid "RAID-4"
msgstr "RAID-4"

msgid "This RAID level, not widely used, uses N disks to store useful data, and an extra disk to store redundancy information. If that disk fails, the system can reconstruct its contents from the other N. If one of the N data disks fails, the remaining N-1 combined with the “parity” disk contain enough information to reconstruct the required data."
msgstr "Este nivel de RAID, que no es muy utilizado, utiliza N discos para almacenar datos útiles y un disco extra para almacenar información de redundancia. Si falla este disco, el sistema puede reconstruir su contenido de los otros N. Si uno de los N discos de datos falla, la combinación de los demás N-1 discos junto con el disco de «paridad» contiene suficiente información para reconstruir los datos necesarios."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>4</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>parity</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "RAID-4 isn't too expensive since it only involves a one-in-N increase in costs and has no noticeable impact on read performance, but writes are slowed down. Furthermore, since a write to any of the N disks also involves a write to the parity disk, the latter sees many more writes than the former, and its lifespan can shorten dramatically as a consequence. Data on a RAID-4 array is safe only up to one failed disk (of the N+1)."
msgstr "RAID-4 no es demasiado costoso ya que sólo implica un aumento de uno-en-N en los costos y no tiene un impacto significativo en el rendimiento de lectura, pero se reduce la velocidad de escritura. Lo que es más, debido a que escribir en cualquier disco involucra escribir en el disco de paridad este último recibirá muchas más escrituras que los demás y, como consecuencia, podría reducir su tiempo de vida dramáticamente. Los datos en un array RAID-4 están seguro sólo contra el fallo de un disco (de los N+1)."

msgid "RAID-5"
msgstr "RAID-5"

msgid "RAID-5 addresses the asymmetry issue of RAID-4: parity blocks are spread over all of the N+1 disks, with no single disk having a particular role."
msgstr "RAID-5 soluciona el problema de asimetría de RAID-4: los bloques de paridad están distribuidos en todos los N+1 discos, ninguno de los discos tiene un rol particular."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>5</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Read and write performance are identical to RAID-4. Here again, the system stays functional with up to one failed disk (of the N+1), but no more."
msgstr "El rendimiento de lectura y escritura es idéntica a la de RAID-4. Aquí también el sistema continuará su funcionamiento con el fallo de hasta un disco (de los N+1), pero no más."

msgid "RAID-6"
msgstr "RAID-6"

msgid "RAID-6 can be considered an extension of RAID-5, where each series of N blocks involves two redundancy blocks, and each such series of N+2 blocks is spread over N+2 disks."
msgstr "Se puede considerar a RAID-6 como una extensión de RAID-5, donde cada serie de N bloques poseen dos bloques de redundancia, y cada serie de N+2 bloques está distribuida en N+2 discos."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>6</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "This RAID level is slightly more expensive than the previous two, but it brings some extra safety since up to two drives (of the N+2) can fail without compromising data availability. The counterpart is that write operations now involve writing one data block and two redundancy blocks, which makes them even slower."
msgstr "Este nivel de RAID es ligeramente más costoso que los dos anteriores, pero agrega seguridad adicional ya que pueden fallar hasta dos discos (de N+2) sin comprometer la disponibilidad de los datos. Por el otro lado, las operaciones de escritura ahora deben escribir un bloque de datos y dos bloques de redundancia, lo que lo hace aún más lento."

msgid "RAID-1+0"
msgstr "RAID-1+0"

msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there is no problem with that."
msgstr "Estrictamente hablando, este no es un nivel RAID sino la combinación de dos agrupaciones RAID. Comience con 2×N discos, configúrelos en pares de N volúmenes RAID-1; y luego agrupe estos N volúmenes en sólo uno, ya sea con «RAID lineal» o (cada vez más) LVM. Este último caso va más allá de RAID puro, pero no hay problemas con ello."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>1+0</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "RAID-1+0 can survive multiple disk failures: up to N in the 2×N array described above, provided that at least one disk keeps working in each of the RAID-1 pairs."
msgstr "RAID-1+o puede sobrevivir el fallo de varios discos, hasta N en el array de 2×N antes descripto, siempre que continúe trabajando al menos uno de los discos en cada par RAID-1."

msgid "<emphasis>GOING FURTHER</emphasis> RAID-10"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> RAID-10"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>10</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "RAID-10 is generally considered a synonym of RAID-1+0, but a Linux specificity makes it actually a generalization. This setup allows a system where each block is stored on two different disks, even with an odd number of disks, the copies being spread out along a configurable model."
msgstr "Generalmente se considera a RAID-10 como sinónimo de RAID-1+0, pero algo específico de Linux lo hace en realidad una generalización. Esta configuración permite un sistema en el que cada bloque está almacenado en dos discos diferentes, aún con una cantidad impar de discos, con las copias distribuidas en un modelo configurable."

msgid "Performances will vary depending on the chosen repartition model and redundancy level, and of the workload of the logical volume."
msgstr "El rendimiento variará dependiendo del modelo de reparto y el nivel de redundancia que seleccione, así como también de la carga en el volúmen lógico."

msgid "Obviously, the RAID level will be chosen according to the constraints and requirements of each application. Note that a single computer can have several distinct RAID arrays with different configurations."
msgstr "Obviamente, seleccionará el nivel RAID según las limitaciones y requisitos de cada aplicación. Sepa que un mismo equipo puede tener varios arrays RAID distintos con diferentes configuraciones."

msgid "Setting up RAID"
msgstr "Configuración de RAID"

msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>create</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Setting up RAID volumes requires the <emphasis role=\"pkg\">mdadm</emphasis> package; it provides the <command>mdadm</command> command, which allows creating and manipulating RAID arrays, as well as scripts and tools integrating it to the rest of the system, including the monitoring system."
msgstr "Para configurar un volumen RAID necesitará el paquete <emphasis role=\"pkg\">mdamd</emphasis>: éste provee el programa <command>mdadm</command>, que permite crear y modificar arrays RAID, así como también scripts y herramientas que lo integran al resto del sistema, incluyendo el sistema de monitorización."

msgid "Our example will be a server with a number of disks, some of which are already used, the rest being available to setup RAID. We initially have the following disks and partitions:"
msgstr "Nuestro ejemplo será un servidor con una cantidad de discos, algunos que ya están utilizados, y el resto se encuentran disponibles para configurar RAID. Inicialmente tendremos los siguientes discos y particiones:"

msgid "the <filename>sdb</filename> disk, 4 GB, is entirely available;"
msgstr "el disco <filename>sdb</filename>, de 4 GB, completamente disponible;"

msgid "the <filename>sdc</filename> disk, 4 GB, is also entirely available;"
msgstr "el disco <filename>sdc</filename>, de 4 GB, también completamente disponible;"

msgid "on the <filename>sdd</filename> disk, only partition <filename>sdd2</filename> (about 4 GB) is available;"
msgstr "en el disco <filename>sdd</filename> hay disponible una única partición <filename>sdd2</filename> (de alrededor de 4 GB);"

msgid "finally, a <filename>sde</filename> disk, still 4 GB, entirely available."
msgstr "finalmente, un disco <filename>sde</filename>, también de 4 GB, completamente disponible."

msgid "<emphasis>NOTE</emphasis> Identifying existing RAID volumes"
msgstr "<emphasis>NOTA</emphasis> Identificación de volúmenes RAID existentes"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/proc</filename></primary><secondary><filename>/proc/mdstat</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The <filename>/proc/mdstat</filename> file lists existing volumes and their states. When creating a new RAID volume, care should be taken not to name it the same as an existing volume."
msgstr "El archivo <filename>/proc/mdstat</filename> enumera los volúmenes existentes y sus estados. Cuando cree volúmenes RAID, debe tener cuidado de no nombrarlos igual a algún volúmen existente."

msgid "We're going to use these physical elements to build two volumes, one RAID-0 and one mirror (RAID-1). Let's start with the RAID-0 volume:"
msgstr "Utilizaremos estos elementos físicos para crear dos volúmenes, un RAID-0 y un espejo (RAID-1). Comencemos con el volúmen RAID-0:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md0 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0:\n"
#| "           Version : 1.2\n"
#| "     Creation Time : Tue Jun 25 08:47:49 2019\n"
#| "        Raid Level : raid0\n"
#| "        Array Size : 8378368 (7.99 GiB 8.58 GB)\n"
#| "      Raid Devices : 2\n"
#| "     Total Devices : 2\n"
#| "       Persistence : Superblock is persistent\n"
#| "\n"
#| "       Update Time : Tue Jun 25 08:47:49 2019\n"
#| "             State : clean \n"
#| "    Active Devices : 2\n"
#| "   Working Devices : 2\n"
#| "    Failed Devices : 0\n"
#| "     Spare Devices : 0\n"
#| "\n"
#| "        Chunk Size : 512K\n"
#| "\n"
#| "Consistency Policy : none\n"
#| "\n"
#| "              Name : mirwiz:0  (local to host debian)\n"
#| "              UUID : 146e104f:66ccc06d:71c262d7:9af1fbc7\n"
#| "            Events : 0\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       32        0      active sync   /dev/sdb\n"
#| "       1       8       48        1      active sync   /dev/sdc\n"
#| "# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
#| "<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
#| "Discarding device blocks: done                            \n"
#| "Creating filesystem with 2094592 4k blocks and 524288 inodes\n"
#| "Filesystem UUID: 413c3dff-ab5e-44e7-ad34-cf1a029cfe98\n"
#| "Superblock backups stored on blocks: \n"
#| "\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
#| "\n"
#| "Allocating group tables: done                            \n"
#| "Writing inode tables: done                            \n"
#| "Creating journal (16384 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "\n"
#| "# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
#| "<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
#| "/dev/md0        7.9G   36M  7.4G   1% /srv/raid-0\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc\n"
"</userinput><computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0\n"
"</userinput><computeroutput>/dev/md0: 7.99GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0\n"
"</userinput><computeroutput>/dev/md0:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 01:54:24 2022\n"
"        Raid Level : raid0\n"
"        Array Size : 8378368 (7.99 GiB 8.58 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 01:54:24 2022\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"            Layout : -unknown-\n"
"        Chunk Size : 512K\n"
"\n"
"Consistency Policy : none\n"
"\n"
"              Name : debian:0  (local to host debian)\n"
"              UUID : a75ac628:b384c441:157137ac:c04cd98c\n"
"            Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8        0        0      active sync   /dev/sdb\n"
"       1       8       16        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0\n"
"</userinput><computeroutput>mke2fs 1.46.2 (28-Feb-2021)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 2094592 4k blocks and 524288 inodes\n"
"Filesystem UUID: ef077204-c477-4430-bf01-52288237bea0\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/raid-0\n"
"</userinput><computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0\n"
"</userinput><computeroutput># </computeroutput><userinput>df -h /srv/raid-0\n"
"</userinput><computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.8G   24K  7.4G   1% /srv/raid-0\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"           Version : 1.2\n"
"     Creation Time : Tue Jun 25 08:47:49 2019\n"
"        Raid Level : raid0\n"
"        Array Size : 8378368 (7.99 GiB 8.58 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 08:47:49 2019\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"        Chunk Size : 512K\n"
"\n"
"Consistency Policy : none\n"
"\n"
"              Name : mirwiz:0  (local to host debian)\n"
"              UUID : 146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"            Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       32        0      active sync   /dev/sdb\n"
"       1       8       48        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 2094592 4k blocks and 524288 inodes\n"
"Filesystem UUID: 413c3dff-ab5e-44e7-ad34-cf1a029cfe98\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.9G   36M  7.4G   1% /srv/raid-0\n"
"</computeroutput>"

msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It is also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgstr "La orden <command>mdadm --create</command> necesita varios parámetros: el nombre del volúmen a crear (<filename>/dev/md*</filename>, donde MD es acrónimo de <foreignphrase>múltiples dispositivos</foreignphrase> — «Multiple Device»), el nivel RAID, la cantidad de discos (que es obligatorio a pesar de que sea sólo importante con RAID-1 y superior), y los dispositivos físicos a utilizar. Una vez que creó el dispositivo, podemos utilizarlo como si fuese una partición normal, crear un sistema de archivos en él, montarlo, etc. Sepa que el que creáramos un volúmen RAID-0 como <filename>md0</filename> es sólo una coincidencia, la numeración del array no tiene correlación alguna con la cantidad de redundancia elegida. También es posible crear arrays RAID con nombre si se proveen los parámetros correctos a <command>mdadm</command>, como <filename>/dev/md/linear</filename> en lugar de <filename>/dev/md0</filename>."

msgid "Creation of a RAID-1 follows a similar fashion, the differences only being noticeable after the creation:"
msgstr "Crear un RAID-1 es similar, las diferencias sólo son notables luego:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: Note: this array has metadata at the start and\n"
#| "    may not be suitable as a boot device.  If you plan to\n"
#| "    store '/boot' on this device please ensure that\n"
#| "    your boot-loader understands md/v1.x metadata, or use\n"
#| "    --metadata=0.90\n"
#| "mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
#| "Continue creating array? </computeroutput><userinput>y</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md1 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "           Version : 1.2\n"
#| "     Creation Time : Tue Jun 25 10:21:22 2019\n"
#| "        Raid Level : raid1\n"
#| "        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
#| "     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
#| "      Raid Devices : 2\n"
#| "     Total Devices : 2\n"
#| "       Persistence : Superblock is persistent\n"
#| "\n"
#| "       Update Time : Tue Jun 25 10:22:03 2019\n"
#| "             State : clean, resyncing \n"
#| "    Active Devices : 2\n"
#| "   Working Devices : 2\n"
#| "    Failed Devices : 0\n"
#| "     Spare Devices : 0\n"
#| "\n"
#| "Consistency Policy : resync\n"
#| "\n"
#| "     Resync Status : 93% complete\n"
#| "\n"
#| "              Name : mirwiz:1  (local to host debian)\n"
#| "              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
#| "            Events : 16\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       64        0      active sync   /dev/sdd2\n"
#| "       1       8       80        1      active sync   /dev/sde\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "          State : clean\n"
#| "[...]\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde\n"
"</userinput><computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdc2) exceeds size (4189184K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y\n"
"</userinput><computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1\n"
"</userinput><computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:08:09 2022\n"
"             State : clean, resync\n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 13% complete\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 17\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       1       8       48        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Tue Jun 25 10:21:22 2019\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 10:22:03 2019\n"
"             State : clean, resyncing \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"     Resync Status : 93% complete\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 16\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       64        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"

msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgstr "<emphasis>SUGERENCIA</emphasis> RAID, discos y particiones"

#, fuzzy
#| msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions, and do not require full disks."
msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions as well, and do not require full disks."
msgstr "Como muestra nuestro ejemplo, puede construir dispositivos RAID con particiones de discos, no necesita discos completos."

msgid "A few remarks are in order. First, <command>mdadm</command> notices that the physical elements have different sizes; since this implies that some space will be lost on the bigger element, a confirmation is required."
msgstr "Son necesarios algunos comentarios. Primero, <command>mdadm</command> está al tanto que los elementos físicos tiene diferentes tamaños; se necesita confirmar ya que esto implicará que perderá espacio en el elemento más grande."

msgid "More importantly, note the state of the mirror. The normal state of a RAID mirror is that both disks have exactly the same contents. However, nothing guarantees this is the case when the volume is first created. The RAID subsystem will therefore provide that guarantee itself, and there will be a synchronization phase as soon as the RAID device is created. After some time (the exact amount will depend on the actual size of the disks…), the RAID array switches to the “active” or “clean” state. Note that during this reconstruction phase, the mirror is in a degraded mode, and redundancy isn't assured. A disk failing during that risk window could lead to losing all the data. Large amounts of critical data, however, are rarely stored on a freshly created RAID array before its initial synchronization. Note that even in degraded mode, the <filename>/dev/md1</filename> is usable, and a filesystem can be created on it, as well as some data copied on it."
msgstr "Lo que es más importante, revise el estado del espejo. El estado normal de un espejo RAID es que ambos discos tengan el mismo contenido. Sin embargo, nada garantiza que este sea el caso cuando se crea el volumen. Por lo tanto, el subsistema RAID dará esta garantía por su cuenta y, tan pronto como se crea el dispositivo RAID, habrá una fase de sincronización. Luego de un tiempo (cuánto exactamente dependerá del tamaño de los discos…), el array RAID cambiará al estado «active» (activo) o «clean» (limpio). Sepa que durante esta fase de reconstrucción el espejo se encuentra en modo degradado y no se asegura redundancia. Si falla un disco durante esta ventana de riesgo podrá perder toda la información. Sin embargo, rara vez se almacenan grandes cantidades de datos críticos en un array RAID creado recientemente antes de su sincronización inicial. Sepa que aún en modo degradado puede utilizar <filename>/dev/md1</filename> y puede crear en él un sistema de archivos así como también copiar datos."

msgid "<emphasis>TIP</emphasis> Starting a mirror in degraded mode"
msgstr "<emphasis>SUGERENCIA</emphasis> Inicio de un espejo en modo degradado"

msgid "Sometimes two disks are not immediately available when one wants to start a RAID-1 mirror, for instance because one of the disks one plans to include is already used to store the data one wants to move to the array. In such circumstances, it is possible to deliberately create a degraded RAID-1 array by passing <filename>missing</filename> instead of a device file as one of the arguments to <command>mdadm</command>. Once the data have been copied to the “mirror”, the old disk can be added to the array. A synchronization will then take place, giving us the redundancy that was wanted in the first place."
msgstr "A veces no se encuentran inmediatamente disponibles dos discos cuando uno desea iniciar un espejo RAID-1, por ejemplo porque uno de los discos que uno planea utilizar está siendo utilizado y contiene los datos que uno quiere almacenar en el array. En estas situaciones, es posible crear intencionalmente un array RAID-1 degradado si se utiliza <filename>missing</filename> en lugar del archivo del dispositivo como uno de los parámetros de <command>mdadm</command>. Una vez que copió los datos al «espejo», puede agregar el disco antiguo al array. Luego ocurrirá la fase de sincronización, proveyendo la redundancia que deseábamos en primer lugar."

msgid "<emphasis>TIP</emphasis> Setting up a mirror without synchronization"
msgstr "<emphasis>SUGERENCIA</emphasis> Configuración de un espejo sin sincronización"

msgid "RAID-1 volumes are often created to be used as a new disk, often considered blank. The actual initial contents of the disk is therefore not very relevant, since one only needs to know that the data written after the creation of the volume, in particular the filesystem, can be accessed later."
msgstr "Usualmente creará volúmenes RAID-1 para ser utilizados como un disco nuevo, generalmente considerados en blanco. El contenido inicial del disco no es realmente relevante, ya que uno sólo necesita saber que se podrán acceder luego a los datos escritos luego que creamos el volumen, en particular: el sistema de archivos."

msgid "One might therefore wonder about the point of synchronizing both disks at creation time. Why care whether the contents are identical on zones of the volume that we know will only be read after we have written to them?"
msgstr "Por lo tanto, uno podría preguntarse el sentido de sincronizar ambos discos al momento de crearlo. ¿Porqué importa si el contenido es idéntico en las zonas del volúmen que sabemos sólo serán accedidas luego que escribamos en ellas?"

msgid "Fortunately, this synchronization phase can be avoided by passing the <literal>--assume-clean</literal> option to <command>mdadm</command>. However, this option can lead to surprises in cases where the initial data will be read (for instance if a filesystem is already present on the physical disks), which is why it isn't enabled by default."
msgstr "Afortunadamente, puede evitar esta fase de sincronización con la opción <literal>--assume-clean</literal> de <command>mdadm</command>. Sin embargo, esta opción puede llevar a sorpresas en casos en el que se lean los datos iniciales (por ejemplo, si ya existe un sistema de archivos en los discos físicos), lo que explica porqué no es activada de forma predeterminada."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>failing</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Now let's see what happens when one of the elements of the RAID-1 array fails. <command>mdadm</command>, in particular its <literal>--fail</literal> option, allows simulating such a disk failure:"
msgstr "Veamos ahora qué sucede cuando falla uno de los elementos del array RAID-1. <command>mdadm</command>, su opción <literal>--fail</literal> en particular, permite simular tal fallo:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "       Update Time : Tue Jun 25 11:03:44 2019\n"
#| "             State : clean, degraded \n"
#| "    Active Devices : 1\n"
#| "   Working Devices : 1\n"
#| "    Failed Devices : 1\n"
#| "     Spare Devices : 0\n"
#| "\n"
#| "Consistency Policy : resync\n"
#| "\n"
#| "              Name : mirwiz:1  (local to host debian)\n"
#| "              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
#| "            Events : 20\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       -       0        0        0      removed\n"
#| "       1       8       80        1      active sync   /dev/sdd2\n"
#| "\n"
#| "       0       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde\n"
"</userinput><computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:15:34 2022\n"
"             State : clean, degraded \n"
"    Active Devices : 1\n"
"   Working Devices : 1\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       -       0        0        1      removed\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"       Update Time : Tue Jun 25 11:03:44 2019\n"
"             State : clean, degraded \n"
"    Active Devices : 1\n"
"   Working Devices : 1\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 20\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       -       0        0        0      removed\n"
"       1       8       80        1      active sync   /dev/sdd2\n"
"\n"
"       0       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "The contents of the volume are still accessible (and, if it is mounted, the applications don't notice a thing), but the data safety isn't assured anymore: should the <filename>sdd</filename> disk fail in turn, the data would be lost. We want to avoid that risk, so we'll replace the failed disk with a new one, <filename>sdf</filename>:"
msgstr "El contenido del volúmen continúa accesible (y, si está montado, las aplicaciones no lo notarán), pero ya no se asegura la seguridad de los datos: en caso que falle el disco <filename>sdd</filename>, perderá los datos. Deseamos evitar este riesgo, por lo que reemplazaremos el disco fallido con uno nuevo, <filename>sdf</filename>:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
#| "<computeroutput>mdadm: added /dev/sdf\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "      Raid Devices : 2\n"
#| "     Total Devices : 3\n"
#| "       Persistence : Superblock is persistent\n"
#| "\n"
#| "       Update Time : Tue Jun 25 11:09:42 2019\n"
#| "             State : clean, degraded, recovering \n"
#| "    Active Devices : 1\n"
#| "   Working Devices : 2\n"
#| "    Failed Devices : 1\n"
#| "     Spare Devices : 1\n"
#| "\n"
#| "Consistency Policy : resync\n"
#| "\n"
#| "    Rebuild Status : 27% complete\n"
#| "\n"
#| "              Name : mirwiz:1  (local to host debian)\n"
#| "              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
#| "            Events : 26\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       2       8       96        0      spare rebuilding   /dev/sdf\n"
#| "       1       8       80        1      active sync   /dev/sdd2\n"
#| "\n"
#| "       0       8       64        -      faulty   /dev/sde\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "       Update Time : Tue Jun 25 11:10:47 2019\n"
#| "             State : clean \n"
#| "    Active Devices : 2\n"
#| "   Working Devices : 2\n"
#| "    Failed Devices : 1\n"
#| "     Spare Devices : 0\n"
#| "\n"
#| "Consistency Policy : resync\n"
#| "\n"
#| "              Name : mirwiz:1  (local to host debian)\n"
#| "              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
#| "            Events : 39\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       2       8       96        0      active sync   /dev/sdd2\n"
#| "       1       8       80        1      active sync   /dev/sdf\n"
#| "\n"
#| "       0       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:25:34 2022\n"
"             State : clean, degraded, recovering \n"
"    Active Devices : 1\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 1\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 47% complete\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 39\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:25:34 2022\n"
"             State : clean\n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      active sync   /dev/sdf\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 11:09:42 2019\n"
"             State : clean, degraded, recovering \n"
"    Active Devices : 1\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 1\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 27% complete\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      spare rebuilding   /dev/sdf\n"
"       1       8       80        1      active sync   /dev/sdd2\n"
"\n"
"       0       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"       Update Time : Tue Jun 25 11:10:47 2019\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 39\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sdf\n"
"\n"
"       0       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "Here again, the kernel automatically triggers a reconstruction phase during which the volume, although still accessible, is in a degraded mode. Once the reconstruction is over, the RAID array is back to a normal state. One can then tell the system that the <filename>sde</filename> disk is about to be removed from the array, so as to end up with a classical RAID mirror on two disks:"
msgstr "Nuevamente, el núcleo automáticamente inicia una fase de reconstruciión durante la que el volúmen, aunque continúa disponible, se encuentra en modo degradado. Una vez finalizada la reconstrucción, el array RAID volverá a estado normal. Uno puede indicarle al sistema que eliminará el disco <filename>sde</filename> del array, para obtener un espejo RAID clásico en dos discos:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       2       8       96        0      active sync   /dev/sdd2\n"
#| "       1       8       80        1      active sync   /dev/sdf</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde\n"
"</userinput><computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      active sync   /dev/sdf\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sdf</computeroutput>"

msgid "From then on, the drive can be physically removed when the server is next switched off, or even hot-removed when the hardware configuration allows hot-swap. Such configurations include some SCSI controllers, most SATA disks, and external drives operating on USB or Firewire."
msgstr "De allí en adelante, puede quitar físicamente el dispositivo la próxima vez que se apague el servidor, o inclusive quitarlo en caliente si la configuración del hardware lo permite. Tales configuraciones incluyen algunos controladores SCSI, la mayoría de los discos SATA y discos externos USB o Firewire."

msgid "Backing up the Configuration"
msgstr "Respaldos de la configuración"

#, fuzzy
#| msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. In reality this leads to the RAID starting from the individual disks alternately - distributing the data also alternately, depending on which disk started the RAID in degraded mode Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgstr "La mayoría de los metadatos de los volúmenes RAID se almacenan directamente en los discos que componen dichos arrays, de esa forma el núcleo puede detectar el array y sus componentes y ensamblarlos automáticamente cuando inicia el sistema. Sin embargo, se recomienda respaldar esta configuración ya que esta detección no es infalible y, como no podía ser de otra forma, fallará precisamente en las circunstancias más sensibles. En nuestro ejemplo, si el fallo del disco <filename>sde</filename> hubiese sido real (en lugar de similada) y se hubiese reiniciado el sistema sin quitar el disco <filename>sde</filename>, éste podría ser utilizado nuevamente debido a haber sido probado durante el reinicio. El núcleo entonces tendría tres elementos físicos, cada uno de los cuales indica poseer la mitad del mismo volumen RAID. Otra fuente de confusión es cuando se consolidan en un servidor volúmenes RAID de dos servidores. Si los arrays funcionaban normalmente antes de quitar los discos, el núcleo podrá detectarlos y reconstruir los pares correctamente; pero si los discos mudados se encontraban agrupados como <filename>md1</filename> en el antiguo servidor pero el nuevo servidor ya posee un grupo <filename>md1</filename>, se modificará el nombre de uno de los espejos."

msgid "Backing up the configuration is therefore important, if only for reference. The standard way to do it is by editing the <filename>/etc/mdadm/mdadm.conf</filename> file, an example of which is listed here:"
msgstr "Por lo tanto es importante respaldar la configuración, aunque sea tan sólo como referencia. La forma estándar de realizarlo es editar el archivo <filename>/etc/mdadm/mdadm.conf</filename>, a continuación un ejemplo del mismo:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/mdadm/mdadm.conf</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "<command>mdadm</command> configuration file"
msgstr "Archivo de configuración de <command>mdadm</command>"

#, fuzzy
#| msgid ""
#| "# mdadm.conf\n"
#| "#\n"
#| "# !NB! Run update-initramfs -u after updating this file.\n"
#| "# !NB! This will ensure that initramfs has an uptodate copy.\n"
#| "#\n"
#| "# Please refer to mdadm.conf(5) for information about this file.\n"
#| "#\n"
#| "\n"
#| "# by default (built-in), scan all partitions (/proc/partitions) and all\n"
#| "# containers for MD superblocks. alternatively, specify devices to scan, using\n"
#| "# wildcards if desired.\n"
#| "DEVICE /dev/sd*\n"
#| "\n"
#| "# auto-create devices with Debian standard permissions\n"
#| "CREATE owner=root group=disk mode=0660 auto=yes\n"
#| "\n"
#| "# automatically tag new arrays as belonging to the local system\n"
#| "HOMEHOST &lt;system&gt;\n"
#| "\n"
#| "# instruct the monitoring daemon where to send mail alerts\n"
#| "MAILADDR root\n"
#| "\n"
#| "# definitions of existing MD arrays\n"
#| "ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c\n"
#| "\n"
#| "# This configuration was auto-generated on Tue, 25 Jun 2019 07:54:35 -0400 by mkconf"
msgid ""
"<![CDATA[# mdadm.conf\n"
"#\n"
"# !NB! Run update-initramfs -u after updating this file.\n"
"# !NB! This will ensure that initramfs has an uptodate copy.\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST <system>\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md/0  metadata=1.2 UUID=a75ac628:b384c441:157137ac:c04cd98c name=debian:0\n"
"ARRAY /dev/md/1  metadata=1.2 UUID=2dfb7fd5:e09e0527:0b5a905a:8334adb8 name=debian:1\n"
"# This configuration was auto-generated on Mon, 28 Feb 2022 01:53:48 +0100 by mkconf\n"
"]]>"
msgstr ""
"# mdadm.conf\n"
"#\n"
"# !NB! Run update-initramfs -u after updating this file.\n"
"# !NB! This will ensure that initramfs has an uptodate copy.\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# auto-create devices with Debian standard permissions\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c\n"
"\n"
"# This configuration was auto-generated on Tue, 25 Jun 2019 07:54:35 -0400 by mkconf"

msgid "One of the most useful details is the <literal>DEVICE</literal> option, which lists the devices where the system will automatically look for components of RAID volumes at start-up time. In our example, we replaced the default value, <literal>partitions containers</literal>, with an explicit list of device files, since we chose to use entire disks and not only partitions, for some volumes."
msgstr "Uno de los detalles más útiles es la opción <literal>DEVICE</literal>, que enumera los dispositivos en los que el sistema buscará componentes de un volumen RAID automáticamente cuando inicia. En nuestro ejemplo, reemplazamos el valor predeterminado, <literal>partitions containers</literal>, con una lista explícita de archivos de dispositivos, ya que para algunos volúmenes elegimos utilizar discos enteros y no sólo particiones."

msgid "The last two lines in our example are those allowing the kernel to safely pick which volume number to assign to which array. The metadata stored on the disks themselves are enough to re-assemble the volumes, but not to determine the volume number (and the matching <filename>/dev/md*</filename> device name)."
msgstr "Las dos últimas líneas en nuestro ejemplo son las que le permiten al núcleo seleccionar de forma segura qué número de volumen asignar a qué array. Los metadatos almacenados en los mismos discos son suficientes para reconstruir los volúmenes, pero no para determinar el número del mismo (y el nombre del dispositivo <filename>/dev/md*</filename> correspondiente)."

msgid "Fortunately, these lines can be generated automatically:"
msgstr "Afortunadamente, puede generar estas líneas automáticamente:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
#| "<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?\n"
"</userinput><computeroutput>ARRAY /dev/md/0  metadata=1.2 UUID=a75ac628:b384c441:157137ac:c04cd98c name=debian:0\n"
"ARRAY /dev/md/1  metadata=1.2 UUID=2dfb7fd5:e09e0527:0b5a905a:8334adb8 name=debian:1\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c</computeroutput>"

msgid "The contents of these last two lines doesn't depend on the list of disks included in the volume. It is therefore not necessary to regenerate these lines when replacing a failed disk with a new one. On the other hand, care must be taken to update the file when creating or deleting a RAID array."
msgstr "El contenido de estas dos últimas líneas no depende de la lista de discos incluidos en el volumen. Por lo tanto, no es necesario regenerar estas líneas cuando reemplace un disco fallido con uno nuevo. Por el otro lado, debe asegurarse de actualizar el archivo cuando cree o elimine un array RAID."

msgid "LVM, the <emphasis>Logical Volume Manager</emphasis>, is another approach to abstracting logical volumes from their physical supports, which focuses on increasing flexibility rather than increasing reliability. LVM allows changing a logical volume transparently as far as the applications are concerned; for instance, it is possible to add new disks, migrate the data to them, and remove the old disks, without unmounting the volume."
msgstr "LVM, el <emphasis>gestor de volúmenes lógicos</emphasis> («Logical Volume Manager»), es otra forma de abstraer volúmenes lógicos de su soporte físico, que se enfoca en ofrecer mayor flexibilidad en lugar de aumentar confiabilidad. LVM permite modificar un volumen lógico de forma transparente a las aplicaciones; por ejemplo, es posible agregar nuevos discos, migrar sus datos y eliminar discos antiguos sin desmontar el volumen."

msgid "LVM Concepts"
msgstr "Conceptos de LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>concept</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>PV</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Physical Volume</primary><see>PV</see>"
msgstr "<primary>Logical Volume Manager (Administrador de volúmenes lógicos)</primary>"

msgid "This flexibility is attained by a level of abstraction involving three concepts."
msgstr "Se consigue esta flexibilidad con un nivel de abstracción que incluye tres conceptos."

msgid "First, the PV (<emphasis>Physical Volume</emphasis>) is the entity closest to the hardware: it can be partitions on a disk, or a full disk, or even any other block device (including, for instance, a RAID array). Note that when a physical element is set up to be a PV for LVM, it should only be accessed via LVM, otherwise the system will get confused."
msgstr "Primero, el PV (<emphasis>volumen físico</emphasis>: «Physical Volume») es la entidad más cercana al hardware: pueden ser particiones en un disco, un disco completo o inclusive cualquier dispositivo de bloque (también un array RAID, por ejemplo). Sepa que cuando configura un elemento físico como PV para LVM, sólo debe acceder al mismo a través de LVM, de lo contrario confundirá al sistema."

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>VG</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Volume Group</primary><see>VG</see>"
msgstr "<primary>Logical Volume Manager (Administrador de volúmenes lógicos)</primary>"

msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there is no risk of using them directly."
msgstr "Puede agrupar una cantidad de PVs en un VG (<emphasis>grupo de volúmenes</emphasis>: «Volume Group»), lo que puede compararse con discos virtuales y extensibles. Los VGs son abstractos y no aparecerán como un archivo de dispositivo en la jerarquía <filename>/dev</filename>, por lo que no hay riesgo de utilizarlos directamente."

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>LV</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Logical Volume</primary><see>LV</see>"
msgstr "<primary>Logical Volume Manager (Administrador de volúmenes lógicos)</primary>"

msgid "The third kind of object is the LV (<emphasis>Logical Volume</emphasis>), which is a chunk of a VG; if we keep the VG-as-disk analogy, the LV compares to a partition. The LV appears as a block device with an entry in <filename>/dev</filename>, and it can be used as any other physical partition can be (most commonly, to host a filesystem or swap space)."
msgstr "El tercer tipo de objeto es el LV (<emphasis>volúmen lógico</emphasis>: «Logical Volume»), que es una porción de un VG; si continuamos con la analogía de un VG-como-disco, un LV se compara a una partición. El LV será un dispositivo de bloque que tendrá un elemento en <filename>/dev</filename> y puede utilizarlo como lo haría con cualquier partición física (usualmente, almacenar un sistema de archivos o espacio de intercambio)."

msgid "The important thing is that the splitting of a VG into LVs is entirely independent of its physical components (the PVs). A VG with only a single physical component (a disk for instance) can be split into a dozen logical volumes; similarly, a VG can use several physical disks and appear as a single large logical volume. The only constraint, obviously, is that the total size allocated to LVs can't be bigger than the total capacity of the PVs in the volume group."
msgstr "Lo importante es que la división de un VG en varios LVs es completamente independiente de sus componentes físicos (los PVs). Puede dividir un VG con un sólo componente físico (un disco por ejemplo) en una docena de volúmenes lógicos; similarmente, un VG puede utilizar varios discos físicos y aparecer como sólo un volúmen lógico grande. La única limitación es que, obviamente, el tamaño total asignado a un LV no puede ser mayor que la capacidad total de los PVs en el grupo de volúmenes."

msgid "It often makes sense, however, to have some kind of homogeneity among the physical components of a VG, and to split the VG into logical volumes that will have similar usage patterns. For instance, if the available hardware includes fast disks and slower disks, the fast ones could be clustered into one VG and the slower ones into another; chunks of the first one can then be assigned to applications requiring fast data access, while the second one will be kept for less demanding tasks."
msgstr "Generalmente tiene sentido, sin embargo, mantener el mismo tipo de homogeneidad entre los componentes físicos de un VG y dividir el VG en volúmenes lógicos que tendrán patrones de uso similares. Por ejemplo, si el hardware disponible incluye discos rápidos y discos lentos, podría agrupar los discos rápidos en un VG y los lentos en otro; puede asignar pedazos del primero a aplicaciones que necesiten acceso rápido a los datos y mantener el segundo para tareas menos exigentes."

msgid "In any case, keep in mind that an LV isn't particularly attached to any one PV. It is possible to influence where the data from an LV are physically stored, but this possibility isn't required for day-to-day use. On the contrary: when the set of physical components of a VG evolves, the physical storage locations corresponding to a particular LV can be migrated across disks (while staying within the PVs assigned to the VG, of course)."
msgstr "En cualquier caso, recuerde que un LV no está asociado especialmente a ningún PV. Es posible influenciar dónde se almacenarán físicamente los datos de un LV, pero esta posibilidad no es necesaria para el uso diario. Por el contrario, cuando evolucionan los componentes físicos de un VG, puede migrar las ubicaciones físicas del almacenamiento que corresponden a un LV particuar (siempre manteniéndose dentro de los PVs asignados al VG por supuesto)."

msgid "Setting up LVM"
msgstr "Configuración de LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>setup</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Let us now follow, step by step, the process of setting up LVM for a typical use case: we want to simplify a complex storage situation. Such a situation usually happens after some long and convoluted history of accumulated temporary measures. For the purposes of illustration, we'll consider a server where the storage needs have changed over time, ending up in a maze of available partitions split over several partially used disks. In more concrete terms, the following partitions are available:"
msgstr "Sigamos ahora, paso a paso, el proceso de configuración de LVM para un caso de uso típico: deseamos simplificar una situación compleja de almacenamiento. Situaciones como esta generalmente ocurren luego de una historia larga y complicada de medidas temporales que se acumulan. A modo ilustrativo utilizaremos un servidor en el que las necesidades de almacenamiento cambiaron con el tiempo, lo que culminó en un laberinto de particiones disponibles divididas en varios discos parcialmente utilizados. En términos más concretos, están disponibles las siguientes particiones:"

msgid "on the <filename>sdb</filename> disk, a <filename>sdb2</filename> partition, 4 GB;"
msgstr "en el disco <filename>sdb</filename>, una partición <filename>sdb2</filename> de 4Gb;"

msgid "on the <filename>sdc</filename> disk, a <filename>sdc3</filename> partition, 3 GB;"
msgstr "en el disco <filename>sdc</filename>, una partición <filename>sdc3</filename> de 3 GB;"

msgid "the <filename>sdd</filename> disk, 4 GB, is fully available;"
msgstr "el disco <filename>sdd</filename>, de 4 GB, completamente disponible;"

msgid "on the <filename>sdf</filename> disk, a <filename>sdf1</filename> partition, 4 GB; and a <filename>sdf2</filename> partition, 5 GB."
msgstr "en el disco <filename>sdf</filename>, una partición <filename>sdf1</filename> de 4 GB y una partición <filename>sdf2</filename> de 5GB."

msgid "In addition, let's assume that disks <filename>sdb</filename> and <filename>sdf</filename> are faster than the other two."
msgstr "Además, asumiremos que los discos <filename>sdb</filename> y <filename>sdf</filename> son más rápidos que los otros dos."

msgid "Our goal is to set up three logical volumes for three different applications: a file server requiring 5 GB of storage space, a database (1 GB) and some space for back-ups (12 GB). The first two need good performance, but back-ups are less critical in terms of access speed. All these constraints prevent the use of partitions on their own; using LVM can abstract the physical size of the devices, so the only limit is the total available space."
msgstr "Nuestro objetivo es configurar tres volúmenes lógicos para tres aplicaciones diferentes: un servidor de archivos que necesita 5 GB como espacio de almacenamiento, una base de datos (1 GB) y un poco de espacio para respaldos (12 GB). Los primeros dos necesitan buen rendimiento, pero los respaldos son menos críticos en cuanto a velocidad de acceso. Todas estas limitaciones evitan que simplemente utilicemos particiones; utilizar LVM puede abstraer el tamaño físico de los dispositivos, por lo que el único límite es el espacio total disponible."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">lvm2</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create PV</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The required tools are in the <emphasis role=\"pkg\">lvm2</emphasis> package and its dependencies. When they're installed, setting up LVM takes three steps, matching the three levels of concepts."
msgstr "El paquete <emphasis role=\"pkg\">lvm2</emphasis> y sus dependencias contienen las herramientas necesarias. Después de instalarlos, configurar LVM son tres pasos que coinciden con los tres niveles de conceptos."

msgid "First, we prepare the physical volumes using <command>pvcreate</command>:"
msgstr "Primero, prepararemos los volúmenes físicos utilizando <command>pvcreate</command>:"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>pvcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>pvdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n"
#| "# </computeroutput><userinput>pvdisplay</userinput>\n"
#| "<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
#| "  --- NEW Physical volume ---\n"
#| "  PV Name               /dev/sdb2\n"
#| "  VG Name               \n"
#| "  PV Size               4.00 GiB\n"
#| "  Allocatable           NO\n"
#| "  PE Size               0   \n"
#| "  Total PE              0\n"
#| "  Free PE               0\n"
#| "  Allocated PE          0\n"
#| "  PV UUID               z4Clgk-T5a4-C27o-1P0E-lIAF-OeUM-e7EMwq\n"
#| "\n"
#| "# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n"
#| "  Physical volume \"/dev/sdd\" successfully created.\n"
#| "  Physical volume \"/dev/sdf1\" successfully created.\n"
#| "  Physical volume \"/dev/sdf2\" successfully created.\n"
#| "# </computeroutput><userinput>pvdisplay -C</userinput><computeroutput>\n"
#| "  PV         VG Fmt  Attr PSize  PFree \n"
#| "  /dev/sdb2     lvm2 ---   4.00g  4.00g\n"
#| "  /dev/sdc3     lvm2 ---   3.00g  3.00g\n"
#| "  /dev/sdd      lvm2 ---   4.00g  4.00g\n"
#| "  /dev/sdf1     lvm2 ---   4.00g  4.00g\n"
#| "  /dev/sdf2     lvm2 ---  &lt;5.00g &lt;5.00g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay\n"
"</userinput><computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               yK0K6K-clbc-wt6e-qk9o-aUh9-oQqC-k1T71B\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n"
"  Physical volume \"/dev/sdd\" successfully created.\n"
"  Physical volume \"/dev/sdf1\" successfully created.\n"
"  Physical volume \"/dev/sdf2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay -C\n"
"</userinput><computeroutput>  PV         VG Fmt  Attr PSize PFree\n"
"  /dev/sdb2     lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3     lvm2 ---  3.00g 3.00g\n"
"  /dev/sdd      lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1     lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf2     lvm2 ---  5.00g 5.00g\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               z4Clgk-T5a4-C27o-1P0E-lIAF-OeUM-e7EMwq\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n"
"  Physical volume \"/dev/sdd\" successfully created.\n"
"  Physical volume \"/dev/sdf1\" successfully created.\n"
"  Physical volume \"/dev/sdf2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay -C</userinput><computeroutput>\n"
"  PV         VG Fmt  Attr PSize  PFree \n"
"  /dev/sdb2     lvm2 ---   4.00g  4.00g\n"
"  /dev/sdc3     lvm2 ---   3.00g  3.00g\n"
"  /dev/sdd      lvm2 ---   4.00g  4.00g\n"
"  /dev/sdf1     lvm2 ---   4.00g  4.00g\n"
"  /dev/sdf2     lvm2 ---  &lt;5.00g &lt;5.00g\n"
"</computeroutput>"

msgid "So far, so good; note that a PV can be set up on a full disk as well as on individual partitions of it. As shown above, the <command>pvdisplay</command> command lists the existing PVs, with two possible output formats."
msgstr "Hasta ahora, todo va bien; sepa que puede configurar un PV en un disco completo así como también en particiones individuales del mismo. Como mostramos, el programa <command>pvdisplay</command> enumera los PVs existentes, con dos formatos de salida posibles."

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>vgcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>vgdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "Now let's assemble these physical elements into VGs using <command>vgcreate</command>. We'll gather only PVs from the fast disks into a <filename>vg_critical</filename> VG; the other VG, <filename>vg_normal</filename>, will also include slower elements."
msgstr "Ahora agruparemos estos elementos físicos en VGs utilizando <command>vgcreate</command>. Reuniremos PVs de los discos rápidos en el VG <filename>vg_critical</filename>; el otro VG, <filename>vg_normal</filename> también incluirá los elementos más lentos."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create VG</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay</userinput>\n"
#| "<computeroutput>  --- Volume group ---\n"
#| "  VG Name               vg_critical\n"
#| "  System ID             \n"
#| "  Format                lvm2\n"
#| "  Metadata Areas        2\n"
#| "  Metadata Sequence No  1\n"
#| "  VG Access             read/write\n"
#| "  VG Status             resizable\n"
#| "  MAX LV                0\n"
#| "  Cur LV                0\n"
#| "  Open LV               0\n"
#| "  Max PV                0\n"
#| "  Cur PV                2\n"
#| "  Act PV                2\n"
#| "  VG Size               7.99 GiB\n"
#| "  PE Size               4.00 MiB\n"
#| "  Total PE              2046\n"
#| "  Alloc PE / Size       0 / 0   \n"
#| "  Free  PE / Size       2046 / 7.99 GiB\n"
#| "  VG UUID               wAbBjx-d82B-q7St-0KFf-z40h-w5Mh-uAXkNZ\n"
#| "\n"
#| "# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
#| "<computeroutput>  Volume group \"vg_normal\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay -C</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize   VFree  \n"
#| "  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n"
#| "  vg_normal     3   0   0 wz--n- &lt;11.99g &lt;11.99g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1\n"
"</userinput><computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay\n"
"</userinput><computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               7.99 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2046\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2046 / 7.99 GiB\n"
"  VG UUID               JgFWU3-emKg-9QA1-stPj-FkGX-mGFb-4kzy1G\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2\n"
"</userinput><computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C\n"
"</userinput><computeroutput><![CDATA[  VG          #PV #LV #SN Attr   VSize   VFree  \n"
"  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n"
"  vg_normal     3   0   0 wz--n- <11.99g <11.99g\n"
"]]></computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               7.99 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2046\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2046 / 7.99 GiB\n"
"  VG UUID               wAbBjx-d82B-q7St-0KFf-z40h-w5Mh-uAXkNZ\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize   VFree  \n"
"  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n"
"  vg_normal     3   0   0 wz--n- &lt;11.99g &lt;11.99g\n"
"</computeroutput>"

msgid "Here again, commands are rather straightforward (and <command>vgdisplay</command> proposes two output formats). Note that it is quite possible to use two partitions of the same physical disk into two different VGs. Note also that we used a <filename>vg_</filename> prefix to name our VGs, but it is nothing more than a convention."
msgstr "Aquí también los programas son bastante directos (y <command>vgdisplay</command> también propone dos formatos de salida). Sepa que es posible utilizar dos particiones del mismo disco físico en dos VGs diferentes. Además utilizamos el prefijo <filename>vg_</filename> para el nombre de nuestros VGs, pero es sólo una convención."

msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgstr "Ahora contamos con dos «discos virtuales», de alrededor 8 GB y 12 GB de tamaño respectivamente. Ahora los repartiremos en «particiones virtuales» (LVs). Esto involucra el programa <command>lvcreate</command> y una sintaxis ligeramente más compleja:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create LV</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>lvcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>lvdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_files\" created.\n"
#| "# </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput>  --- Logical volume ---\n"
#| "  LV Path                /dev/vg_critical/lv_files\n"
#| "  LV Name                lv_files\n"
#| "  VG Name                vg_critical\n"
#| "  LV UUID                W6XT08-iBBx-Nrw2-f8F2-r2y4-Ltds-UrKogV\n"
#| "  LV Write Access        read/write\n"
#| "  LV Creation host, time debian, 2019-11-30 22:45:46 -0500\n"
#| "  LV Status              available\n"
#| "  # open                 0\n"
#| "  LV Size                5.00 GiB\n"
#| "  Current LE             1280\n"
#| "  Segments               2\n"
#| "  Allocation             inherit\n"
#| "  Read ahead sectors     auto\n"
#| "  - currently set to     256\n"
#| "  Block device           254:0\n"
#| "\n"
#| "# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_base\" created.\n"
#| "# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal</userinput>\n"
#| "<computeroutput>  Rounding up size to full physical extent 11.98 GiB\n"
#| "  Logical volume \"lv_backups\" created.\n"
#| "# </computeroutput><userinput>lvdisplay -C</userinput>\n"
#| "<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_base    vg_critical -wi-a---  1.00g                                           \n"
#| "  lv_files   vg_critical -wi-a---  5.00g                                           \n"
#| "  lv_backups vg_normal   -wi-a--- 11.98g</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>lvdisplay\n"
"</userinput><computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical\n"
"</userinput><computeroutput>  Logical volume \"lv_files\" created.\n"
"# </computeroutput><userinput>lvdisplay\n"
"</userinput><computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                Nr62xe-Zu7d-0u3z-Yyyp-7Cj1-Ej2t-gw04Xd\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time debian, 2022-03-01 00:17:46 +0100\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical\n"
"</userinput><computeroutput>  Logical volume \"lv_base\" created.\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal\n"
"</userinput><computeroutput>  Rounding up size to full physical extent 11.98 GiB\n"
"  Rounding up size to full physical extent 11.98 GiB\n"
"  Logical volume \"lv_backups\" created.\n"
"# </computeroutput><userinput>lvdisplay -C\n"
"</userinput><computeroutput>  LV         VG          Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_base    vg_critical -wi-a-----  1.00g                                                    \n"
"  lv_files   vg_critical -wi-a-----  5.00g                                                    \n"
"  lv_backups vg_normal   -wi-a----- 11.98g             \n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created.\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                W6XT08-iBBx-Nrw2-f8F2-r2y4-Ltds-UrKogV\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time debian, 2019-11-30 22:45:46 -0500\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           254:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created.\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal</userinput>\n"
"<computeroutput>  Rounding up size to full physical extent 11.98 GiB\n"
"  Logical volume \"lv_backups\" created.\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 11.98g</computeroutput>"

msgid "Two parameters are required when creating logical volumes; they must be passed to the <command>lvcreate</command> as options. The name of the LV to be created is specified with the <literal>-n</literal> option, and its size is generally given using the <literal>-L</literal> option. We also need to tell the command what VG to operate on, of course, hence the last parameter on the command line."
msgstr "Necesita dos parámetros cuando cree volúmenes lógicos; debe proveerlos a <command>lvcreate</command> como opciones. Especificará el nombre del LV a crear con la opción <literal>-n</literal> y, usualmente, su tamaño con la opción <literal>-L</literal>. Por supuesto, también necesitaremos indicarle sobre qué VG trabajar, de allí el último parámetro en la ejecución."

msgid "<emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> options"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> Opciones de <command>lvcreate</command>"

msgid "The <command>lvcreate</command> command has several options to allow tweaking how the LV is created."
msgstr "El programa <command>lvcreate</command> tiene varias opciones que modifican la creación del LV."

msgid "Let's first describe the <literal>-l</literal> option, with which the LV's size can be given as a number of blocks (as opposed to the “human” units we used above). These blocks (called PEs, <emphasis>physical extents</emphasis>, in LVM terms) are contiguous units of storage space in PVs, and they can't be split across LVs. When one wants to define storage space for an LV with some precision, for instance to use the full available space, the <literal>-l</literal> option will probably be preferred over <literal>-L</literal>."
msgstr "Primero describamos la opción <literal>-l</literal>, con la que puede indicar el tamaño del LV como una cantidad de bloques (en lugar de las unidades «humanas» que utilizamos en el ejemplo). Estos bloques (PEs en términos de LVM, <emphasis>extensiones físicas</emphasis>: «physical extents») son unidades de espacio de almacenamiento contiguo en los PVs, y no pueden dividirse entre LVs. Cuando uno desea definir el espacio de almacenamiento para un LV con cierta precisión, por ejemplo para utilizar todo el espacio disponible, generalmente es preferible utilizar la opción <literal>-l</literal> en lugar de <literal>-L</literal>."

msgid "It is also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgstr "También es posible sugerir la ubicación física de un LV para que se almacenen sus extensiones en un PV particular (obviamente limitándose a aquellas asignadas al VG). Dado que sabemos que <filename>sdb</filename> es más rápido que <filename>sdf</filename>, desearíamos almacenar <filename>lv_base</filename> allí si nos interesa darle una ventaja al servidor de base de datos comparado con el servidor de archivos. De esa forma, la orden a ejecutar sería: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Sepa que esta ejecución puede fallar si el PV no posee suficientes extensiones libres. En nuestro ejemplo, probablemente deberíamos crear <filename>lv_base</filename> antes que <filename>lv_files</filename> para evitar esta situación — o liberar algo de espacio en <filename>sdb2</filename> con el programa <command>pvmove</command>."

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>pvmove</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "Logical volumes, once created, end up as block device files in <filename>/dev/mapper/</filename>:"
msgstr "Una vez que creó los volúmenes lógicos, éstos serán archivos de dispositivos de bloque en <filename>/dev/mapper/</filename>:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/dev</filename></primary><secondary><filename>/dev/mapper/</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>device</primary><secondary>block</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
#| "<computeroutput>total 0\n"
#| "crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
#| "# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
#| "<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
#| "brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
#| "brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"crw------- 1 root root 10, 236 Mar  1 00:17 control\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:19 vg_critical-lv_base -> ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:17 vg_critical-lv_files -> ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:19 vg_normal-lv_backups -> ../dm-2 ]]>\n"
"# </computeroutput><userinput>ls -l /dev/dm-*\n"
"</userinput><computeroutput>brw-rw---- 1 root disk 253, 0 Mar  1 00:17 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Mar  1 00:19 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Mar  1 00:19 /dev/dm-2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"

msgid "<emphasis>NOTE</emphasis> Auto-detecting LVM volumes"
msgstr "<emphasis>NOTA</emphasis> Autodetección de volúmenes LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>lvm2-activation.service</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>vgchange</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "When the computer boots, the <filename>lvm2-activation</filename> systemd service unit executes <command>vgchange -aay</command> to “activate” the volume groups: it scans the available devices; those that have been initialized as physical volumes for LVM are registered into the LVM subsystem, those that belong to volume groups are assembled, and the relevant logical volumes are started and made available. There is therefore no need to edit configuration files when creating or modifying LVM volumes."
msgstr "Cuando inicia el equipo, el <filename>lvm2-activation</filename> systemd service unit ejecuta <command>vgchange -aay</command> para \"activar\" grupos de volúmenes: escanea los dispositivos disponibles; registra en el subsistema LVM a aquellos que fueron inicializados como volúmenes físicos para LVM, agrupa aquellos que pertenecen a grupos de volúmenes e inicializa y hace disponibles los volúmenes lógicos relevantes. Por lo tanto, no es necesario editar archivos de configuración cuando crea o modifica volúmenes LVM."

msgid "Note, however, that the layout of the LVM elements (physical and logical volumes, and volume groups) is backed up in <filename>/etc/lvm/backup</filename>, which can be useful in case of a problem (or just to sneak a peek under the hood)."
msgstr "Sepa, sin embargo, que se respalda la distribución de los elementos de LVM (volúmenes físicos y loǵicos y grupos de volúmenes) en <filename>/etc/lvm/backup</filename>, lo cual puede ser útil en caso de algún problema (o tan sólo para espiar tras bambalinas)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/lvm/backup</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "To make things easier, convenience symbolic links are also created in directories matching the VGs:"
msgstr "Para hacer las cosas más sencillas, se crean enlaces simbólicos convenientes en directorios que coinciden con los VGs:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
#| "<computeroutput>total 0\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
#| "# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
#| "<computeroutput>total 0\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:19 lv_base -> ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:17 lv_files -> ../dm-0 ]]>\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:19 lv_backups -> ../dm-2 ]]>\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"

msgid "The LVs can then be used exactly like standard partitions:"
msgstr "Puede utilizar LVs exactamente de la misma forma que particiones estándar:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
#| "<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
#| "Discarding device blocks: done                            \n"
#| "Creating filesystem with 3140608 4k blocks and 786432 inodes\n"
#| "Filesystem UUID: b9e6ed2f-cb37-43e9-87d8-e77568446225\n"
#| "Superblock backups stored on blocks: \n"
#| "\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n"
#| "\n"
#| "Allocating group tables: done                            \n"
#| "Writing inode tables: done                            \n"
#| "Creating journal (16384 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "\n"
#| "# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_normal-lv_backups   12G   41M   12G   1% /srv/backups\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
#| "<computeroutput>[...]\n"
#| "/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
#| "/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
#| "/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups\n"
"</userinput><computeroutput>mke2fs 1.46.2 (28-Feb-2021)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 3140608 4k blocks and 786432 inodes\n"
"Filesystem UUID: 7eaf0340-b740-421e-96b2-942cdbf29cb3\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/backups\n"
"</userinput><computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups\n"
"</userinput><computeroutput># </computeroutput><userinput>df -h /srv/backups\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   24K   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab\n"
"</userinput><computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 3140608 4k blocks and 786432 inodes\n"
"Filesystem UUID: b9e6ed2f-cb37-43e9-87d8-e77568446225\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>S.ficheros                 Tamaño Usados  Disp Uso% Montado en\n"
"/dev/mapper/vg_normal-lv_backups   12G   41M    12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"

msgid "From the applications' point of view, the myriad small partitions have now been abstracted into one large 12 GB volume, with a friendlier name."
msgstr "Desde el punto de vista de las aplicaciones, todas las pequeñas particiones se encuentran abstraídas en un gran volumen de 12 GB con un nombre más amigable."

msgid "LVM Over Time"
msgstr "LVM en el tiempo"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>lvresize</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>resize2fs</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>resize</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>resize LV</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Even though the ability to aggregate partitions or physical disks is convenient, this is not the main advantage brought by LVM. The flexibility it brings is especially noticed as time passes, when needs evolve. In our example, let's assume that new large files must be stored, and that the LV dedicated to the file server is too small to contain them. Since we haven't used the whole space available in <filename>vg_critical</filename>, we can grow <filename>lv_files</filename>. For that purpose, we'll use the <command>lvresize</command> command, then <command>resize2fs</command> to adapt the filesystem accordingly:"
msgstr "Aún cuando es conveniente poder agrupar particiones o discos físicos, esta no es la principal ventaja que provee LVM. La flexibilidad que brinda es especialmente notable con el paso del tiempo cuando evolucionan las necesidades. En nuestro ejemplo, supongamos que debemos almacenar nuevos archivos grandes y que el LV dedicado al servidor de archivos es demasiado pequeño para contenerlos. Debido a que no utilizamos todo el espacio disponibleen <filename>vg_critical</filename>, podemos aumentar el tamaño de <filename>lv_files</filename>. Para ello, utilizaremos el programa <command>lvresize</command> y luego <command>resize2fs</command> para adaptar el sistema de archivos según corresponda:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  4.9G  4.2G  485M  90% /srv/files\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 5.00g\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   2   2   0 wz--n- 7.99g 1.99g\n"
#| "# </computeroutput><userinput>lvresize -L 6G vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\n"
#| "  Logical volume vg_critical/lv_files successfully resized.\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
#| "  lv_files vg_critical -wi-ao---- 6.00g\n"
#| "# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
#| "<computeroutput>resize2fs 1.44.5 (15-Dec-2018)\n"
#| "Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
#| "old_desc_blocks = 1, new_desc_blocks = 1\n"
#| "The filesystem on /dev/vg_critical/lv_files is now 1572864 (4k) blocks long.\n"
#| "\n"
#| "# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  5.9G  4.2G  1.5G  75% /srv/files</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  4.9G  4.2G  485M  90% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files\n"
"</userinput><computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 5.00g                                                    \n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 7.99g 1.99g\n"
"# </computeroutput><userinput>lvresize -L 6G vg_critical/lv_files\n"
"</userinput><computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\n"
"  Logical volume vg_critical/lv_files successfully resized.\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files\n"
"</userinput><computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 6.00g                                                    \n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files\n"
"</userinput><computeroutput>resize2fs 1.46.2 (28-Feb-2021)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1572864 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.9G  4.2G  1.5G  75% /srv/files\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>S.ficheros     Tamaño Usados  Disp Uso% Montado en\n"
"/dev/mapper/vg_critical-lv_files  4.9G  4.2G  485M  90% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 7.99g 1.99g\n"
"# </computeroutput><userinput>lvresize -L 6G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\n"
"  Logical volume vg_critical/lv_files successfully resized.\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 6.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.44.5 (15-Dec-2018)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1572864 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>S.ficheros     Tamaño Usados  Disp Uso% Montado en\n"
"/dev/mapper/vg_critical-lv_files  5.9G  4.2G  1.5G  75% /srv/files</computeroutput>"

msgid "<emphasis>CAUTION</emphasis> Resizing filesystems"
msgstr "<emphasis>PRECAUCIÓN</emphasis> Redimensión de sistemas de archivos"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>filesystem</primary><secondary>resize</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It is rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgstr "No todos los sistemas de archivos pueden cambiar su tamaño fácilmente; modificar un volúmen, por lo tanto, requerirá primero desmotar el sistema de archivos y volver a montarlo luego. Por supuesto, si uno desea disminuir el espacio asignado a un LV, primero debe reducir el sistema de archivos; el orden se invierte cuando el cambio de tamaño es en la otra dirección: primero debe aumentar el volumen lógico antes que el sistema de archivos que contiene. Es bastante directo ya que en ningún momento el sistema de archivos puede ser más grande que el dispositivo de bloques en el que reside (tanto cuando éste dispositivo sea una partición física o volumen lógico)."

msgid "The ext3, ext4 and xfs filesystems can be grown online, without unmounting; shrinking requires an unmount. The reiserfs filesystem allows online resizing in both directions. The venerable ext2 allows neither, and always requires unmounting."
msgstr "Los sistemas de archivos ext3, ext4 y xfs pueden agrandarse sin desmontarlos; deberá desmontarlos para reducirlos. El sistema de archivos reiserfs permite cambiar el tamaño en cualquier dirección sin desmontarlo. El venerable ext2 no lo permite y siempre necesitará desmontarlo primero."

msgid "We could proceed in a similar fashion to extend the volume hosting the database, only we've reached the VG's available space limit:"
msgstr "Podemos proceder de una forma similar para extender el volumen que almacena la base de datos, sólo que habremos alcanzado el límite de espacio disponible del VG:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base  976M  882M   28M  97% /srv/base\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree   \n"
#| "  vg_critical   2   2   0 wz--n- 7.99g 1016.00m</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/\n"
"</userinput><computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  974M  883M   25M  98% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput>  VG          #PV #LV #SN Attr   VSize VFree   \n"
"  vg_critical   2   2   0 wz--n- 7.99g 1016.00m\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>S.ficheros     Tamaño Usados  Disp Uso% Montado en\n"
"/dev/mapper/vg_critical-lv_base  976M  882M   28M  97% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree   \n"
"  vg_critical   2   2   0 wz--n- 7.99g 1016.00m</computeroutput>"

#, fuzzy
#| msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb1</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb3</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgstr "Esto no importa ya que LVM permite agregar volúmenes físicos a grupos de volúmenes existentes. Por ejemplo, podríamos haber notado que la partición <filename>sdb1</filename>, que se encontraba fuera de LVM hasta ahora, sólo contenía archivos que podían ser movidos a <filename>lv_backups</filename>. Ahora podremos reciclarla e integrarla al grupo de volúmenes y reclamar así espacio disponible. Este es el propósito del programa <command>vgextend</command>. Por supuesto, debe prepara la partición como un volúmen físico antes. Una vez que extendió el VG, puede ejecutar órdenes similares a las anteriores para aumentar el volumen lógico y luego el sistema de archivos:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb1\" successfully created.\n"
#| "# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
#| "  vg_critical   3   2   0 wz--n- &lt;9.99g &lt;1.99g\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base  2.0G  882M  994M  48% /srv/base</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb3\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdb3\" successfully created.\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb3\n"
"</userinput><computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput><![CDATA[  VG          #PV #LV #SN Attr   VSize   VFree \n"
"  vg_critical   3   2   0 wz--n- <12.99g <5.99g ]]>\n"
"# </computeroutput><userinput>lvresize -L 2G vg_critical/lv_base\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_base\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/\n"
"</userinput><computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  886M  991M  48% /srv/base\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created.\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
"  vg_critical   3   2   0 wz--n- &lt;9.99g &lt;1.99g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>S.ficheros                Tamaño Usados  Disp Uso% Montado en\n"
"/dev/mapper/vg_critical-lv_base  2.0G  882M  994M  48% /srv/base</computeroutput>"

msgid "<emphasis>GOING FURTHER</emphasis> Advanced LVM"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> LVM avanzado"

msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance, to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgstr "LVM también se adapta a usuarios más avanzados que pueden especificar a mano muchos detalles. Por ejemplo, un administrador puede adaptar el tamaño de los bloques que componen a los volúmenes lógicos y físicos así como también la distribución física. También es posible mover bloques entre PVs, por ejemplo, para ajustar el rendimiento o, lo que es menos interesante, liberar un PV cuando uno necesite extraer el disco físico correspondiente del VG (ya sea para asociarlo a otro VG o para eliminarlo completamente de LVM). Las páginas de manual que describen estos programas generalmente son claras y detalladas. Un buen punto de partida es la página de manual <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry>."

msgid "RAID or LVM?"
msgstr "¿RAID o LVM?"

msgid "RAID and LVM both bring indisputable advantages as soon as one leaves the simple case of a desktop computer with a single hard disk where the usage pattern doesn't change over time. However, RAID and LVM go in two different directions, with diverging goals, and it is legitimate to wonder which one should be adopted. The most appropriate answer will of course depend on current and foreseeable requirements."
msgstr "Tanto RAID como LVM proveen ventajas indiscutibles tan pronto como uno deja el caso simple de un equipo de escritorio con sólo un disco duro en el que los patrones de uso no cambian con el tiempo. Sin embargo, RAID y LVM toman direcciones diferentes, con objetivos distintos y es legítimo preguntarse cuál utilizar. La respuestas más apropiada, por supuesto, dependerá de los requerimientos actuales y previstos."

msgid "There are a few simple cases where the question doesn't really arise. If the requirement is to safeguard data against hardware failures, then obviously RAID will be set up on a redundant array of disks, since LVM doesn't really address this problem. If, on the other hand, the need is for a flexible storage scheme where the volumes are made independent of the physical layout of the disks, RAID doesn't help much and LVM will be the natural choice."
msgstr "Hay unos pocos casos simples en los que no surge esta pregunta. Si los requisitos son proteger los datos contra fallos de hardware, obviamente entonces configurará RAID en un array de discos redundantes ya que LVM no soluciona este problema realmente. Si, por el otro lado, necesita un esquema de almacenamiento flexible en el que los volúmenes sean independientes de la distribución física de los discos, RAID no es de mucha ayuda y LVM es la elección natural."

msgid "<emphasis>NOTE</emphasis> If performance matters…"
msgstr "<emphasis>NOTA</emphasis> Si el rendimiento importa…"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>SSD</primary>"
msgstr "<primary>RAID</primary>"

msgid "<primary>Solid State Drives</primary><see>SSD</see>"
msgstr ""

#, fuzzy
#| msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgstr "Si la velocidad de entrada/salida es esencial, especialmente en cuanto a tiempos de acceso, utilizar LVM y/o RAID es una de las numerosas combinaciones que tendrán impacto en el rendimiento y esto influenciará las decisiones sobre cuál elegir. Sin embargo, estas diferencias de rendimiento son realmente mínimas y sólo podrán ser medidas en unos pocos casos de uso. Si importa el rendimiento, la mejor ganancia que puede obtener sería utilizar medios de almacenamiento no rotativos (<indexterm><primary>SSD</primary></indexterm><emphasis>discos de estado sólido</emphasis> o SSDs, «Solid State Drives»); su costo por megabyte es más alto que otros discos duros estándar y su capacidad generalmente es menor, pero proveen un rendimiento excelente para accesos aleatorios. Si el patrón de uso incluye muchas operaciones de entrada/salida distribuídas en todo el sistema de archivos, por ejemplos en bases de datos donde se ejecutan frecuentemente consultas complejas, la ventaja de ejecutarlas en un SSD sobrepasan grandemente cualquier ganancia de elegir LVM sobre RAID o su inversa. En estas situaciones debe realizar su selección según consideraciones diferentes a sólo la velocidad ya que puede controlar este aspecto más fácilmente utilizando SSDs."

msgid "The third notable use case is when one just wants to aggregate two disks into one volume, either for performance reasons or to have a single filesystem that is larger than any of the available disks. This case can be addressed both by a RAID-0 (or even linear-RAID) and by an LVM volume. When in this situation, and barring extra constraints (for instance, keeping in line with the rest of the computers if they only use RAID), the configuration of choice will often be LVM. The initial set up is barely more complex, and that slight increase in complexity more than makes up for the extra flexibility that LVM brings if the requirements change or if new disks need to be added."
msgstr "El tercer caso notable de uso es uno en el que uno sólo desea agrupar dos discos en un solo volumen, ya sea por razones de rendimiento o para tener sólo un sistema de archivos más grande que cualquiera de los discos disponibles. Puede solucionar este caso tanto con RAID-0 (o inclusive RAID lineal) como con un volumen LVM. Cuando se encuentre con esta situación, y sin limitaciones adicionales (por ejemplo, ser consistente con el resto de los equipos si sólo utilizan RAID), generalmente elegirá utilizar LVM. La configuración inicial es ligeramente más compleja y es compensada por la flexibilidad adicional que provee LVM si cambian los requisitos o necesita agregar nuevos discos."

msgid "Then of course, there is the really interesting use case, where the storage system needs to be made both resistant to hardware failure and flexible when it comes to volume allocation. Neither RAID nor LVM can address both requirements on their own; no matter, this is where we use both at the same time — or rather, one on top of the other. The scheme that has all but become a standard since RAID and LVM have reached maturity is to ensure data redundancy first by grouping disks in a small number of large RAID arrays, and to use these RAID arrays as LVM physical volumes; logical partitions will then be carved from these LVs for filesystems. The selling point of this setup is that when a disk fails, only a small number of RAID arrays will need to be reconstructed, thereby limiting the time spent by the administrator for recovery."
msgstr "Luego por supuesto, está el caso de uso realmente interesante, en el que el sistema de almacenamiento debe ser resistente a fallos de hardware y también flexible en cuanto a la asignación de volúmenes. Ni RAID ni LVM pueden solucionar ambos requisitos por sí mismos; no importa, esta es la situación en la que utilizaremos ambos al mismo tiempo — o más bien, uno sobre el otro. El esquema más utilizado, casi un estándar desde que RAID y LVM son suficientemente maduros, es asegurar redundancia en los datos primero agrupando discos en una cantidad menor de arrays RAID grandes y luego utilizar estos arrays RAID como volúmenes físicos LVM; conseguirá las particiones lógicas para los sistemas de archivo a partir de estos LVs. El punto fuerte de esta configuración es que, cuando falla un disco, sólo necesitará reconstruir una pequeña cantidad de arrays RAID, de esa forma limitando el tiempo que utiliza el administrador en recuperarlo."

#, fuzzy
#| msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited videoclips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited video clips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgstr "Veamos un caso concreto: el departamento de relaciones públicas en Falcot Corp necesita una estación de trabajo para edición de video, pero el presupuesto del mismo no permite invertir en hardware de gama alta desde el principio. Se decide entonces utilizar el presupuesto en hardware específico a la naturaleza gráfica del trabajo (pantalla y tarjeta de video) y utilizar hardware genérico para el almacenamiento. Sin embargo, como es públicamente conocido, el video digital tiene ciertas necesidades particulares para su almacenamiento: una gran cantidad de datos que guardar y es importante la tasa de rendimiento para leer y escribir estos datos es importante para el rendimiento general del sistema (más que el tiempo típico de acceso, por ejemplo). Necesita cumplir estos requisitos con hardware genérico, en este caso dos discos duros SATA de 300 Gb; también debe hacer que los datos de sistema, y algunos datos de usuarios, puedan resistir fallos en el hardware. Los videos editados deben estar seguros, pero los videos que todavía no fueron editados son menos críticos ya que todavía se encuentran en cinta."

msgid "RAID-1 and LVM are combined to satisfy these constraints. The disks are attached to two different SATA controllers to optimize parallel access and reduce the risk of a simultaneous failure, and they therefore appear as <filename>sda</filename> and <filename>sdc</filename>. They are partitioned identically along the following scheme:"
msgstr "Satisfacemos estas limitaciones combinando RAID-1 y LVM. Conectamos los discos a dos controladoras SATA diferentes para optimizar el acceso en paralelo y reducir el riesgo de fallos simultáneos, por lo que aparecerán como <filename>sda</filename> y <filename>sdc</filename>. Los particionamos de forma idéntica según el siguiente esquema:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
#| "<computeroutput>\n"
#| "Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
#| "Units: sectors of 1 * 512 = 512 bytes\n"
#| "Sector size (logical/physical): 512 bytes / 512 bytes\n"
#| "I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
#| "Disklabel type: dos\n"
#| "Disk identifier: 0x00039a9f\n"
#| "\n"
#| "Device    Boot     Start       End   Sectors Size Id Type\n"
#| "/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
#| "/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
#| "/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
#| "/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
#| "/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
#| "/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>sfdisk -l /dev/sda\n"
"</userinput><computeroutput>Disk /dev/sda: 894.25 GiB, 960197124096 bytes, 1875385008 sectors\n"
"Disk model: SAMSUNG MZ7LM960\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: gpt\n"
"Disk identifier: BB14C130-9E9A-9A44-9462-6226349CA012\n"
"\n"
"Device         Start        End   Sectors   Size Type\n"
"/dev/sda1        2048       4095      2048     1M BIOS boot\n"
"/dev/sda2        4096  100667391 100663296    48G Linux RAID\n"
"/dev/sda3   100667392  134221823  33554432    16G Linux RAID\n"
"/dev/sda4   134221824  763367423 629145600   300G Linux RAID\n"
"/dev/sda5   763367424 1392513023 629145600   300G Linux RAID\n"
"/dev/sda6  1392513024 1875384974 482871951 230.3G Linux LVM\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"

msgid "The first partitions of both disks are BIOS boot partitions."
msgstr ""

#, fuzzy
#| msgid "The first partitions of both disks (about 1 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgid "The next two partitions <filename>sda2</filename> and <filename>sdc2</filename> (about 48 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgstr "Agrupamos las primeras particiones de ambos discos (de alrededor de 1 GB) en un volúmen RAID-1, <filename>md0</filename>. Utilizamos el espejo directamente para almacenar el sistema de archivos raíz."

#, fuzzy
#| msgid "The <filename>sda2</filename> and <filename>sdc2</filename> partitions are used as swap partitions, providing a total 2 GB of swap space. With 1 GB of RAM, the workstation has a comfortable amount of available memory."
msgid "The <filename>sda3</filename> and <filename>sdc3</filename> partitions are assembled into a RAID-0 volume, <filename>md1</filename>, and used as swap partition, providing a total 32 GB of swap space. Modern systems can provide plenty of RAM and our system won't need hibernation. So with this amount added, our system will unlikely run out of memory."
msgstr "Utilizamos las particiones <filename>sda2</filename> y <filename>sdc2</filename> como particiones de intercambio que proveen un total de 2 GB de espacio de intercambio. Con 1 GB de RAM, la estación de trabajo tiene una cantidad adecuada de memoria disponible."

#, fuzzy
#| msgid "The <filename>sda5</filename> and <filename>sdc5</filename> partitions, as well as <filename>sda6</filename> and <filename>sdc6</filename>, are assembled into two new RAID-1 volumes of about 100 GB each, <filename>md1</filename> and <filename>md2</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 200 GB of safe space."
msgid "The <filename>sda4</filename> and <filename>sdc4</filename> partitions, as well as <filename>sda5</filename> and <filename>sdc5</filename>, are assembled into two new RAID-1 volumes of about 300 GB each, <filename>md2</filename> and <filename>md3</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 600 GB of safe space."
msgstr "Agrupamos las particiones <filename>sda5</filename> y <filename>sdc5</filename>, así como también <filename>sda6</filename> y <filename>sdc6</filename>, en dos nuevos volúmenes RAID-1 de alrededor de 100 GB cada uno: <filename>md1</filename> y <filename>md2</filename>. Inicializamos ambos espejos como volúmenes físicos para LVM y se los asigna al grupo de volúmenes <filename>vg_raid</filename>. Por lo tanto, este VG contiene aproximadamente 200 GB de espacio seguro."

#, fuzzy
#| msgid "The remaining partitions, <filename>sda7</filename> and <filename>sdc7</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 200 GB of space."
msgid "The remaining partitions, <filename>sda6</filename> and <filename>sdc6</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 460 GB of space."
msgstr "Utilizamos las particiones restantes, <filename>sda7</filename> y <filename>sdc7</filename>, directamente como volúmenes físicos y las asignamos a otro VG llamado <filename>vg_bulk</filename> que contiene, de esa forma, alrededor de 200 GB de espacio."

msgid "Once the VGs are created, they can be partitioned in a very flexible way. One must keep in mind that LVs created in <filename>vg_raid</filename> will be preserved even if one of the disks fails, which will not be the case for LVs created in <filename>vg_bulk</filename>; on the other hand, the latter will be allocated in parallel on both disks, which allows higher read or write speeds for large files."
msgstr "Una vez que crearomos los VGs, podemos particionalos de forma muy flexible. Uno debe recordar que se preservarán los LVs creados en <filename>vg_raid</filename> aún si falla uno de los discos, pero no será el caso de los LVs creados en <filename>vg_bulk</filename>; por el otro lado, este último será resevado en paralelo en ambos discos lo que permitirá velocidades de lectura y escritura mayores para archivos grandes."

msgid "We will therefore create the <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgstr "Así que crearemos los LVs <filename>lv_var</filename> y <filename>lv_home</filename> en <filename>vg_raid</filename> para almacenar los sistemas de archivos correspondientes; utilizaremos otro LV grande, <filename>lv_movies</filename>, para almacenar las versiones finales de los videos luego de editarlos. Dividiremos el otro VG en un gran <filename>lv_rushes</filename>, para datos directamente obtenidos de las cámaras de video digital, y <filename>lv_tmp</filename> para archivos temporales. La ubicación del área de trabajo es una decisión menos directa: si bien necesitamos buen rendimiento en dicho volúmen, ¿se justifica perder trabajo si falla un disco durante una sesión de edición? Dependiendo de la respuesta a dicha pregunta, crearemos el LV correspondiente en un VG o el otro."

msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications."
msgstr "Ahora tenemos tanto redundancia para datos importantes como flexibilidad sobre la forma en la que se divide el espacio disponible entre las aplicaciones."

msgid "<emphasis>NOTE</emphasis> Why three RAID-1 volumes?"
msgstr "<emphasis>NOTA</emphasis> ¿Porqué tres volúmenes RAID-1?"

msgid "We could have set up one RAID-1 volume only, to serve as a physical volume for <filename>vg_raid</filename>. Why create three of them, then?"
msgstr "Podríamos haber creado sólo un volumen RAID-1 a utilizar como volumen físico para <filename>vg_raid</filename>. ¿Por qué creamos tres entonces?"

msgid "The rationale for the first split (<filename>md0</filename> vs. the others) is about data safety: data written to both elements of a RAID-1 mirror are exactly the same, and it is therefore possible to bypass the RAID layer and mount one of the disks directly. In case of a kernel bug, for instance, or if the LVM metadata become corrupted, it is still possible to boot a minimal system to access critical data such as the layout of disks in the RAID and LVM volumes; the metadata can then be reconstructed and the files can be accessed again, so that the system can be brought back to its nominal state."
msgstr "El razonamiento para la primera división (<filename>md0</filename> y los demás) es por seguridad de los datos: los datos escritos a ambos elementos de un espejo RAID-1 son exactamente los mismos, por lo que es posible evitar la capa RAID y montar uno de los discos directamente. En caso de un error del núcleo, por ejemplo, o si se corrompen los metadatos LVM todavía es posible arrancar un sistema mínimo para acceder datos críticos como la distribución de discos en los volúmenes RAID y LVM; podremos luego reconstruir los metadatos y acceder a los archivos nuevamente, para poder devolver el sistema a su estado normal."

#, fuzzy
#| msgid "The rationale for the second split (<filename>md1</filename> vs. <filename>md2</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md2</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md2</filename> and integrate its components <filename>sda6</filename> and <filename>sdc6</filename> into the bulk VG (which grows by 200 GB instead of 100 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgid "The rationale for the second split (<filename>md2</filename> vs. <filename>md3</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md3</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md3</filename> and integrate its components <filename>sda5</filename> and <filename>sdc5</filename> into the bulk VG (which grows by 600 GB instead of 300 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgstr "El razonamiento para la segunda división (<filename>md1</filename> vs. <filename>md2</filename>) es menos estricto y está más relacionado con el reconocimiento que el futuro es incierto. Cuando se ensambló el equipo, no se conocían exactamente los requisitos; también puede evolucionar con el tiempo. En nuestro caso, no podemos saber por adelantado la necesidad de espacio de almacenamiento de cada tipo de videos. Si un video en particular necesita una gran cantidad de videos sin editar, y el VG dedicado para datos redundantes no tiene más de la mitad del espacio disponible, podemos reutilizar parte de su espacio innecesario. Podemos quitar uno de los volúmenes físicos, por ejemplo <filename>md2</filename> de <filename>vg_raid</filename> y asignarlo a <filename>vg_bulk</filename> directamente (si la duración esperada de la operación es suficientemente corta como para que no nos preocupe la pérdida temporal de rendimiento), o deshacer la configuración RAID en <filename>md2</filename> e integrar sus componentes, <filename>sda6</filename> y <filename>sdc6</filename> en el VG (que crecerá 200 GB en lugar de 100 GB); luego podremos aumentar el volumen lógico <filename>lv_rushes</filename> según se necesite."

msgid "<primary>virtualization</primary>"
msgstr "<primary>virtualización</primary>"

msgid "Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security."
msgstr "La virtualización es uno de los avances más grandes de la informática en los últimos años. El término abarca varias abstracciones y técnicas de simulación de equipos virtuales con un grado variable de independencia de hardware real. Un servidor físico puede almacenar varios sistemas que funcionan de forma simultánea y aislada. Sus aplicaciones son muchas y generalmente surgen de este aislamiento: entornos de prueba con diferentes configuraciones o separar los servicios provistos entre diferentes máquinas virtuales por seguridad."

msgid "There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:"
msgstr "Hay múltiples soluciones de virtualización, cada una con sus ventajas y desventajas. Este libro se concentrará en Xen, LXC y KVM; pero otras implementaciones notables incluyen las siguientes:"

#, fuzzy
#| msgid "<primary>LXC</primary>"
msgid "<primary>Xen</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>VMWare</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Nagios</primary>"
msgid "<primary>Bochs</primary>"
msgstr "<primary>Nagios</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>QEMU</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary>VirtualBox</primary>"
msgstr "<primary>virtualización</primary>"

msgid "<primary>KVM</primary>"
msgstr "<primary>KVM</primary>"

msgid "<primary>LXC</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"https://www.qemu.org/\" />"
msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"https://qemu.org/\" />"
msgstr "QEMU es un emulador en software para un equipo completo; su rendimiento está lejos de la velocidad que uno podría conseguir si ejecutara nativamente, pero esto permite ejecutar en el hardware emulado sistemas operativos sin modificación o experimentales. También permite emular una arquitectura de hardware diferente: por ejemplo, un sistema <emphasis>amd64</emphasis> puede emular una máquina <emphasis>arm</emphasis>. QEMU es software libre. <ulink type=\"block\" url=\"https://www.qemu.org/\" />"

msgid "Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64)."
msgstr "Bochs es otra máquina virtual libre, pero sólo emula la arquitectura x86 (i386 y amd64)."

#, fuzzy
#| msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"https://www.vmware.com/\" />"
msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as “snapshotting“ a running virtual machine. <ulink type=\"block\" url=\"https://vmware.com/\" />"
msgstr "VMWare es una máquina virtual privativa; como es una de las más antiguas es también una de las más conocidas. Funciona sobre cimientos similares a los de QEMU. VMWare propone funcionalidad avanzada como instantáneas («snapshot») de una máquina virtual en ejecución. <ulink type=\"block\" url=\"https://www.vmware.com/\" />"

#, fuzzy
#| msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type=\"block\" url=\"https://www.virtualbox.org/\" />"
msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some “snapshotting“ and other interesting features. <ulink type=\"block\" url=\"https://www.virtualbox.org/\" />"
msgstr "VirtualBox es una máquina virtual que es software libre en su mayor parte (algunos componentes adicionales están disponibles bajo una licencia privativa). Por desgracia está en la sección \"contrib\" de Debian porque incluye algunos ficheros precompilados que no se pueden recrear sin un compilador propietario y actualmente solo se encuentra en Debian Unstable porque las políticas de Oracle hacen que sea imposible mantenerlo seguro en una publicación estable de Debian (véase <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). Es más joven que VMWare y limitada a las arquitecturas i386 y amd64, pero incluye cierta compatibilidad con instantáneas y otras funcionalidades interesantes. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"

msgid "<emphasis>HARDWARE</emphasis> Virtualization support"
msgstr "<emphasis>HARDWARE</emphasis> Compatibilidad con virtualización"

msgid "Some computers might not have hardware virtualization support; when they do, it should be enabled in the BIOS."
msgstr "Puede que algunos ordenadores sean compatibles con el hardware de virtualización; cuando lo son, debe activarse en la BIOS."

msgid "To know if you have virtualization support enabled, you can check if the relevant flag is enabled with <command>grep</command>. If the following command for your processor returns some text, you already have virtualization support enabled:"
msgstr "Para saber si tiene activada la compatibilidad de virtualización, puede comprobar si la marca relevante está habilitada con <command>grep</command>. Si la siguiente orden para su procesador devuelve algún texto, ya tiene la compatibilidad de virtualización habilitada:"

#, fuzzy
#| msgid "For Intel processors you can execute <command>grep vmx /proc/cpuinfo</command>"
msgid "For Intel processors you can execute <command>grep vmx /proc/cpuinfo</command> to check for Intel's Virtual Machine Extensions."
msgstr "Para procesadores Intel puede ejecutar <command>grep vmx /proc/cpuinfo</command>"

#, fuzzy
#| msgid "For AMD processors you can execute <command>grep svm /proc/cpuinfo</command>"
msgid "For AMD processors you can execute <command>grep svm /proc/cpuinfo</command> to check for AMD's Secure Virtual Machine."
msgstr "Para procesadores AMD puede ejecutar <command>grep svm /proc/cpuinfo</command>"

msgid "If that is not the case, you can access the BIOS of your system and check for entries like “Intel Virtualization Technology”/“Intel VT-x” or “SVM mode” (AMD) - usually to be found in the CPU configuration in the Advanced section."
msgstr ""

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary>paravirtualization</primary>"
msgstr "<primary>virtualización</primary>"

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary>hypervisor</primary>"
msgstr "<primary>libvirt</primary>"

msgid "Xen <indexterm><primary>Xen</primary></indexterm> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen."
msgstr "Xen <indexterm><primary>Xen</primary></indexterm> es una solución de «paravirtualización». Introduce una fina capa de abstracción, llamada «hypervisor», entre el hardware y los sistemas superiores; ésta actúa como árbitro controlando el acceso al hardware desde las máquinas virtuales. Sin embargo, sólo gestiona unas pocas instrucciones, las demás se ejecutan directamente en el hardware en nombre de los sistemas. La principal ventaja es que no se degrada el rendimiento y los sistemas ejecutan a velocidades cercanas a la nativa; la desventaja es que el núcleo de los sistemas operativos que uno desee utilizar en un hypervisor Xen necesita ser adaptado para ejecutar sobre Xen."

#, fuzzy
#| msgid "<primary>deployment</primary>"
msgid "<primary>dom0</primary>"
msgstr "<primary>despliegue</primary>"

#, fuzzy
#| msgid "<primary>deployment</primary>"
msgid "<primary>domU</primary>"
msgstr "<primary>despliegue</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>virtualization</primary><secondary>host</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>virtualization</primary><secondary>guest</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Let's spend some time on terms. The hypervisor is the lowest layer, which runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgstr "Pasemos un poco de tiempo en los términos. El hypervisor es la capa más baja que ejecuta directamente en el hardware, inclusive debajo del núcleo. Este hypervisor puede dividir el resto del software entre varios <emphasis>dominios</emphasis> («domains»), pueden interpretarse como máquinas virtuales. Se conoce a uno de estos dominios (el primero en iniciar) como <emphasis>dom0</emphasis> y tiene un rol especial ya que sólo este dominio puede controlar al hypervisor y la ejecución de otros dominios. Se conocen a los otros dominios como <emphasis>domU</emphasis>. En otras palabras, desde el punto de vista del usuario, el <emphasis>dom0</emphasis> es el «anfitrión» de los demás sistemas de virtualización, mientras que los <emphasis>domU</emphasis> son sus «invitados»."

msgid "<emphasis>CULTURE</emphasis> Xen and the various versions of Linux"
msgstr "<emphasis>CULTURA</emphasis> Xen y las varias versiones de Linux"

msgid "Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <emphasis>paravirt_ops</emphasis> or <emphasis>pv_ops</emphasis> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially."
msgstr "Inicialmente, se desarrolló Xen como un conjunto de parches que existían fuera del árbol oficial y no estaban integrados en el núcleo Linux. Al mismo tiempo, muchos sistemas de virtualización emergentes (incluyendo KVM) necesitaban ciertas funciones relacionadas con la virtualización para facilitar su integración y el núcleo Linux desarrolló dichas funciones (conocidas como la interfaz <emphasis>paravirt_ops</emphasis> o <emphasis>pv_ops</emphasis>). Debido a que algunos parches de Xen duplicaban parte de la funcionalidad de esta interfaz no podían ser aceptados oficialmente."

msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgstr "Xensource, la empresa detrás de Xen, tuvo entonces que migrar Xen a esta nueva interfaz para que se pudieran integrar los parches Xen al núcleo Linux oficial. Esto significó reescribir mucho código y, si bien Xensource consiguió una versión funcional basada en la interfaz paravirt_ops rápidamente, los parches fueron incluidos progresivamente en el núcleo oficial. Esta integración se completó en Linux 3.0. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/XenParavirtOps\" />"

msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgstr "Dado que <emphasis role=\"distribution\">Jessie</emphasis> utiliza la versión 3.16 del núcleo Linux, los paquetes <emphasis role=\"pkg\">linux-image-686-pae</emphasis> y <emphasis role=\"pkg\">linux-image-amd64</emphasis> incluyen el código necesario, ya no existen los parches específicos necesarios para Debian <emphasis role=\"distribution\">Squeeze</emphasis> y anteriores. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"

msgid "<emphasis>CULTURE</emphasis> Xen and non-Linux kernels"
msgstr "<emphasis>CULTURA</emphasis> Xen y núcleos distintos a Linux"

msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgstr "Xen necesita modificaciones en todos los sistemas operativos que uno desee ejecutar en él; no todos los núcleos tiene el mismo nivel de madurez en este aspecto. Muchos son completamente funcionales, tanto para dom0 como para domU: Linux 3.0 y posterior, NetBSD 4.0 y posterior y OpenSolaris. Otros sólo funcionan como domU. Puede comprobar el estado de cada sistema operativo en la wiki de Xen: <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"

msgid "However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows)."
msgstr "Sin embargo, si Xen puede confiar en funciones de hardware dedicadas a la virtualización (que sólo están presentes en procesadores más recientes) inclusive sistemas operativos sin modificación pueden ejecutar como domU (incluyendo Windows)."

msgid "<emphasis>NOTE</emphasis> Architectures compatible with Xen"
msgstr "<emphasis>NOTA</emphasis> Arquitecturas compatibles con Xen"

msgid "Xen is currently only available for the i386, amd64, arm64 and armhf architectures."
msgstr "Xen actualmente solo está disponible para las arquitecturas i386, amd64, arm64 y armhf."

msgid "Using Xen under Debian requires three components:"
msgstr "Utilizar Xen en Debian requiere tres componentes:"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-hypervisor</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.11-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.11-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.11-arm64</emphasis>."
msgid "The hypervisor itself. According to the available hardware, the appropriate package providing <emphasis role=\"pkg\">xen-hypervisor</emphasis> will be either <emphasis role=\"pkg\">xen-hypervisor-4.14-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.14-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.14-arm64</emphasis>."
msgstr "El hipervisor en sí mismo. Según el hardware disponible, el paquete apropiado será <emphasis role=\"pkg\">xen-hypervisor-4.11-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.11-armhf</emphasis> o <emphasis role=\"pkg\">xen-hypervisor-4.11-arm64</emphasis>."

#, fuzzy
#| msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 4.19 version present in <emphasis role=\"distribution\">Buster</emphasis>."
msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 5.10 version present in <emphasis role=\"distribution\">Bullseye</emphasis>."
msgstr "Un núcleo que ejecuta sobre dicho hipervisor. Cualquier núcleo posterior a 3.0 funcionará, incluyendo la versión 4.19 presente en <emphasis role=\"distribution\">Buster</emphasis>."

msgid "The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <emphasis role=\"pkg\">libc6-xen</emphasis> package."
msgstr "La arquitectura i386 también necesita una biblioteca estándar con los parches apropiados para aprovechar Xen; ésta se encuentra en el paquete <emphasis role=\"pkg\">libc6-xen</emphasis>."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-utils</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.11</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the GRUB bootloader menu, so as to start the chosen kernel in a Xen dom0. Note, however, that this entry is not usually set to be the first one in the list, but it will be selected by default."
msgid "The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.14</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the GRUB bootloader menu, so as to start the chosen kernel in a Xen dom0. Note, however, that this entry is not usually set to be the first one in the list, but it will be selected by default."
msgstr "El hypervisor también incluirá <emphasis role=\"pkg\">xen-utils-4.11</emphasis>, que contien las herramientas para controlar el hypervisor desde el dom0. A su vez, éste incluirá la biblioteca estándar apropiada. Durante la instalación de todo esto, los scripts de configuración también crearán un nuevo elemento en el menú del gestor de arranque GRUB para iniciar el núcleo elegido en un dom0 Xen. Sepa, sin embargo, que generalmente este no será el primero en la lista, pero estará seleccionado de forma predeterminada."

msgid "Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps."
msgstr "Una vez que instaló estos prerequisitos, el siguiente paso es probar el comportamiento del dom0 en sí mismo; esto incluye reiniciar para utilizar el hypervisor y núcleo Xen. El sistema debería iniciar como siempre, con unos pocos mensajes adicionales en la consola durante los primeros pasos de inicialización."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-tools</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>xen-create-image</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/xen-tools/xen-tools.conf</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Now is the time to actually install useful systems on the domU systems, using the tools from <emphasis role=\"pkg\">xen-tools</emphasis>. This package provides the <command>xen-create-image</command> command, which largely automates the task. The only mandatory parameter is <literal>--hostname</literal>, giving a name to the domU; other options are important, but they can be stored in the <filename>/etc/xen-tools/xen-tools.conf</filename> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <command>xen-create-image</command> invocation. Important parameters of note include the following:"
msgstr "Ahora es el momento de instalar sistemas útiles en los sistemas domU, utilizando las herramientas en <emphasis role=\"pkg\">xen-tools</emphasis>. Este paquete provee el programa <command>xen-create-image</command>, que automatiza en gran parte esta tarea. El único parámetro obligatorio es <literal>--hostname</literal>, que le da un nombre al domU; otras opciones son importantes, pero puede guardarlas en el archivo de configuración <filename>/etc/xen-tools/xen-tools.conf</filename> y si no las especifica no generará ningún error. Por lo tanto es importante revisar el contenido de este archivo antes de crear imágenes o utilizar los parámetros adicionales en la invocación de <command>xen-create-image</command>. Los parámetros importantes a saber incluyen los siguientes:"

msgid "<literal>--memory</literal>, to specify the amount of RAM dedicated to the newly created system;"
msgstr "<literal>--memory</literal> para especificar la cantidad de RAM dedicada a este nuevo sistema creado;"

msgid "<literal>--size</literal> and <literal>--swap</literal>, to define the size of the “virtual disks” available to the domU;"
msgstr "<literal>--size</literal> y <literal>--swap</literal> para definir el tamaño de los «discos virtuales» disponibles al domU;"

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary><command>debootstrap</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

#, fuzzy
#| msgid "<literal>--debootstrap-cmd</literal>, to specify the which debootstrap command is used. The default is <command>debootstrap</command> if debootstrap and cdebootstrap are installed. In that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">buster</emphasis>)."
msgid "<literal>--debootstrap-cmd</literal>, to specify the which debootstrap command is used. The default is <command>debootstrap</command> if debootstrap and cdebootstrap are installed. In that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">bullseye</emphasis>)."
msgstr "<literal>--debootstrap-cmd</literal> para especificar que orden de <foreignphrase>debootstrap</foreignphrase> utilizar. La predeterminada es <command>debootstrap</command> si debootstrap y cdebootstrap están instalados. En tal caso, generalmente también utilizará la opción <literal>--dist</literal> (con el nombre de una distribución como <emphasis role=\"distribution\">buster</emphasis>)."

msgid "<emphasis>GOING FURTHER</emphasis> Installing a non-Debian system in a domU"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> Instalación de un sistema distinto a Debian en un domU"

msgid "In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <literal>--kernel</literal> option."
msgstr "En el caso de un sistema distinto a Linux, debe tener cuidado de definir el núcleo que debe utilizar el domU con la opción <literal>--kernel</literal>."

msgid "<literal>--dhcp</literal> states that the domU's network configuration should be obtained by DHCP while <literal>--ip</literal> allows defining a static IP address."
msgstr "<literal>--dhcp</literal> indica que el domU debe obtener su configuración de red a través de DHCP, mientras que <literal>--ip</literal> permite definir una dirección IP estática."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>Xen</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <literal>--dir</literal> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <literal>--lvm</literal> option, followed by the name of a volume group; <command>xen-create-image</command> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive."
msgstr "Por último, debe elegir un método de almacenamiento para las imágenes a crear (que el domU verá como discos duros). El método más simple, que corresponde a la opción <literal>--dir</literal>, es crear un archivo en el dom0 para cada dispositivo que se le provee al domU. La alternativa en sistemas que utilizan LVM es la opción <literal>--lvm</literal> seguida del nombre de un grupo de volúmenes; <command>xen-create-image</command> luego creará un nuevo volumen lógico dentro de dicho grupo y éste estará disponible en el domU como un disco duro."

msgid "<emphasis>NOTE</emphasis> Storage in the domU"
msgstr "<emphasis>NOTA</emphasis> Almacenamiento en el domU"

msgid "Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <command>xen-create-image</command>, however, so editing the Xen image's configuration file is in order after its initial creation with <command>xen-create-image</command>."
msgstr "También puede exportar discos duros completos al domU, particiones, arrays RAID o volúmenes lógicos LVM preexistentes. Sin embargo, estas operaciones no están automatizadas por <command>xen-create-image</command>, por lo que deberá editar el archivo de configuración de la imagen luego de crearlo con <command>xen-create-image</command>."

msgid "Once these choices are made, we can create the image for our future Xen domU:"
msgstr "Una vez que realizó esta elección, puede crear la imagen para nuestro futuro domU Xen:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=buster --role=udev</userinput>\n"
#| "<computeroutput>\n"
#| "[...]\n"
#| "General Information\n"
#| "--------------------\n"
#| "Hostname       :  testxen\n"
#| "Distribution   :  buster\n"
#| "Mirror         :  http://deb.debian.org/debian\n"
#| "Partitions     :  swap            512M  (swap)\n"
#| "                  /               2G    (ext4)\n"
#| "Image type     :  sparse\n"
#| "Memory size    :  256M\n"
#| "Kernel path    :  /boot/vmlinuz-4.19.0-5-amd64\n"
#| "Initrd path    :  /boot/initrd.img-4.19.0-5-amd64\n"
#| "[...]\n"
#| "Logfile produced at:\n"
#| "         /var/log/xen-tools/testxen.log\n"
#| "\n"
#| "Installation Summary\n"
#| "---------------------\n"
#| "Hostname        :  testxen\n"
#| "Distribution    :  buster\n"
#| "MAC Address     :  00:16:3E:0C:74:2F\n"
#| "IP Address(es)  :  dynamic\n"
#| "SSH Fingerprint :  SHA256:PuAGX4/4S07Xzh1u0Cl2tL04EL5udf9ajvvbufBrfvU (DSA)\n"
#| "SSH Fingerprint :  SHA256:ajFTX54eakzolyzmZku/ihq/BK6KYsz5MewJ98BM5co (ECDSA)\n"
#| "SSH Fingerprint :  SHA256:/sFov86b+rD/bRSJoHKbiMqzGFiwgZulEwpzsiw6aSc (ED25519)\n"
#| "SSH Fingerprint :  SHA256:/NJg/CcoVj+OLE/cL3yyJINStnla7YkHKe3/xEdVGqc (RSA)\n"
#| "Root Password   :  EwmQMHtywY9zsRBpqQuxZTb\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=bullseye --role=udev\n"
"</userinput><computeroutput>\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  bullseye\n"
"Mirror         :  http://deb.debian.org/debian\n"
"Partitions     :  swap            512M  (swap)\n"
"                  /               2G    (ext4)\n"
"Image type     :  sparse\n"
"Memory size    :  256M\n"
"Bootloader     :  pygrub\n"
"\n"
"[...]\n"
"Logfile produced at:\n"
"\t /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  bullseye\n"
"MAC Address     :  00:16:3E:C2:07:EE\n"
"IP Address(es)  :  dynamic\n"
"SSH Fingerprint :  SHA256:K+0QjpGzZOacLZ3jX4gBwp0mCESt5ceN5HCJZSKWS1A (DSA)\n"
"SSH Fingerprint :  SHA256:9PnovvGRuTw6dUcEVzzPKTITO0+3Ki1Gs7wu4ke+4co (ECDSA)\n"
"SSH Fingerprint :  SHA256:X5z84raKBajUkWBQA6MVuanV1OcV2YIeD0NoCLLo90k (ED25519)\n"
"SSH Fingerprint :  SHA256:VXu6l4tsrCoRsXOqAwvgt57sMRj2qArEbOzHeydvV34 (RSA)\n"
"Root Password   :  FS7CUxsY3xkusv7EkbT9yae\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=buster --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  buster\n"
"Mirror         :  http://deb.debian.org/debian\n"
"Partitions     :  swap            512M  (swap)\n"
"                  /               2G    (ext4)\n"
"Image type     :  sparse\n"
"Memory size    :  256M\n"
"Kernel path    :  /boot/vmlinuz-4.19.0-5-amd64\n"
"Initrd path    :  /boot/initrd.img-4.19.0-5-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  buster\n"
"MAC Address     :  00:16:3E:0C:74:2F\n"
"IP Address(es)  :  dynamic\n"
"SSH Fingerprint :  SHA256:PuAGX4/4S07Xzh1u0Cl2tL04EL5udf9ajvvbufBrfvU (DSA)\n"
"SSH Fingerprint :  SHA256:ajFTX54eakzolyzmZku/ihq/BK6KYsz5MewJ98BM5co (ECDSA)\n"
"SSH Fingerprint :  SHA256:/sFov86b+rD/bRSJoHKbiMqzGFiwgZulEwpzsiw6aSc (ED25519)\n"
"SSH Fingerprint :  SHA256:/NJg/CcoVj+OLE/cL3yyJINStnla7YkHKe3/xEdVGqc (RSA)\n"
"Root Password   :  EwmQMHtywY9zsRBpqQuxZTb\n"
"</computeroutput>"

msgid "We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters."
msgstr "Ahora tenemos una máquina virtual, pero no está ejecutando (por lo tanto sólo utiliza espacio en el disco duro del dom0). Por supuesto, podemos crear más imágenes, posiblemente con diferentes parámetros."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>network models</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgstr "Antes de encender estas máquinas virtuales, necesitamos definir cómo accederemos a ellas. Por supuesto, podemos considerarlas máquinas aisladas a las que sólo podemos acceder a través de su consola de sistema, pero rara vez esto coincide con el patrón de uso. La mayoría de las veces, consideraremos un domU como un servidor remoto al que sólo podemos acceder a través de la red. Sin embargo, sería un gran inconveniente agregar una tarjeta de red para cada domU; es por esto que Xen permite crear interfaces virtuales que cada dominio puede ver y utilizar de la forma estándar. Sepa que estas tarjetas, aunque sean virtuales, sólo serán útiles cuando estén conectadas a una red, inclusive una virtual. Xen tiene varios modelos de red para esto:"

msgid "The simplest model is the <emphasis>bridge</emphasis> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch."
msgstr "El modelo más simple es el modelo <emphasis>puente</emphasis> («bridge»); todas las tarjetas de red eth0 (tanto en los sistemas domU como en el dom0) se comportarán como si estuvieran conectadas directamente a un switch Ethernet."

msgid "Then comes the <emphasis>routing</emphasis> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network."
msgstr "Luego está el modelo <emphasis>enrutamiento</emphasis> («routing») en el que el dom0 se comporta como el router entre los sistemas domU y la red (física) externa."

msgid "Finally, in the <emphasis>NAT</emphasis> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0."
msgstr "Finalmente, en el modelo <emphasis>NAT</emphasis>, nuevamente el dom0 se encuentra entre los sistemas domU y el resto de la red, pero no se puede acceder a los sistemas domU directamente desde afuera y el tráfico atraviesa una traducción de direcciones de red en el dom0."

msgid "These three networking nodes involve a number of interfaces with unusual names, such as <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> and <filename>xenbr0</filename>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model."
msgstr "Estos tres modos de red involucran una cantidad de interfaces con nombres inusuales, como <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> y <filename>xenbr0</filename>. El hypervisor Xen los acomoda en la distribución definida bajo el control de las herramientas en espacio de usuario. Debido a que los modelos NAT y de enrutamiento sólo se adaptan a casos particulares sólo discutiremos el modelo de puente."

#, fuzzy
#| msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.11</emphasis> package recommends it) to replace the existing eth0 entry:"
msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils</emphasis> package recommends it) to replace the existing <replaceable>eth0</replaceable> entry (be careful to use the correct network device name):"
msgstr "La configuración estándar de los paquetes Xen no modifica la configuración de red del sistema. Sin embargo, se configura el demonio <command>xend</command> para integrar las interfaces de red virtuales en un puente de red preexistente (<filename>xenbr0</filename> tiene precedencia si existen varios de ellos). Por lo tanto, debemos configurar un puente en <filename>/etc/network/interfaces</filename> (lo que requiere que instalemos el paquete <emphasis role=\"pkg\">bridge-utils</emphasis>, razón por la que lo recomienda el paquete <emphasis role=\"pkg\">xen-utils-4.11</emphasis>) para reemplazar el elemento eth0 existente:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/network/interfaces</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">bridge-utils</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary><literal>xenbr0</literal></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid ""
#| "auto xenbr0\n"
#| "iface xenbr0 inet dhcp\n"
#| "    bridge_ports eth0\n"
#| "    bridge_maxwait 0\n"
#| "    "
msgid ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports <replaceable>eth0</replaceable>\n"
"    bridge_maxwait 0"
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>xl</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them. You might need to increase the default memory by editing the variable memory from configuration file (in this case, <filename>/etc/xen/testxen.cfg</filename>). Here we have set it to 1024 (megabytes)."
msgstr "Luego de reiniciar para asegurarse que se crea el puente automáticamente, podemos iniciar el domU con las herramientas de control de Xen, en particular el programa <command>xl</command>. Este programa permite varias manipulaciones de los dominios, entre ellas: enumerarlos, iniciarlos y detenerlos. Puede que tenga que aumentar la memoria predeterminada editando la variable memory en el archivo de configuración (en este caso, <filename>/etc/xen/testxen.cfg</filename>). Aquí le hemos asignado 1024 (<foreignphrase>megabytes</foreignphrase>)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/xen/testxen.cfg</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
#| "Domain-0                                     0  1894     2     r-----      63.5\n"
#| "# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
#| "<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
#| "# </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
#| "Domain-0                                     0  1505     2     r-----     100.0\n"
#| "testxen                                     13  1024     0     --p---       0.0</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl list\n"
"</userinput><computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  3918     2     r-----      35.1\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg\n"
"</userinput><computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list\n"
"</userinput><computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  2757     2     r-----      45.2\n"
"testxen                                      3  1024     1     r-----       1.3\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  1894     2     r-----      63.5\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  1505     2     r-----     100.0\n"
"testxen                                     13  1024     0     --p---       0.0</computeroutput>"

msgid "<emphasis>TOOL</emphasis> Choice of toolstacks to manage Xen VM"
msgstr "<emphasis>HERRAMIENTA</emphasis> Elección del conjunto de herramientas para gestionar las máquinas virtuales de Xen"

msgid "<primary><command>xm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "<primary><command>xe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "<primary><command>virsh</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">libvirt</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of libvirt and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of <emphasis role=\"pkg\">libvirt</emphasis> and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgstr "En Debian 7 y versiones anteriores, la herramienta de línea de comando <command>xm</command> era la referencia para gestionar máquinas virtuales Xen. Ahora ha sido reemplazada por <command>xl</command>, la cual es mayormente compatible con versiones anteriores. Pero no son las únicas herramientas: <command>virsh</command> de libvirt y <command>xe</command> de la XAPI de XenServer (ofrecimiento comercial de Xen) son herramientas alternativas."

msgid "<emphasis>CAUTION</emphasis> Only one domU per image!"
msgstr "<emphasis>PRECAUCIÓN</emphasis> ¡Sólo un domU por imagen!"

msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is, however, quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgstr "Si bien es posible tener varios sistemas domU ejecutando en paralelo, siempre necesitarán utilizar su propia imagen ya que se le hace creer a cada domU que ejecuta en su propio hardware (además de la pequeña porción del núcleo que interactúa con el hypervisor). En particular, no es posible que dos sistemas domU ejecutando en paralelo compartan espacio de almacenamiento. Si los sistemas domU no ejecutan al mismo tiempo, sin embargo, es posible reutilizar la misma partición de intercambio o la partición que alberga el sistema de archivos <filename>/home</filename>."

msgid "Note that the <filename>testxen</filename> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly."
msgstr "Sepa que el domU <filename>testxen</filename> utiliza memoria real - no simulada - de la RAM que, de lo contrario, estaría disponible en el dom0. Debe tener cuidado al construir un servidor para instancias Xen, asegurándose de incluir suficente RAM física."

msgid "Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <filename>hvc0</filename> console, with the <command>xl console</command> command:"
msgstr "¡Voilà! Nuestra máquina virtual está iniciando. Podemos acceder a ella de dos formas. La forma usual es conectarnos «remotamente» a través de la red, como lo haríamos con una máquina real; esto usualmente requerirá configurar un servidor DHCP o alguna configuración de DNS. La otra forma, que puede ser la única forma si la configuración de red era incorrecta, es utilizar la consola <filename>hvc0</filename> ejecutando <command>xl console</command>:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary><literal>hvc0</literal></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
#| "<computeroutput>[...]\n"
#| "\n"
#| "Debian GNU/Linux 10 testxen hvc0\n"
#| "\n"
#| "testxen login: </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 11 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[…]\n"
"\n"
"Debian GNU/Linux 10 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"

msgid "One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> key combination."
msgstr "Uno puede abrir una sesión, tal como si estuviera sentado frente al teclado de la máquina virtual. Puede desconectarse de esta consola con la combinación de teclas <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo>."

msgid "<emphasis>TIP</emphasis> Getting the console straight away"
msgstr "<emphasis>SUGERENCIA</emphasis> Ingreso a la consola inmediatamente"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>console</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Sometimes one wishes to start a domU system and get to its console straight away; this is why the <command>xl create</command> command takes a <literal>-c</literal> switch. Starting a domU with this switch will display all the messages as the system boots."
msgstr "A veces uno desea iniciar un sistema domU e ingresar a su consola inmediatamente; es por esto que el comando <command>xl create</command> usa la opción <literal>-c</literal>. Iniciar un domU con esta opción mostrará todo los mensajes del sistema que se inicie."

#, fuzzy
#| msgid "<emphasis>TOOL</emphasis> OpenXenManager"
msgid "<emphasis>TOOL</emphasis> Graphical Xen managers"
msgstr "<emphasis>HERRAMIENTA</emphasis> OpenXenManager"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>manager</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <command>xl</command> command."
msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package), a graphical interface allowing remote management of Xen domains via Xen's API, is no longer provided by Debian due to the lack of upstream development. If you are looking for a replacement, <emphasis role=\"pkg\">virt-manager</emphasis> provides support to handle Xen VMs as well."
msgstr "OpenXenManager (en el paquete <emphasis role=\"pkg\">openxenmanager</emphasis>) es una interfaz gráfica que permite controlar remotamente los dominios Xen a través de la API de Xen. Provee la mayoría de la funciondalidad del programa <command>xl</command>."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">openxenmanager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

msgid "Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <command>xl pause</command> and <command>xl unpause</command> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <command>xl save</command> and <command>xl restore</command> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system."
msgstr "Una vez que el domU está ejecutando, puede utilizarlo como cualquier otro servidor (al fin y al cabo es un sistema GNU/Linux). Sin embargo, su existencia como máquina virtual permite cierta funcionalidad adicional. Por ejemplo, puede pausar y resumir temporalmente un domU, ejecutando <command>xl pause</command> y <command>xl unpause</command>. Sepa que aunque un domU pausado no utiliza el procesador, la memoria reservada a él sigue en uso. Puede ser interesante considerar las órdenes <command>xl save</command> y <command>xl restore</command>: guardar un domU libera los recursos utilizados por este domU, incluyendo la RAM. Cuando restaure (o resuma) un domU, éste no notará nada a excepción del paso del tiempo. Si un domU está ejecutando cuando se apague el dom0, los scripts empaquetados automáticamente guardarán el domU y lo restaurarán cuando vuelva a iniciar. Esto, por supuesto, tiene los mismos inconvenientes estándar que cuando hiberna un equipo portátil, por ejemplo; en particular, si se suspende por demasiado tiempo al domU, pueden expirar las conexiones de red. Sepa también que, hasta el momento, Xen es incompatible con gran parte de la gestión de energía ACPI, lo que evita que pueda suspender el sistema anfitrión (dom0)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>ACPI</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>ACPI</primary>"
msgstr "<primary>RAID</primary>"

msgid "Halting or rebooting a domU can be done either from within the domU (with the <command>shutdown</command> command) or from the dom0, with <command>xl shutdown</command> or <command>xl reboot</command>."
msgstr "Puede apagar o reiniciar un domU tanto desde dentro del domU (con el programa <command>shutdown</command>) como también desde el dom0, ejecutando <command>xm shutdown</command> o <command> xl reboot</command>."

msgid "Most of the <command>xl</command> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> manual page."
msgstr "La mayoría de las subórdenes de <command>xl</command> esperan uno o más parámetros, generalmente el nombre de un domU. Se describen en detalle estos parámetros en la página de manual <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry>."

msgid "<emphasis>GOING FURTHER</emphasis> Advanced Xen"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> Xen avanzado"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>documentation</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"https://xenproject.org/help/documentation/\" />"
msgstr "Xen tiene mucha más funcionalidad de la que podemos describir en estos pocos párrafos. En particular, el sistema es muy dinámico y puede ajustar muchos parámetros de un dominio (como cantidad de memoria reservada, discos duros visibles, comportamiento de las tareas programadas, etc.) aún cuando éste está ejecutando. ¡Inclusive puede migrar un domU entre servidors sin apagarlo y sin perder sus conexiones de red! Para saber más de todos estos aspectos avanzados, la fuente de información principal es la documentación oficial de Xen. <ulink type=\"block\" url=\"https://xenproject.org/help/documentation/\" />"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>Linux Containers</primary><see>LXC</see>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>kernel</primary><secondary>control groups</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <emphasis>control groups</emphasis>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system."
msgstr "Aún cuando es utilizado para crear «máquinas virtuales», LXC no es, estrictamente hablando, un sistema de virtualización sino un sistema para aislar grupos de procesos entre sí aún cuando estos ejecutan en el mismo equipo. Aprovecha un conjunto de evoluciones recientes del núcleo Linux, conocidos colectivamente como <emphasis>grupos de control</emphasis> («control groups»), mediante los que diferentes conjuntos de procesos llamados «grupos» tienen diferentes visiones de ciertos aspectos de todo el sistema. Entre estos aspectos, los más notables son los identificadores de procesos, la configuración de red y los puntos de montaje. Un grupo de procesos aislados no podrá acceder a otros procesos en el sistema y puede restringir su acceso al sistema de archivos a un subconjunto específico. También puede tener su propia interfaz de red y tabla de enrutamiento y puede configurarlo para que sólo pueda ver un subconjunto de los dispositivos disponibles que están presentes en el sistema."

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>container</primary>"
msgstr "<primary>Munin</primary>"

msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there is no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgstr "Puede combinar estas funcionalidades para aislar una familia de procesos completa que inicia desde el proceso <command>init</command>, y el conjunto resultante es muy similar a una máquina virtual. El nombre oficial de esta configuración es «contenedor» (de allí LXC: <emphasis>contenedores Linux</emphasis>, «LinuX Containers»), pero una diferencia importante con máquinas virtuales «reales» como aquellas provistas por Xen o KVM es que no hay un segundo núcleo; el contenedor utiliza el mismo núcleo que el sistema anfitrión. Esto tiene tanto ventajas como desventajas: las ventajas incluyen un rendimiento excelente debido a una falta completa de sobrecarga y el hecho de que el núcleo tiene una visión global de todos los procesos que ejecutan en el sistema por lo que la gestión de procesos puede ser más eficiente que si existieran dos núcleos independientes administrando conjuntos de tareas. La mayor de las desventajas es la imposibilidad de ejecutar un núcleo diferente en un contenedor (sea una versión diferente de Linux o directamente un sistema operativo distinto)."

msgid "<emphasis>NOTE</emphasis> LXC isolation limits"
msgstr "<emphasis>NOTA</emphasis> Límites de aislamiento en LXC"

msgid "LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:"
msgstr "Los contenedores LXC no proveen el nivel de aislamiento que proveen emuladores o virtualizadores más pesados. En particular:"

msgid "since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;"
msgstr "debido a que el sistema anfitrión y los contendores comparten el núcleo, los procesos limitados en un contenedor todavía pueden acceder a los mensajes del núcleo, lo que puede causar que se filtre información si un contenedor emite mensajes;"

msgid "for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;"
msgstr "por razones similares, si se compromete un contenedor y se explota una vulnerabilidad del núcleo, puede afectar a otros contenedores;"

msgid "on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers."
msgstr "en el sistema de archivos, el núcleo supervisa los permisos según identificadores numéricos para los usuarios y grupos; estos identificadores pueden designar usuarios y grupos diferentes según el contenedor, debe tenerlo en cuenta si los contenedores comparten permisos de escritura a partes del sistema de archivos."

msgid "Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container."
msgstr "Debido a que estamos trabajando con aislamiento en lugar de virtualización, configurar contenedores LXC es más complejo que simplemente ejecutar debian-installer en una máquina virtual. Describiremos unos pocos prerequisitos y luego continuaremos con la configuración de red; finalmente podremos crear realmente el sistema a ejecutar en el contenedor."

msgid "Preliminary Steps"
msgstr "Pasos preliminares"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">lxc</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains the tools required to run LXC, and must therefore be installed."
msgstr "El paquete <emphasis role=\"pkg\">lxc</emphasis> contiene las herramientas necesarias para utilizar LXC, por lo tanto debe instalarlo."

msgid "<primary><filename>/sys</filename></primary><secondary><filename>/sys/fs/cgroup</filename></secondary>"
msgstr ""

msgid "LXC also requires the <emphasis>control groups</emphasis> configuration system, which is a virtual filesystem to be mounted on <filename>/sys/fs/cgroup</filename>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration."
msgstr "LXC también necesita del sistema de configuración de <emphasis>grupos de control</emphasis> («control groups»), que es un sistema de archivos virtual montado en <filename>/sys/fs/cgroup</filename>. Desde que Debian 8 se ha cambiado a systemd, el cual confía tambien en los grupos de control, eso ya se ha hecho automáticamente en el momento de arranque sin necesidad de configuraciones adicionales."

msgid "Network Configuration"
msgstr "Configuración de red"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary>network configuration</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The goal of installing LXC is to set up virtual machines; while we could, of course, keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgstr "El objetivo de instalar LXC es configurar máquinas virtuales; si bien podríamos mantenerlas aisladas de la red, y sólo comunicarnos con ellas a través del sistema de archivos, la mayoría de los casos de uso involucran proveer a los contenedores al menos un acceso mínimo a la red. En el caso típico, cada contenedor obtendrá una interfaz de red virtual, conectada a la red real a través de un puente. Esta interfaz virtual puede conectarse directamente a la interfaz de red física del anfitrión (en cuyo caso el contenedor se encuentra en la red directamente) o a otra interfaz virtual definida en el anfitrión (y en la que éste puede filtrar o enrutar tráfico). En ambos casos, necesitará el paquete <emphasis role=\"pkg\">bridge-utils</emphasis>."

#, fuzzy
#| msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance, <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance, <literal>eth0</literal> or <literal>enp1s0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgstr "El caso más simple es sólo cuestión de editar <filename>/etc/network/interfaces</filename>, moviendo la configuración de la interfaz física (por ejemplo, <literal>eth0</literal>) a la interfaz bridge (generalmente <literal>br0</literal>) y configurar un enlace entre ellas. Por ejemplo, si el archivo de configuración de la interfaz de red inicialmente contiene elementos como los siguientes:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>br</literal> interface</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary><literal>br</literal>, network interface</primary>"
msgstr "<primary>virtualización</primary>"

msgid ""
"auto eth0\n"
"iface eth0 inet dhcp"
msgstr ""
"auto eth0\n"
"iface eth0 inet dhcp"

msgid "They should be disabled and replaced with the following:"
msgstr "Debería desactivarlas y reemplazarlas con lo siguiente:"

#, fuzzy
#| msgid ""
#| "auto xenbr0\n"
#| "iface xenbr0 inet dhcp\n"
#| "    bridge_ports eth0\n"
#| "    bridge_maxwait 0\n"
#| "    "
msgid ""
"auto br0\n"
"iface br0 inet dhcp\n"
"    bridge-ports <replaceable>eth0</replaceable>"
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

msgid "The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <literal>eth0</literal> as well as the interfaces defined for the containers."
msgstr "El efecto de esta configuración será similar a lo que podría obtener si los controladores fueran máquinas conectadas a la misma red física que el anfitrión. La configuración del «puente» gestiona el tránsito de tramas Ethernet entre todas las interfaces en él, lo que incluye la interfaz física <literal>eth0</literal> así como también las interfaces definidas para los contenedores."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>tap</literal> interface</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "In cases where this configuration cannot be used (for instance, if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgstr "En casos en los que no pueda utilizar esta configuración (por ejemplo, si no puede asignarle una IP pública a los contenedores), crearemos una sola interfaz virtual <emphasis>tap</emphasis> y la conectaremos al puente. La topología de red equivalente sería aquella de un equipo con una segunda tarjeta de red conectada a un switch independiente al que también están conectados los contenedores. El anfitrión deberá actuar como puerta de enlace para los contenedores si éstos deben comunicarse con el mundo exterior."

msgid "In addition to <emphasis role=\"pkg\">bridge-utils</emphasis>, this “rich” configuration requires the <emphasis role=\"pkg\">vde2</emphasis> package; the <filename>/etc/network/interfaces</filename> file then becomes:"
msgstr "Además de <emphasis role=\"pkg\">bridge-utils</emphasis>, esta configuración «enriquecida» necesita el paquete <emphasis role=\"pkg\">vde2</emphasis>; el archivo <filename>/etc/network/interfaces</filename> se convierte entonces en:"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">vde2</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid ""
#| "# Interface eth0 is unchanged\n"
#| "auto eth0\n"
#| "iface eth0 inet dhcp\n"
#| "\n"
#| "# Virtual interface \n"
#| "auto tap0\n"
#| "iface tap0 inet manual\n"
#| "  vde2-switch -t tap0\n"
#| "\n"
#| "# Bridge for containers\n"
#| "auto br0\n"
#| "iface br0 inet static\n"
#| "  bridge-ports tap0\n"
#| "  address 10.0.0.1\n"
#| "  netmask 255.255.255.0"
msgid ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"    vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"    bridge-ports tap0\n"
"    address 10.0.0.1\n"
"    netmask 255.255.255.0"
msgstr ""
"# Interfaz eth0 sin cambios\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Interfaz virtual\n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Puente para los contenedores\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"

msgid "The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <literal>br0</literal> interface."
msgstr "Luego puede configurar la red en los contenedores de forma estática o dinámica con un servidor DHCP ejecutando en el anfitrión. Deberá configurar este servidor DHCP para que responda a pedidos en la interfaz <literal>br0</literal>."

msgid "Setting Up the System"
msgstr "Configuración del sistema"

#, fuzzy
#| msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> package includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgstr "Configuremos ahora el sistema que utilizará el contenedor. Debido a que esta «máquina virtual» no ejecutará directamente sobre el hardware, son necesarios algunos ajustes comparados con un sistema de archivos estándar, especialmente en aquello que involucra al núcleo, los dispositivos y las consolas. Afortunadamente, el paquete <emphasis role=\"pkg\">lxc</emphasis> incluye scripts que automatizan la mayoría de esta configuración. Por ejemplo, las siguientes órdenes (que requieren los paquetes <emphasis role=\"pkg\">debootstrap</emphasis> y <emphasis role=\"pkg\">rsync</emphasis>) instalará un contenedor Debian:"

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
#| "</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
#| "Checking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \n"
#| "Downloading debian minimal ...\n"
#| "I: Retrieving Release \n"
#| "I: Retrieving Release.gpg \n"
#| "[...]\n"
#| "Download complete.\n"
#| "Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
#| "[...]\n"
#| "root@mirwiz:~# </computeroutput>\n"
#| "        "
msgid ""
"<computeroutput># </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"root@mirwiz:~# </computeroutput>\n"
"        "

msgid "Note that the filesystem is initially created in <filename>/var/cache/lxc</filename>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required."
msgstr "Sepa que inicialmente se crea el sistema de archivos en <filename>/var/cache/lxc</filename> y luego es mudado a su directorio de destino. Esto permite crear contenedores idénticos mucho más rápido ya que luego sólo necesita copiarlo."

msgid "Note that the Debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgstr "Tenga en cuenat que el script de creación de plantillas de Debian acepta la opción <option>--arch</option> para especificar la arquitectura del sistema a instalar y la opción <option>--release</option> si desea instalar algo diferente a la versión estable actual de Debian. También puede definir la variable de entorno <literal>MIRROR</literal> apuntando a una réplica Debian local."

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package further creates a bridge interface <literal>lxcbr0</literal>, which by default is used by all newly created containers via <filename>/etc/lxc/default.conf</filename> and the <filename>lxc-net</filename> service:"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/lxc/default.conf</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>lxc-net.service</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>veth</literal> interface</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "<primary><literal>veth</literal>, network interface</primary>"
msgstr ""

#, fuzzy
#| msgid ""
#| "lxc.net.0.type = veth\n"
#| "lxc.net.0.flags = up\n"
#| "lxc.net.0.link = br0\n"
#| "lxc.net.0.hwaddr = 4a:49:43:49:79:20"
msgid ""
"lxc.net.0.type = veth\n"
"lxc.net.0.link = lxcbr0\n"
"lxc.net.0.flags = up"
msgstr ""
"lxc.net.0.type = veth\n"
"lxc.net.0.flags = up\n"
"lxc.net.0.link = br0\n"
"lxc.net.0.hwaddr = 4a:49:43:49:79:20"

#, fuzzy
#| msgid "These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <literal>br0</literal> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated."
msgid "These entries mean, respectively, that a virtual interface will be created in every new container; that it will automatically be brought up when said container is started; and that it will be automatically connected to the <literal>lxcbr0</literal> bridge on the host. You will find these settings in the created container's configuration (<filename>/var/lib/lxc/testlxc/config</filename>), where also the device' MAC address will be specified in <literal>lxc.net.0.hwaddr</literal>. Should this last entry be missing or disabled, a random MAC address will be generated."
msgstr "Estas líneas significan, respectivamente, que se creará una interfaz virtual en el contenedor; que será iniciada automáticamente cuando inicie el contenedor; que será conectada automáticamente al puente <literal>br0</literal> en el anfitrión; y que su dirección MAC será la especificada. En caso que esta última línea no exista o esté desactivada, se generará una dirección MAC aleatoria."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary>container configuration</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Another useful entry in that file is the setting of the hostname:"
msgstr "Otro elemento útil en dicho archivo es la configuración del nombre del equipo:"

msgid "lxc.uts.name = testlxc"
msgstr "lxc.uts.name = testlxc"

msgid "The newly-created filesystem now contains a minimal Debian system and a network interface."
msgstr ""

msgid "Starting the Container"
msgstr "Inicio del contenedor"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-start</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-attach</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "Now that our virtual machine image is ready, let's start the container with <command>lxc-start --daemon --name=testlxc</command>."
msgid "Now that our virtual machine image is ready, let's start the container with <command>lxc-start --name=testlxc</command>."
msgstr "Ahora que nuestra máquina virtual está lista, iniciemos el contenedor con <command>lxc-start --daemon --name=testlxc</command>."

#, fuzzy
#| msgid "In LXC releases following 2.0.8, root passwords are not set by default. We can set one running <command>lxc-attach -n testlxc <replaceable>passwd</replaceable>.</command> Now we can login:"
msgid "In LXC releases following 2.0.8, root passwords are not set by default. We can set one running <command>lxc-attach -n testlxc <replaceable>passwd</replaceable></command> if we want. We can login with:"
msgstr "En publicaciones de LXC posteriores a la 2.0.8, las contraseñas del superusuario por defecto no están asignadas. Podemos asignar una ejecutando <command>lxc-attach -n testlxc <replaceable>contraseña</replaceable>.</command>. Ahora podemos acceder:"

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
#| "</userinput><computeroutput>Debian GNU/Linux 9 testlxc console\t\n"
#| "\n"
#| "testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
#| "Password: \n"
#| "Linux testlxc 4.19.0-5-amd64 #1 SMP Debian 4.19.37-5 (2019-06-19) x86_64\n"
#| "\n"
#| "The programs included with the Debian GNU/Linux system are free software;\n"
#| "the exact distribution terms for each program are described in the\n"
#| "individual files in /usr/share/doc/*/copyright.\n"
#| "\n"
#| "Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
#| "permitted by applicable law.\n"
#| "root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
#| "<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
#| "root         1  0.0  0.2  56736  6608 ?        Ss   09:28   0:00 /sbin/init\n"
#| "root        32  0.0  0.1  46096  4680 ?        Ss   09:28   0:00 /lib/systemd/systemd-journald\n"
#| "root        75  0.0  0.1  67068  3328 console  Ss   09:28   0:00 /bin/login --\n"
#| "root        82  0.0  0.1  19812  3664 console  S    09:30   0:00  \\_ -bash\n"
#| "root        88  0.0  0.1  38308  3176 console  R+   09:31   0:00      \\_ ps auxwf\n"
#| "root        76  0.0  0.1  69956  5636 ?        Ss   09:28   0:00 /usr/sbin/sshd -D\n"
#| "root@testlxc:~# </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput><![CDATA[Connected to tty 1\n"
"Type <Ctrl+a q> to exit the console, <Ctrl+a Ctrl+a> to enter Ctrl+a itself\n"
"\n"
"Debian GNU/Linux 11 testlxc tty1\n"
"\n"
"testlxc login: ]]></computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 5.10.0-11-amd64 #1 SMP Debian 5.10.92-1 (2022-01-18) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"Last login: Wed Mar  9 01:45:21 UTC 2022 on console\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf\n"
"</userinput><computeroutput>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root           1  0.0  0.2  18964 11464 ?        Ss   01:36   0:00 /sbin/init\n"
"root          45  0.0  0.2  31940 10396 ?        Ss   01:37   0:00 /lib/systemd/systemd-journald\n"
"root          71  0.0  0.1  99800  5724 ?        Ssl  01:37   0:00 /sbin/dhclient -4 -v -i -pf /run/dhclient.eth0.pid [..]\n"
"root          97  0.0  0.1  13276  6980 ?        Ss   01:37   0:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups\n"
"root         160  0.0  0.0   6276  3928 pts/0    Ss   01:46   0:00 /bin/login -p --\n"
"root         169  0.0  0.0   7100  3824 pts/0    S    01:51   0:00  \\_ -bash\n"
"root         172  0.0  0.0   9672  3348 pts/0    R+   01:51   0:00      \\_ ps auxwf\n"
"root         164  0.0  0.0   5416  2128 pts/1    Ss+  01:49   0:00 /sbin/agetty -o -p -- \\u --noclear [...]\n"
"root@testlxc:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 9 testlxc console\t\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 4.19.0-5-amd64 #1 SMP Debian 4.19.37-5 (2019-06-19) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  56736  6608 ?        Ss   09:28   0:00 /sbin/init\n"
"root        32  0.0  0.1  46096  4680 ?        Ss   09:28   0:00 /lib/systemd/systemd-journald\n"
"root        75  0.0  0.1  67068  3328 console  Ss   09:28   0:00 /bin/login --\n"
"root        82  0.0  0.1  19812  3664 console  S    09:30   0:00  \\_ -bash\n"
"root        88  0.0  0.1  38308  3176 console  R+   09:31   0:00      \\_ ps auxwf\n"
"root        76  0.0  0.1  69956  5636 ?        Ss   09:28   0:00 /usr/sbin/sshd -D\n"
"root@testlxc:~# </computeroutput>"

msgid "We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<filename>/var/lib/lxc/testlxc/rootfs</filename>). We can exit the console with <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."
msgstr "Ahora estamos dentro del contenedor; nuestro acceso a los procesos está restringido a aquellos iniciados dentro del mismo contenedor y nuestro acceso al sistema de archivos está limitado de forma similar al subconjunto dedicado del sistema de archivos completo (<filename>/var/lib/lxc/testlxc/rootfs</filename>). Podemos salir a la consola con <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."

#, fuzzy
#| msgid "Note that we ran the container as a background process, thanks to the <option>--daemon</option> option of <command>lxc-start</command>. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgid "Note that we ran the container as a background process, thanks to <command>lxc-start</command> starting using the <option>--daemon</option> option by default. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgstr "Tenga en cuenta que ejecutamos el contenedor como un proceso en segundo plano gracias a la opción <option>--daemon</option> de <command>lxc-start</command>. Podemos interrumpir el contenedor ejecutando <command>lxc-stop --name=testlxc</command>."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-stop</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <command>lxc-autostart</command> which starts containers whose <literal>lxc.start.auto</literal> option is set to 1). Finer-grained control of the startup order is possible with <literal>lxc.start.order</literal> and <literal>lxc.group</literal>: by default, the initialization script first starts containers which are part of the <literal>onboot</literal> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <literal>lxc.start.order</literal> option."
msgstr "El paquete <emphasis role=\"pkg\">lxc</emphasis> contiene un script de inicialización que puede automatizar el inicio de uno o más contenedores cuando el sistema principal arranca (confía en el comando <command>lxc-autostart</command> el cual inicia los contenedores que tienen la opción <literal>lxc.start.auto</literal> configurada a 1). Se puede obtener un control más detallado del orden de inicio con <literal>lxc.start.order</literal> y <literal>lxc.group</literal>: por defecto, el script de inicialización inicia los contenedores que son parte del grupo <literal>onboot</literal> y luego los contenedores que no forman parte de este grupo. En ambos casos el orden dentro de un grupo es definido por la opción <literal>lxc.start.order</literal>."

msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> Virtualización en masa"

msgid "Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <literal>tap</literal> and <literal>veth</literal> interfaces should be enough in many cases."
msgstr "Debido a que LXC es un sistema de aislación muy liviano, puede adaptarse particularmente al almacenamiento masivo de servidores virtuales. La configuración de red probablemente sea un poco más avanzada que la que describimos, pero la configuración «enriquecida» utilizando interfaces <literal>tap</literal> y <literal>veth</literal> debería ser suficiente en muchos casos."

msgid "It may also make sense to share part of the filesystem, such as the <filename>/usr</filename> and <filename>/lib</filename> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <literal>lxc.mount.entry</literal> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage."
msgstr "También puede tener sentido compartir parte del sistema de archivos, como los subárboles <filename>/usr</filename> y <filename>/lib</filename> para evitar duplicar el software que puede ser común a varios contenedores. Generalmente se consigue esto con elementos <literal>lxc.mount.entry</literal> en el archivo de configuración de los contenedores. Un efecto secundario interesante es que el proceso utilizará menos memoria física ya que el núcleo puede detectar que se comparten los programas. El costo marginal de un contenedor adicional se puede reducir al espacio en disco dedicado a sus datos específicos y unos pocos procesos adicionales que el núcleo debe gestionar y programar."

msgid "We haven't described all the available options, of course; more comprehensive information can be obtained from the <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> and <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual pages and the ones they reference."
msgstr "Obviamente, no describimos todas las opciones disponibles; puede obtener información más completa en las páginas de manual <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> y <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> así como también aquellas a las que hacen referencia."

msgid "Virtualization with KVM"
msgstr "Virtualización con KVM"

msgid "<primary>Kernel-based Virtual Machine</primary><see>KVM</see>"
msgstr ""

msgid "KVM, which stands for <emphasis>Kernel-based Virtual Machine</emphasis>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <command>qemu-*</command> commands: it is still about KVM."
msgstr "KVM, acrónimo de <emphasis>máquina virtual basada en el núcleo</emphasis> («Kernel-based Virtual Machine»), es primero que nada un módulo del núcleo que provee la mayor parte de la infraestructura que puede usar un virtualizador, pero no es un virtualizador en sí mismo. El control real de la virtualización es gestionado por una aplicación basada en QEMU. No se preocupe si esta sección menciona programas <command>qemu-*</command>, continúa hablando sobre KVM."

msgid "Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <filename>/proc/cpuinfo</filename>."
msgstr "A diferencia de otros sistemas de virtualización, se integró KVM al núcleo Linux desde el comienzo. Sus desarrolladores eligieron aprovechar el conjunto de instrucciones de procesador dedicados a la virtualización (Intel-VT y AMD-V), lo que mantiene a KVM liviano, elegante y no muy hambriento de recursos. La contraparte, obviamente, es que KVM no funciona en ordenadores con procesadores distintos a estos. Para los ordenadores basados en i386 y amd64, puede verificar si tiene uno de estos procesadores si encuentra a «vmx» o «svm» entre las opciones de CPU («flags») enumeradas en <filename>/proc/cpuinfo</filename>."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/proc</filename></primary><secondary><filename>/proc/cpuinfo</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization."
msgstr "Con Red Hat respaldando activamente su desarrollo, KVM parece haberse convertido en la referencia de virtualización en Linux."

msgid "<primary><command>virt-install</command></primary>"
msgstr "<primary><command>virt-install</command></primary>"

#, fuzzy
#| msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The virtual <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgstr "A diferencia de herramientas como VirtualBox, KVM por sí mismo no incluye ninguna interfaz de usuario para crear y administrar máquinas virtuales. El paquete <emphasis role=\"pkg\">qemu-kvm</emphasis> sólo provee un ejecutable para iniciar máquinas virtuales así como el script de inicialización que carga los módulos de núcleo apropiados."

msgid "<primary>libvirt</primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>OpenVZ</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>UML</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <command>virtual-manager</command> is a graphical interface that uses libvirt to create and manage virtual machines."
msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare, and UML). <command>virt-manager</command> is a graphical interface that uses <emphasis>libvirt</emphasis> to create and manage virtual machines."
msgstr "Afortunadamente, Red Hat también provee otro conjunto de herramientas para solucionar este problema con el desarrollo de la biblioteca <emphasis>libvirt</emphasis> y las herramientas <emphasis>gestor de máquina virtual</emphasis> («virtual machine manager») asociadas. libvirt permite administrar máquinas virtuales de manera uniforme e independiente al sistema de virtualización subyacente (actualmente es compatible con QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare y UML). <command>virtual-manager</command> es una interfaz gráfica que utiliza libvirt para crear y administrar máquinas virtuales."

msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>daemon</primary><secondary>libvirtd</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>libvirtd.service</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "We first install the required packages, with <command>apt-get install libvirt-clients libvirt-daemon-system qemu-kvm virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-daemon-system</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. <emphasis role=\"pkg\">libvirt-clients</emphasis> provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgstr "Primero instalaremos los paquetes necesarios con <command>apt-get install libvirt-clients libvirt-daemon-system qemu-kvm virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-daemon-system</emphasis> provee el demonio <command>libvirtd</command>, que permite la gestión (posiblemente remota) de máquinas virtuales ejecutando en el equipo e inicia las VMs necesarias cuando éste inicia. <emphasis role=\"pkg\">libvirt-clients</emphasis> provee la herramienta de consola <command>virsh</command> que permite controlar los equipos administrados con <command>libvirtd</command>."

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>virt-viewer</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "The <emphasis role=\"pkg\">virtinst</emphasis> package provides <command>virt-install</command>, which allows creating virtual machines from the command line. Finally, <emphasis role=\"pkg\">virt-viewer</emphasis> allows accessing a VM's graphical console."
msgstr "El paquete <emphasis role=\"pkg\">virtinst</emphasis> provee <command>virt-install</command>, que permite crear máquinas virtuales desde una consola. Finalmente, <emphasis role=\"pkg\">virt-viewer</emphasis> permite acceder a la consola gráfica de una VM."

msgid "Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <xref linkend=\"sect.lxc.network\" />)."
msgstr "De la misma forma que en Xen y LXC, la configuración de red más frecuente involucra un puente que agrupa las interfaces de red de las máquinas virtuales (revise la <xref linkend=\"sect.lxc.network\" />)."

msgid "Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network."
msgstr "Alternativamente, y de forma predeterminada en la configuración de KVM, se le asigna una dirección privada (en el rango 192.168.122.0/24) a la máquina virtual y se configura NAT para que la VM pueda acceder a la red externa."

msgid "The rest of this section assumes that the host has an <literal>eth0</literal> physical interface and a <literal>br0</literal> bridge, and that the former is connected to the latter."
msgstr "El resto de esta sección asume que el anfitrión posee una interfaz física <literal>eth0</literal> y un puente <literal>br0</literal> que está conectado a la primera interfaz."

msgid "Installation with <command>virt-install</command>"
msgstr "Instalación con <command>virt-install</command>"

msgid "Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line."
msgstr "Crear una máquina virtual es muy similar a instalar un sistema normal, excepto que describirá las características de la máquina virtual en una línea que parecerá infinita."

msgid "Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <xref linkend=\"sect.remote-desktops\" /> for details), which will allow us to control the installation process."
msgstr "En la práctica, esto significa que utilizaremos el instalador de Debian, iniciando la máquina virtual en un dispositivo DVD-ROM virtual que está asociado con la imagen del DVD Debian almacenado en el sistema anfitrión. La VM exportará su consola gráfica sobre el protocolo VNC (revise la <xref linkend=\"sect.remote-desktops\" /> para más detalles), lo que nos permitirá controlar el proceso de instalación."

#, fuzzy
#| msgid "We first need to tell libvirtd where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgid "We first need to tell <command>libvirtd</command> where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgstr "Primero necesitaremos indicarle a libvirtd dónde almacenar las imágenes de disco, a menos que la ubicación predeterminada (<filename>/var/lib/libvirt/images</filename>) sea adecuada."

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
#| "<computeroutput>Pool srv-kvm created\n"
#| "\n"
#| "root@mirwiz:~# </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mkdir /srv/kvm\n"
"</userinput><computeroutput># </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm\n"
"</userinput><computeroutput>Pool srv-kvm created\n"
"\n"
"# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"

msgid "<emphasis>TIP</emphasis> Add your user to the libvirt group"
msgstr "<emphasis>CONSEJO</emphasis> Añada su usuario al grupo libvirt"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>group</primary><secondary><literal>libvirt</literal></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yourself to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgstr "En todos los ejemplos de esta sección se da por hecho que Ud. está ejecutando los comandos como root. Efectivamente, si quiere controlar el demonio local libvirt, necesitará ser root o ser un miembro del grupo <literal>libvirt</literal> (lo cual no viene por defecto). POr tanto, si quiere evitar usar permisos de root muy a menudo, puede añadirse al grupo <literal>libvirt</literal> y ejecutar los distintos comandos bajo su identidad."

msgid "Let us now start the installation process for the virtual machine, and have a closer look at <command>virt-install</command>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed."
msgstr "Ahora iniciaremos el proceso de instalación para la máquina virtual y veremos en más detalle las opciones más importantes de <command>virt-install</command>. Este programa registra en libvirtd la máquina virtual y sus parámetros y luego la inicia para continuar el proceso de instalación."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
#| "               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
#| "               --name testkvm            <co id=\"virtinst.name\"></co>\n"
#| "               --memory 1024             <co id=\"virtinst.ram\"></co>\n"
#| "               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n"
#| "               --cdrom /srv/isos/debian-10.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
#| "               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n"
#| "               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n"
#| "               --os-type linux           <co id=\"virtinst.os\"></co>\n"
#| "               --os-variant debian10\n"
#| "</userinput><computeroutput>\n"
#| "Starting install...\n"
#| "Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --memory 2048             <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-11.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n"
"               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debiantesting\n"
"</userinput><computeroutput>\n"
"\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'\n"
"\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --memory 1024             <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-10.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n"
"               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debian10\n"
"</userinput><computeroutput>\n"
"Empezando la instalación...\n"
"Asignando 'testkvm2.qcow'                                   |  10 GB  00:00\n"
"</computeroutput>"

msgid "The <literal>--connect</literal> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<literal>/system</literal>) from others (<literal>/session</literal>)."
msgstr "La opción <literal>--connect</literal> especifica el «hypervisor» a utilizar. En forma de una URL que contiene un sistema de virtualización (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, etc.) y el equipo que alojará la VM (puede dejarlo vacío si es el equipo local). Además, y en el caso de QEMU/KVM, cada usuario puede administrar máquinas virtuales con permisos restringidos, y la ruta de la URL permite diferenciar equipos de «sistema» (<literal>/system</literal>) de los demás (<literal>/session</literal>)."

msgid "Since KVM is managed the same way as QEMU, the <literal>--virt-type kvm</literal> allows specifying the use of KVM even though the URL looks like QEMU."
msgstr "Debido a que se administra KVM de la misma forma que QEMU, la opción <literal>--virt-type kvm</literal> permite especificar que se utilice KVM aunque la URL parezca una de QEMU."

msgid "The <literal>--name</literal> option defines a (unique) name for the virtual machine."
msgstr "La opción <literal>--name</literal> define un nombre (único) para la máquina virtual."

msgid "The <literal>--memory</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgstr "La opción <literal>--memory</literal> permite especificar la cantidad de RAM (en MB) que reservar para la máquina virtual."

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary><literal>qcow2</literal></primary>"
msgstr "<primary>libvirt</primary>"

msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>qcow2</literal>) allows starting with a small file that only grows when the virtual machine starts actually using space."
msgstr "La opción <literal>--disk</literal> especifica la ubicación del archivo de imagen que representará el disco duro de nuestra máquina virtual; se creará este archivo, a menos que ya exista, de un tamaño (en GB) especificado por el parámetro <literal>size</literal>. El parámetro <literal>format</literal> permite elegir entre las diferentes formas de almacenar el archivo de imagen. El formato predeterminado (<literal>cow2</literal>) permite iniciar con un archivo pequeño que sólo crece cuando la máquina virtual realmente utiliza el espacio."

msgid "The <literal>--cdrom</literal> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <literal>/dev/cdrom</literal>)."
msgstr "Utilizamos la opción <literal>--cdrom</literal> para indicar dónde encontrar el disco óptico a utilizar para la instalación. La ruta puede ser una ruta local para un archivo ISO, una URL donde se puede obtener el archivo o el archivo de dispositivo de un CD-ROM físico (es decir: <literal>/dev/cdrom</literal>)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>virbr</literal> interface</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>libvirt</primary><secondary><literal>virbr</literal></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "<primary><literal>virbr</literal>, network interface</primary>"
msgstr ""

msgid "The <literal>--network</literal> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24)."
msgstr "La opción <literal>--network</literal> especifica cómo se integra la tarjeta de red virtual a la configuración de red del anfitrión. El comportamiento predeterminado (que forzamos explícitamente en nuestro ejemplo) es integrarla en un puente de red preexistente. Si no existe dicho puente, la máquina virtual sólo llegará a la red física mediante NAT, por lo que se asignará una dirección en el rango de subredes privadas (192.168.122.0/24)."

msgid "The default network configuration, which contains the definition for a <literal>virbr0</literal> bridge interface, can be edited using <command>virsh net-edit default</command> and started via <command>virsh net-start default</command> if not already done automatically during system start."
msgstr ""

msgid "<literal>--graphics vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, <literal>--graphics vnc,listen=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgstr "<literal>--graphics vnc</literal> indica que debe estar disponible la consola gráfica a través de VNC. El comportamiento predeterminado para el servidor VNC es sólo escuchar en la interfaz local; si debe ejecutar el cliente VNC en otro equipo, necesitará establecer un túnel SSH (revise la <xref linkend=\"sect.ssh-port-forwarding\" />) para poder establecer una conexión. Alternativamente, puede utilizar <literal>--graphics vnc,listen=0.0.0.0</literal> para poder acceder al servidor VNC desde todas las interfaces; sepa que si hace esto, realmente debe diseñar su firewall de forma acorde."

msgid "The <literal>--os-type</literal> and <literal>--os-variant</literal> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there."
msgstr "Las opciones <literal>--os-type</literal> y <literal>--os-variant</literal> permiten optimizar unos pocos parámetros de la máquina virtual basado en características conocidas del sistema operativo mencionado en ellas."

msgid "The full list of OS types can be shown using the <command>osinfo-query os</command> command from the <emphasis role=\"pkg\">libosinfo-bin</emphasis> package."
msgstr ""

msgid "At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <command>virt-viewer</command> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):"
msgstr "En este punto, la máquina virtual está ejecutando y necesitaremos conectarnos a la consola gráfica para continuar con el proceso de instalación. Si realizó la operación anterior de un entorno de escritorio gráfico, esta conexión debería iniciar automáticamente. De lo contrario, o si estamos trabajando de forma remota, puede ejecutar <command>virt-viewer</command> desde cualquier entorno gráfico para abrir la consola gráfica (sepa que le pedirá la contraseña de root del equipo remoto dos veces ya que esta operación necesita dos conexiones SSH):"

msgid ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>servidor</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@servidor password: \n"
"root@servidor's password: </computeroutput>"

#, fuzzy
#| msgid "Installation with <command>virt-install</command>"
msgid "Connecting to installer session using <command>virt-viewer</command>"
msgstr "Instalación con <command>virt-install</command>"

msgid "When the installation process ends, the virtual machine is restarted, now ready for use."
msgstr "Cuando finaliza el proceso de instalación, se reinicia la máquina virtual y está lista para que la utilice."

msgid "Managing Machines with <command>virsh</command>"
msgstr "Administración de máquinas con <command>virsh</command>"

msgid "Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <command>libvirtd</command> for the list of the virtual machines it manages:"
msgstr "Ahora que finalizó la instalación, veamos como gestionar las máquinas virtuales disponibles. Lo primero a intentar es pedirle a <command>libvirtd</command> la lista de máquinas virtuales que administra:"

msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  8 testkvm              shut off\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  8 testkvm              shut off\n"
"</userinput>"

msgid "Let's start our test virtual machine:"
msgstr "Iniciemos nuestra máquina virtual de pruebas:"

msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"

msgid "We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <command>vncviewer</command>):"
msgstr "Ahora podemos obtener las instrucciones de conexión para la consola gráfica (puede pasar como parámetro de <command>vncviewer</command> la pantalla VNC devuelta):"

msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>127.0.0.1:0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>127.0.0.1:0</computeroutput>"

msgid "Other available <command>virsh</command> subcommands include:"
msgstr "Entre otras subórdenes disponibles en <command>virsh</command> encontraremos:"

msgid "<literal>reboot</literal> to restart a virtual machine;"
msgstr "<literal>reboot</literal> para reiniciar una máquina virtual;"

msgid "<literal>shutdown</literal> to trigger a clean shutdown;"
msgstr "<literal>shutdown</literal> para apagarla de forma segura;"

msgid "<literal>destroy</literal>, to stop it brutally;"
msgstr "<literal>destroy</literal>, para detenerla brutalmente;"

msgid "<literal>suspend</literal> to pause it;"
msgstr "<literal>suspend</literal> para pausarla;"

msgid "<literal>resume</literal> to unpause it;"
msgstr "<literal>resume</literal> para continuar su ejecución;"

msgid "<literal>autostart</literal> to enable (or disable, with the <literal>--disable</literal> option) starting the virtual machine automatically when the host starts;"
msgstr "<literal>autostart</literal> para activar (o desactivar con la opción <literal>--disable</literal>) que se inicie la máquina virtual automáticamente cuando inicia el anfitrión;"

msgid "<literal>undefine</literal> to remove all traces of the virtual machine from <command>libvirtd</command>."
msgstr "<literal>undefine</literal> para eliminar todo rastro de la máquina virtual en <command>libvirtd</command>."

msgid "All these subcommands take a virtual machine identifier as a parameter."
msgstr "Todas estas subórdenes aceptan un identificador de máquina virtual como parámetro."

#, fuzzy
#| msgid "Installing an RPM based system in Debian with yum"
msgid "Installing an RPM based chroot in Debian with yum"
msgstr "Instalación de un sistema basado en RPM sobre Debian con yum"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>RPM</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary>chroot</primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>yum</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>rpm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility (available in the package of the same name)."
msgid "If a chroot is meant to run Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>. But if it is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility, available as <command>yum4</command> in the <emphasis role=\"pkg\">nextgen-yum4</emphasis> package, since the original program has been removed from Debian before the <emphasis role=\"distribution\">Bullseye</emphasis> release due to being unmaintained, outdated, and obsoleted by <command>dnf</command>."
msgstr "Si pretende que la máquina virtual ejecute Debian (o uno de sus derivados), puede inicializar el sistema con <command>debootstrap</command> como se describió anteriormente. Pero desea instalar un sistema basado en RMP en la máquina virtual (como Fedora, CentOS o Scientific Linux), necesita realizar la configuración con la aplicación <command>yum</command> (disponible en el paquete del mismo nombre)."

#, fuzzy
#| msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum</command> to extract the remaining set of packages. But since we call <command>yum</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum4</command> to extract the remaining set of packages. But since we call <command>yum4</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgstr "El procedimiento require usar <command>rpm</command> para extraer un conjunto inicial de archivos, incluyendo probablemente bastantes archivos de configuración de <command>yum</command>, y luego ejecutar el comando <command>yum</command> para descomprimir el conjunto de paquetes restantes. Pero desde que podemos llamar a <command>yum</command> desde fuera de una jaula chroot, necesitaremos algunos cambios provisionales. En los ejemplos siguientes, el destino de chroot es <filename>/src/centos</filename>."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
#| "</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
#| "</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
#| "</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
#| "rpm: However assuming you know what you are doing...\n"
#| "warning: centos-release-7-6.1810.2.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
#| "</userinput><computeroutput>[...]\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-9.2009.0.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-9.2009.0.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-9.2009.0.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum4 --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>chroot /srv/centos/\n"
"</userinput><computeroutput>[root@testsystem /]# </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-6.1810.2.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"

msgid "Automated Installation"
msgstr "Instalación automatizada"

msgid "<primary>deployment</primary>"
msgstr "<primary>despliegue</primary>"

msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The Falcot Corp administrators, like many administrators of large IT services, need tools to install (or reinstall) quickly, and automatically if possible, their new machines."
msgstr "Los administradores de Falcot Corp, como muchos administradores de grandes servicios IT, necesitan herramientas para instalar (o reinstalar) rápidamente, y automáticamente si es posible, nuevas máquinas."

msgid "These requirements can be met by a wide range of solutions. On the one hand, generic tools such as SystemImager handle this by creating an image based on a template machine, then deploy that image to the target systems; at the other end of the spectrum, the standard Debian installer can be preseeded with a configuration file giving the answers to the questions asked during the installation process. As a sort of middle ground, a hybrid tool such as FAI (<emphasis>Fully Automatic Installer</emphasis>) installs machines using the packaging system, but it also uses its own infrastructure for tasks that are more specific to massive deployments (such as starting, partitioning, configuration and so on)."
msgstr "Un amplio rango de soluciones pueden satisfacer estos requisitos. Por el otro lado, herramientas genéricas como SystemImager lo hacen creando una imagen basada en una máquina patrón y luego desplegando dicha imagen en los sistemas objetivo; en el otro extremo del espectro, el instalador Debian estándar puede ser presembrado con un archivo de configuración que provee las respuestas a las preguntas realizadas durante el proceso de instalación. Como un tipo de punto medio, una herramienta híbrida como FAI (<emphasis>instalador completamente automático</emphasis>: «Fully Automatic Installer») instala los equipos con el sistema de paquetes, pero también utiliza su propia infraestructura para tareas más específicas de despliegues masivos (como inicialización, particionado, configuración, etc)."

#, fuzzy
#| msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an i386 image won't fit on a powerpc machine, and so on)."
msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an amd64 image won't fit on a powerpc machine, and so on)."
msgstr "Cada una de estas herramientas tiene sus ventajas y desventajas: SystemImager funciona independientemente de cualquier sistema de paquetes particular, lo que permite gestionar grandes conjuntos de máquinas que utilizan diferentes distribuciones Linux. También incluye un sistema de actualización que no necesita una reinstalación, pero sólo puede confiar en este sistema de actualización si no se modifican las máquinas de forma independiente; en otras palabras, el usuario no debe actualizar ningún software por su cuenta ni instalar otro software. De forma similar, no se debe automatizar las actualizaciones de seguridad porque éstos deben pasar por la imagen de referencia centralizada que administra SystemImager. Esta solución también requiere que las máquinas objetivo sean homogéneas, de lo contrario necesitará mantener y administrar diferentes imágenes (no podrá utilizar una imagen i386 en una máquina powerpc, etc.)."

#, fuzzy
#| msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly simple-CDD (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even the simple-CDD solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly <emphasis role=\"pkg\">simple-cdd</emphasis> (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even this solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgstr "Por el otro lado, puede adaptar la instalación automatizada con debian-installer a cada máquina específica: el instalador obtendrá el núcleo y los paquetes de software apropiados de los repositorios relevantes, detectará el hardware disponible, particionará el disco duro completo para aprovechar todo el espacio disponible, instalará el sistema Debian correspondiente y configurará el gestor de arranque adecuado. Sin embargo, el instalador estándar sólo instalará versiones de Debian estándar, con el sistema base y un subconjunto de «tareas» preseleccionadas; esto no permite instalar un sistema particular con aplicaciones no empaquetadas. Satisfacer esta necesidad particular requerirá modificar el instalador… afortunadamente el instalador es muy modular y existen herramientas para automatizar la mayor parte del trabajo necesario para esta personalización, la más importante siendo simple-CDD (CDD es acrónimo de <emphasis>derivado personalizado de Debian</emphasis>: «Custom Debian Derivative»). Inclusive la solución simple-CDD, sin embargo, sólo gestiona la instalación inicial; lo que no es un problema generalmente ya que las herramientas de APT permite desplegar actualizaciones de forma eficiente más adelante."

#, fuzzy
#| msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian), in order to focus more intently on debian-installer and simple-CDD, which are more interesting in a Debian-only context."
msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian, but available as a third-party package), in order to focus more intently on debian-installer and <emphasis role=\"pkg\">simple-cdd</emphasis>, which are more interesting in a Debian-only context."
msgstr "Sólo haremos una revisión general de FAI y saltearemos SystemImager por completo (ya no se encuentra en Debian), para poder enfocarnos más intensamente en debian-installer y simple-CDD, que son más interesantes en un contexto sólo con Debian."

msgid "Fully Automatic Installer (FAI)"
msgstr "Instalador completamente automático (FAI: «Fully Automatic Installer»)"

#, fuzzy
#| msgid "<primary>Fully Automatic Installer (FAI)</primary>"
msgid "<primary>Fully Automatic Installer</primary><see>FAI</see>"
msgstr "<primary>Fully Automatic Installer (FAI)</primary>"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>FAI</primary>"
msgstr "<primary>RAID</primary>"

msgid "<foreignphrase>Fully Automatic Installer</foreignphrase> is probably the oldest automated deployment system for Debian, which explains its status as a reference; but its very flexible nature only just compensates for the complexity it involves."
msgstr "<foreignphrase>Fully Automatic Installer</foreignphrase> es probablemente el sistema de despliegue automático para Debian más antiguo, lo que explica su estado como referencia; pero su naturaleza flexible compensa su complejidad."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary>FAI</primary><secondary><emphasis role=\"pkg\">fai-server</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary>FAI</primary><secondary><emphasis role=\"pkg\">fai-quickstart</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

msgid "FAI requires a server system to store deployment information and allow target machines to boot from the network. This server requires the <emphasis role=\"pkg\">fai-server</emphasis> package (or <emphasis role=\"pkg\">fai-quickstart</emphasis>, which also brings the required elements for a standard configuration)."
msgstr "FAI necesita un sistema servidor para almacenar la información de despliegue y permitir que las máquinas objetivo arranquen desde la red. Este servidor necesita el paquete <emphasis role=\"pkg\">fai-server</emphasis> (o <emphasis role=\"pkg\">fai-quickstart</emphasis>, que también incluye los elementos necesarios para una configuración estándar)."

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/fai/</filename></secondary><see>FAI</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><filename>/etc/fai/nfsroot.conf</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> according to <filename>/etc/fai/nfsroot.conf</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgstr "FAI utiliza un enfoque específico para definir los varios perfiles instalables. En lugar de simplemente duplicar una instalación de referencia, FAI es un instalador completo, totalmente configurable a través de archivos y scripts almacenados en el servidor; no se crea automáticamente la ubicación predeterminada <filename>/srv/fai/config/</filename>, por lo que el administrador debe crearla junto con los archivos relevantes. La mayoría de las veces, estos archivos serán personalizados de archivos de ejemplos disponibles en la documentación del paquete <emphasis role=\"pkg\">fai-doc</emphasis>, en el directorio <filename>/usr/share/doc/fai-doc/examples/simple/</filename> en particular."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai-setup</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai-cd</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start a FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgstr "Una vez que definimos los perfiles, el programa <command>fai-setup</command> genera los elementos necesarios para iniciar una instalación FAI; esto significa en su mayor parte preparar o actualizar un sistema mínimo (raíz NFS) para utilizar durante la instalación. Una alternativa es generar un CD de arranque dedicado con <command>fai-cd</command>."

msgid "Creating all these configuration files requires some understanding of the way FAI works. A typical installation process is made of the following steps:"
msgstr "Crear todos estos archivos de configuración requiere entender cómo funciona FAI. Un proceso de instalación típico consiste de los siguientes pasos:"

msgid "fetching a kernel from the network, and booting it;"
msgstr "obtener un núcleo de la red e iniciarlo;"

msgid "mounting the root filesystem from NFS;"
msgstr "montar el sistema de archivos raíz desde NFS;"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "executing <command>/usr/sbin/fai</command>, which controls the rest of the process (the next steps are therefore initiated by this script);"
msgstr "ejecutar <command>/usr/sbin/fai</command> que controla el resto del proceso (los pasos siguientes, por lo tanto, son iniciados por este script);"

msgid "copying the configuration space from the server into <filename>/fai/</filename>;"
msgstr "copiar el espacio de configuración desde el servidor a <filename>/fai/</filename>;"

msgid "running <command>fai-class</command>. The <filename>/fai/class/[0-9][0-9]*</filename> scripts are executed in turn, and return names of “classes” that apply to the machine being installed; this information will serve as a base for the following steps. This allows for some flexibility in defining the services to be installed and configured."
msgstr "ejecutar <command>fai-class</command>. Se ejecutan en orden los scripts <filename>/fai/class/[0-9][0-9]*</filename> y devuelve los nombres de «clases» que aplican a la máquina siendo instalada; esta información servirá como base para los pasos siguientes. Esto permite cierta flexibilidad en la definición de los servicios a instalar y configurar."

msgid "fetching a number of configuration variables, depending on the relevant classes;"
msgstr "obtener una cantidad de variables de configuración, que dependen de las clases relevantes;"

msgid "partitioning the disks and formatting the partitions, based on information provided in <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;"
msgstr "particionar los discos y dar formato a las particiones basándose en la información provista por <filename>/fai/disk_config/<replaceable>clase</replaceable></filename>;"

msgid "mounting said partitions;"
msgstr "montar dichas particiones;"

msgid "installing the base system;"
msgstr "instalar el sistema base;"

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary>FAI</primary><secondary><command>fai-debconf</command></secondary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "preseeding the Debconf database with <command>fai-debconf</command>;"
msgstr "presembrar la base de datos Debconf con <command>fai-debconf</command>;"

msgid "fetching the list of available packages for APT;"
msgstr "obtener la lista de paquetes disponibles para APT;"

msgid "installing the packages listed in <filename>/fai/package_config/<replaceable>class</replaceable></filename>;"
msgstr "instalar los paquetes enumerados en <filename>/fai/package_config/<replaceable>clase</replaceable></filename>;"

msgid "executing the post-configuration scripts, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;"
msgstr "ejecutar los scripts postconfiguración, <filename>/fai/scripts/<replaceable>clase</replaceable>/[0-9][0-9]*</filename>;"

msgid "recording the installation logs, unmounting the partitions, and rebooting."
msgstr "grabar los registros de instalación, desmontar las particiones y reiniciar."

msgid "Preseeding Debian-Installer"
msgstr "Presembrado de Debian-Installer"

msgid "<primary>preseed</primary>"
msgstr "<primary>presembrar</primary>"

msgid "<primary>preconfiguration</primary>"
msgstr "<primary>preconfiguración</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>installation</primary><secondary>preseeding</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <emphasis>preseeding</emphasis>."
msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <foreignphrase>preseeding</foreignphrase>."
msgstr "Después de todo, la mejor herramienta para instalar sistemas Debian lógicamente debería ser el instalador oficial de Debian. Es por esto que, desde su concepción, se diseñó debian-installer para usarlo de forma automatizada aprovechando la infraestructura que provee <emphasis role=\"pkg\">debconf</emphasis>. Este último permite, por un lado, reducir la cantidad de preguntas realizadas (las preguntas escondidas utilizarán la respuesta predeterminada provista) y por el otro proveer respuestas predeterminadas por separado para que la instalación pueda no ser interactiva. Se conoce a esta última funcionalidad como <emphasis>presembrado</emphasis> («preseeding»)."

msgid "<emphasis>GOING FURTHER</emphasis> Debconf with a centralized database"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> Debconf con una base de datos centralizada"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">debconf-doc</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Preseeding allows to provide a set of answers to Debconf questions at installation time, but these answers are static and do not evolve as time passes. Since already-installed machines may need upgrading, and new answers may become required, the <filename>/etc/debconf.conf</filename> configuration file can be set up so that Debconf uses external data sources (such as an LDAP directory server, or a remote file accessed via NFS or Samba). Several external data sources can be defined at the same time, and they complement one another. The local database is still used (for read-write access), but the remote databases are usually restricted to reading. The <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual page describes all the possibilities in detail (you need the <emphasis role=\"pkg\">debconf-doc</emphasis> package)."
msgstr "El presembrado permite proveer un conjunto de respuestas a preguntas Debconf en el momento de instalación, pero estas respuestas son estáticas y no evolucionan con el tiempo. Debido a que máquinas ya instaladas puede necesitar ser actualizadas, y podrían requerir nuevas respuestas, puede definir el archivo de configuración <filename>/etc/debconf.conf</filename> para que Debconf utilice fuentes de datos externas (como un servidor de directorio LDAP o un archivo remoto al que accede con NFS o Samba). Puede definir varias fuentes de datos externas simultáneamente y que éstas se complementen. Todavía utilizará la base de datos local (para acceso de lectura y escritura), pero generalmente se restringen para lectura a las bases de datos remotas. La página de manual <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> describe en detalle todas las posibilidades (necesitará el paquete <emphasis role=\"pkg\">debconf-doc</emphasis>)."

msgid "<primary><command>debconf</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "Using a Preseed File"
msgstr "Utilización de un archivo de presembrado"

msgid "There are several places where the installer can get a preseeding file:"
msgstr "Hay varios lugares de los que el instalador puede obtener un archivo de presembrado:"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary><filename>preseed.cfg</filename></primary>"
msgstr "<primary>presembrar</primary>"

msgid "in the initrd used to start the machine; in this case, preseeding happens at the very beginning of the installation, and all questions can be avoided. The file just needs to be called <filename>preseed.cfg</filename> and stored in the initrd root."
msgstr "en el initrd que arranca la máquina; en este caso, el presembrado ocurre muy al comienzo de la instalación y puede evitar todas las preguntas. Sólo debe asegurarse que el archivo tenga el nombre <filename>preseed.cfg</filename> y esté almacenado en la raíz del initrd."

msgid "on the boot media (CD or USB key); preseeding then happens as soon as the media is mounted, which means right after the questions about language and keyboard layout. The <literal>preseed/file</literal> boot parameter can be used to indicate the location of the preseeding file (for instance, <filename>/cdrom/preseed.cfg</filename> when the installation is done off a CD-ROM, or <filename>/hd-media/preseed.cfg</filename> in the USB-key case)."
msgstr "en el medio de arranque (CD o llave USB); el presembrado ocurre tan pronto como se monte el medio, lo que significa inmediatamente después de las preguntas sobre idioma y distribución de teclado. Puede utilizar el parámetro de arranque <literal>preseed/file</literal> para indicar la ubicación del archivo de presembrado (por ejemplo, <filename>/cdrom/preseed.cfg</filename> cuando se realiza la instalación desde un CD-ROM o <filename>/hd-media/preseed.cfg</filename> en el caso de una llave USB)."

#, fuzzy
#| msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."
msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal> (HTTPS, FTPS, SFTP, etc. are not supported)."
msgstr "desde la red; el presembrado ocurrirá entonces sólo después que se configure (automáticamente) la red; el parámetro de arranque relevante es <literal>preseed/url=http://<replaceable>servidor</replaceable>/preseed.cfg</literal>."

msgid "At a glance, including the preseeding file in the initrd looks like the most interesting solution; however, it is rarely used in practice, because generating an installer initrd is rather complex. The other two solutions are much more common, especially since boot parameters provide another way to preseed the answers to the first questions of the installation process. The usual way to save the bother of typing these boot parameters by hand at each installation is to save them into the configuration for <command>isolinux</command> (in the CD-ROM case) or <command>syslinux</command> (USB key)."
msgstr "A primera vista, incluir el archivo de presembrado en el initrd parecería la solución más interesante; sin embargo, rara vez se la utiliza en la práctica porque generar un initrd de instalación es bastante complejo. Las otras dos soluciones son mucho más comunes, especialmente debido a que los parámetros de arranque proveen otra forma de presembrar las respuestas a las primeras preguntas del proceso de instalación. La forma usual de evitar la molestia de tipear estos parámetros a mano en cada instalación es guardarlos en la configuración de <command>isolinux</command> (en el caso del CD-ROM) o <command>syslinux</command> (para la llave USB)."

msgid "Creating a Preseed File"
msgstr "Creación de un archivo de presembrado"

msgid "A preseed file is a plain text file, where each line contains the answer to one Debconf question. A line is split across four fields separated by whitespace (spaces or tabs), as in, for instance, <literal>d-i mirror/suite string stable</literal>:"
msgstr "Un archivo de presembrado es un archivo en texto plano en el que cada línea contiene la respuesta a una pregunta Debconf. Cada línea está dividida en cuatro campos separados por espacios en blancos (espacios o tabulaciones) como, por ejemplo, <literal>d-i mirror/suite string stable</literal>:"

msgid "the first field is the “owner” of the question; “d-i” is used for questions relevant to the installer, but it can also be a package name for questions coming from Debian packages;"
msgstr "el primer campo es el «dueño» de la pregunta; utilizamos «d-i» para las preguntas relevantes al instalador, pero también puede ser el nombre de un paquete para las preguntas que provengan de un paquete Debian;"

#, fuzzy
#| msgid "the second field is an identifier for the question;"
msgid "the second field is an identifier for the question (the template name);"
msgstr "el segundo campo es un identificador para la pregunta;"

msgid "third, the type of question;"
msgstr "tercero, el tipo de pregunta;"

msgid "the fourth and last field contains the value for the answer. Note that it must be separated from the third field with a single space; if there are more than one, the following space characters are considered part of the value."
msgstr "el cuarto y último campo contiene el valor de la respuesta. Tenga en cuenta que debe estar separado del tercer campo sólo por un espacio; si hay más de uno, el siguiente carácter de espacio es considerado parte del valor."

msgid "The simplest way to write a preseed file is to install a system by hand. Then <command>debconf-get-selections --installer</command> will provide the answers concerning the installer. Answers about other packages can be obtained with <command>debconf-get-selections</command>. However, a cleaner solution is to write the preseed file by hand, starting from an example and the reference documentation: with such an approach, only questions where the default answer needs to be overridden can be preseeded; using the <literal>priority=critical</literal> boot parameter will instruct Debconf to only ask critical questions, and use the default answer for others."
msgstr "La forma más simple de escribir un archivo de presembrado es instalar un sistema a mano. Luego, <command>debconf-get-selections --installer</command> proveerá las respuestas que involucran al instalador. Puede obtener las respuestas sobre otros paquetes con <command>debconf-get-selections</command>. Sin embargo, una solución más limpia es escribir el archivo de presembrado a mano, comenzando con un ejemplo y la documentación de referencia: con este enfoque, sólo necesitará presembrar las preguntas en las que desea modificar la respuesta predeterminada; utilizar el parámetro de arranque <literal>priority=critical</literal> le indicará a Debconf que sólo realice las preguntas críticas y que utilice las respuestas predeterminadas para las demás."

msgid "Pre-setting a value in a preseed file automatically instructs the Debian installer to not ask that question. This happens, because loading the preseed file does not just set the given value(s), but also marks each of the affected dialogs as “seen“ by the user. Thus it is possible to pre-set a question's value and still present the dialog to the user by resetting the “seen“ flag. Beware that order in this case matters and that the value has to be preseeded before setting the dialog to “unseen“ as shown in the following example:"
msgstr ""

msgid ""
"d-i netcfg/hostname string worker\n"
"d-i netcfg/hostname seen false"
msgstr ""

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary><command>debconf-get-selections</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "<emphasis>DOCUMENTATION</emphasis> Installation guide appendix"
msgstr "<emphasis>DOCUMENTACIÓN</emphasis> Apéndice de la guía de instalación"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>preseed</primary><secondary>all templates</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/apb\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/example-preseed.txt\" />"
msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. There are also collections of all debconf templates extracted from each component and suite of Debian: <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/apb\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/example-preseed.txt\" /> <ulink type=\"block\" url=\"https://preseed.debian.net/\" />"
msgstr "La guía de instalación, disponible en internet, incluye documentación detallada sobre el uso de un archivo de presembrado en un apéndice. También incluye un archivo de ejemplo detallado y comentado, que puede servir como base para personalizaciones locales. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/apb\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/example-preseed.txt\" />"

msgid "Preseeding an installation is often not as straightforward as one would wish. It sometimes requires to understand how packages process the given values in their scripts. Don't hesitate to ask on the <email>debian-cd@lists.debian.org</email> mailing list or in the <literal>#debian-cd</literal> IRC channel if you require help. Also be aware that some complex setups still cannot be achieved by preseeding."
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>mailing lists</primary><secondary><email>debian-cd@lists.debian.org</email></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Creating a Customized Boot Media"
msgstr "Creación de un medio de arranque personalizado"

msgid "Knowing where to store the preseed file is all very well, but the location isn't everything: one must, one way or another, alter the installation boot media to change the boot parameters and add the preseed file."
msgstr "Saber dónde almacenar el archivo de presembrado está bien, pero la ubicación no lo es todo: uno debe, de una u otra forma, alterar el medio de arranque de la instalación para modificar los parámetros de arranque y agregar el archivo de presembrado."

msgid "Booting From the Network"
msgstr "Arranque desde la red"

#, fuzzy
#| msgid "<primary>LXC</primary>"
msgid "<primary>PXE</primary>"
msgstr "<primary>LXC</primary>"

msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/ch04s05\" />"
msgstr "Cuando un equipo arranca desde la red, el servidor que envía los elementos de inicialización también define los parámetros de arranque. Por lo tanto, debe modificar la configuración de PXE en el servidor de arranque; más específicamente, en su archivo de configuración <filename>/tftpboot/pxelinux.cfg/default</filename>. Definir el arranque por red es un prerequisito; revise la guía de instalación para más detalles. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/ch04s05\" />"

msgid "Preparing a Bootable USB Key"
msgstr "Preparación de una llave USB de arranque"

#, fuzzy
#| msgid "<primary><emphasis>VirtualBox</emphasis></primary>"
msgid "<primary><filename>syslinux.cfg</filename></primary>"
msgstr "<primary><emphasis>VirtualBox</emphasis></primary>"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>syslinux</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>isolinux</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>Icinga</primary>"
msgid "<primary><filename>grub.cfg</filename></primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>:"
msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>, copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>."
msgstr "Una vez que preparó una llave de arranque (revise la <xref linkend=\"sect.install-usb\" />), necesitará unas pocas operaciones adicionales. Asumiendo que el contenido de la llave se encuentra en <filename>/media/usbdisk/</filename>:"

msgid "If you have been using a hybrid ISO image to create the bootable USB stick, then you have to edit <filename>/media/usbdisk/boot/grub/grub.cfg</filename> (for the EFI boot screen):"
msgstr ""

#, fuzzy
#| msgid "syslinux.cfg file and preseeding parameters"
msgid "boot/grub/grub.cfg file and preseeding parameters"
msgstr "Archivo syslinux.cfg y parámetros de presembrado"

#, fuzzy
#| msgid ""
#| "default vmlinuz\n"
#| "append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgid ""
"menuentry --hotkey=i 'Install' {\n"
"    set background_color=black\n"
"    linux    /install.amd/vmlinuz preseed/file=/cdrom/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 --- quiet \n"
"    initrd   /install.amd/initrd.gz\n"
"}"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

msgid "And you have to edit <filename>/media/usbdisk/isolinux/isolinux.cfg</filename> (for BIOS boot) or one of the files it utilizes - e.g. <filename>/media/usbdisk/isolinux/txt.cfg</filename> - to add required boot parameters:"
msgstr ""

#, fuzzy
#| msgid "syslinux.cfg file and preseeding parameters"
msgid "isolinux/txt.cfg file and preseeding parameters"
msgstr "Archivo syslinux.cfg y parámetros de presembrado"

#, fuzzy
#| msgid ""
#| "default vmlinuz\n"
#| "append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgid ""
"label install\n"
"        menu label ^Install\n"
"        kernel [...]\n"
"        append preseed/file=/cdrom/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=/install.amd/initrd.gz --- quiet"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

#, fuzzy
#| msgid "edit <filename>/media/usbdisk/syslinux.cfg</filename> and add required boot parameters (see example below)."
msgid "If you have been using the <filename>hd-media</filename> installer image for a custom USB stick, edit <filename>/media/usbdisk/syslinux.cfg</filename> and add the required boot parameters as shown in the example below:"
msgstr "edite <filename>/media/usbdisk/syslinux.cfg</filename> y agrege los parámetros de arranque necesarios (revise el ejemplo a continuación)."

msgid "syslinux.cfg file and preseeding parameters"
msgstr "Archivo syslinux.cfg y parámetros de presembrado"

msgid ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

msgid "Creating a CD-ROM Image"
msgstr "Creación de una imagen de CD-ROM"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">debian-cd</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>genisoimage</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>mkisofs</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>xorriso</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "A USB key is a read-write media, so it was easy for us to add a file there and change a few parameters. In the CD-ROM case, the operation is more complex, since we need to regenerate a full ISO image. This task is handled by <emphasis role=\"pkg\">debian-cd</emphasis>, but this tool is rather awkward to use: it needs a local mirror, and it requires an understanding of all the options provided by <filename>/usr/share/debian-cd/CONF.sh</filename>; even then, <command>make</command> must be invoked several times. <filename>/usr/share/debian-cd/README</filename> is therefore a very recommended read."
msgstr "Una llave USB es un medio de lectura y escritura, por lo que es sencillo agregar un archivo allí y cambiar unos pocos parámetros. En el caso de un CD-ROM, la operación es más compleja ya que necesitamos generar una imagen ISO completa. <emphasis role=\"pkg\">debian-cd</emphasis> se encarga de esto, pero es bastante extraño utilizar esta herramienta: necesita un repositorio local y requiere entender todas las opciones que provee <filename>/usr/share/debian-cd/CONF.sh</filename>; aún entonces, debe ejecutar <command>make</command> varias veces. Se recomienda leer <filename>/usr/share/debian-cd/README</filename>."

#, fuzzy
#| msgid "Having said that, debian-cd always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific file is <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgid "Having said that, <emphasis role=\"pkg\">debian-cd</emphasis> always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific files are <filename>$TDIR/$CODENAME/CD1/isolinux/isolinux.cfg</filename> and <filename>$TDIR/$CODENAME/CD1/boot/grub/grub.cfg</filename> as shown above). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgstr "Habiendo dicho esto, debian-cd siempre funciona de forma similar: genera un directorio «image» con el contenido exacto del CD-ROM y luego lo convierte en un archivo ISO con una herramienta como <command>genisoimage</command>, <command>mkisofs</command> o <command>xorriso</command>. El directorio de imagen es completado luego del paso <command>make image-trees</command> de debian-cd. En este punto, agregaremos el archivo de presembrado en el directorio apropiado (usualmente <filename>$TDIR/$CODENAME/CD1/</filename>, donde $TDIR y $CODENAME son parámetros definidos por el archivo de configuración <filename>CONF.sh</filename>). El CD-ROM utiliza <command>isolinux</command> como gestor de arranque, y debemos adaptar el archivo de configuración que generó debian-cd para poder agregar los parámetros de arranque necesarios (el archivo específico es <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Luego puede continuar el proceso «normal» y generar la imagen ISO con <command>make image CD=1</command> (o <command>make images</command> si está generando varios CD-ROMs)."

msgid "Simple-CDD: The All-In-One Solution"
msgstr "Simple-CDD: la solución todo-en-uno"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">simple-cdd</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Simply using a preseed file is not enough to fulfill all the requirements that may appear for large deployments. Even though it is possible to execute a few scripts at the end of the normal installation process, the selection of the set of packages to install is still not quite flexible (basically, only “tasks” can be selected); more important, this only allows installing official Debian packages, and precludes locally-generated ones."
msgstr "Utilizar sólamente un archivo de presembrado no es suficiente para satisfacer todos los requisitos que podrían aparecer en despliegues grandes. Aunque es posible ejecutar algunos scripts al final del proceso normal de instalación, todavía no es muy flexible la selección del conjunto de paquetes a instalar (básicamente, sólo puede seleccionar «tareas»); lo que es más importante, esto sólo permite instalar paquetes Debian oficiales y excluye aquellos generados localmente."

#, fuzzy
#| msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what Simple-CDD (in the <emphasis role=\"pkg\">simple-cdd</emphasis> package) does."
msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what <emphasis role=\"pkg\">simple-cdd</emphasis> does."
msgstr "Por el otro lado, debian-cd puede integrar paquetes externos y se puede extender debian-installer agregando nuevos pasos en el proceso de instalación. Combinando estas capacidades, debería ser posible crear un instalador completamente personalizado que satisfaga nuestras necesidades; inclusive debería poder configurar algunos servicios luego de desempaquetar los paquetes necesarios. Afortunadamente, esto no es sólo una hipótesis ya que esto es exactamente lo que hace Simple-CDD (en el paquete <emphasis role=\"pkg\">simple-cdd</emphasis>)."

#, fuzzy
#| msgid "The purpose of Simple-CDD is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgid "The purpose of this tool is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgstr "El propósito de Simple-CDD es permitir que cualquiera pueda crear fácilmente una distribución derivada de Debian seleccionando un subconjunto de los paquetes disponibles, preconfigurarlos con Debconf, agregar software específico y ejecutar scripts personalizados al final del proceso de instalación. Esto coincide con la filosofía de «sistema operativo universal» ya que cualquiera puede adaptarlo a sus necesidades."

msgid "Creating Profiles"
msgstr "Creación de perfiles"

msgid "Simple-CDD defines “profiles” that match the FAI “classes” concept, and a machine can have several profiles (determined at installation time). A profile is defined by a set of <filename>profiles/<replaceable>profile</replaceable>.*</filename> files:"
msgstr "Simple-CDD define «perfiles» que coinciden con el concepto de «clases» de FAI; una máquina puede tener varios perfiles (determinados en el momento de la instalación). Se define un perfil con un conjunto de archivos <filename>profiles/<replaceable>perfil</replaceable>.*</filename>:"

msgid "the <filename>.description</filename> file contains a one-line description for the profile;"
msgstr "el archivo <filename>.description</filename> contiene una descripción de una línea sobre el perfil;"

msgid "the <filename>.packages</filename> file lists packages that will automatically be installed if the profile is selected;"
msgstr "el archivo <filename>.packages</filename> enumera los paquetes que se instalarán automáticamente si se selecciona el perfil;"

msgid "the <filename>.downloads</filename> file lists packages that will be stored onto the installation media, but not necessarily installed;"
msgstr "el archivo <filename>.downloads</filename> enumera los paquetes que se almacenarán en el medio de instalación pero no se instalarán obligatoriamente;"

msgid "the <filename>.preseed</filename> file contains preseeding information for Debconf questions (for the installer and/or for packages);"
msgstr "el archivo <filename>.preseed</filename> contiene información de presembrado para las preguntas de Debconf (para el instalador y/o los paquetes);"

msgid "the <filename>.postinst</filename> file contains a script that will be run at the end of the installation process;"
msgstr "el archivo <filename>.postinst</filename> contiene un script que se ejecutará al final del proceso de instalación;"

#, fuzzy
#| msgid "lastly, the <filename>.conf</filename> file allows changing some Simple-CDD parameters based on the profiles to be included in an image."
msgid "lastly, the <filename>.conf</filename> file allows changing some parameters based on the profiles to be included in an image."
msgstr "finalmente, el archivo <filename>.conf</filename> permite modificar algunos parámetros de Simple-CDD basado en los perfiles incluidos en la imagen."

msgid "The <literal>default</literal> profile has a particular role, since it is always selected; it contains the bare minimum required for Simple-CDD to work. The only thing that is usually customized in this profile is the <literal>simple-cdd/profiles</literal> preseed parameter: this allows avoiding the question, introduced by Simple-CDD, about what profiles to install."
msgstr "El perfil <literal>default</literal> («predeterminado») tiene un rol particular ya que siempre está activo; contiene lo mínimo necesario para que funcione Simple-CDD. Lo único que generalmente personalizaremos en este perfile es el parámetro de presembrado <literal>simple-cdd/profiles</literal>: esto permite esquivar la pregunta sobre los perfiles a instalar que agrega Simple-CDD."

msgid "Note also that the commands will need to be invoked from the parent directory of the <filename>profiles</filename> directory."
msgstr "Sepa también que necesitará ejecutar todo desde el directorio que contenga el directorio <filename>profiles</filename>."

msgid "Configuring and Using <command>build-simple-cdd</command>"
msgstr "Configuración y uso de <command>build-simple-cdd</command>"

msgid "<primary><command>build-simple-cdd</command></primary>"
msgstr "<primary><command>build-simple-cdd</command></primary>"

msgid "<emphasis>QUICK LOOK</emphasis> Detailed configuration file"
msgstr "<emphasis>VISTA RÁPIDA</emphasis> Archivo de configuración detallado"

#, fuzzy
#| msgid "An example of a Simple-CDD configuration file, with all possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). This can be used as a starting point when creating a custom configuration file."
msgid "An example of a Simple-CDD configuration file, with most possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed</filename>). This can be used as a starting point when creating a custom configuration file. Unfortunately not everything is documented there, so some variables are only listed and explained in <filename>/usr/lib/python3/dist-packages/simple_cdd/variables.py</filename>."
msgstr "El paquete incluye un ejemplo de archivo de configuración de Simple-CDD con todos los parámetros posibles (<filename>/usr/share/docs/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). Puede utilizarlo como punto de partida cuando cree un archivo de configuración personalizado."

msgid "It is further important to familiarize yourself with the variables understood by <filename>/usr/share/debian-cd/CONF.sh</filename>."
msgstr ""

msgid "Simple-CDD requires many parameters to operate fully. They will most often be gathered in a configuration file, which <command>build-simple-cdd</command> can be pointed at with the <literal>--conf</literal> option, but they can also be specified via dedicated parameters given to <command>build-simple-cdd</command>. Here is an overview of how this command behaves, and how its parameters are used:"
msgstr "Simple-CDD necesita muchos parámetros para todo su funcionamiento. En la mayoría de los casos los obtendrá de un archivo de configuración al que podemos apuntar con la opción <literal>--conf</literal> de <command>build-simple-cdd</command>, pero también podemos especificarlos como parámetros específicos al ejecutar <command>build-simple-cdd</command>. Aquí hay una vista rápida sobre cómo funciona este programa y cómo utilizar sus parámetros:"

msgid "the <literal>profiles</literal> parameter lists the profiles that will be included on the generated CD-ROM image;"
msgstr "el parámetro <literal>profiles</literal> enumera los perfiles que se incluirán en la imagen de CD-ROM generada;"

msgid "based on the list of required packages, Simple-CDD downloads the appropriate files from the server mentioned in <literal>server</literal>, and gathers them into a partial mirror (which will later be given to debian-cd);"
msgstr "basado en la lista de paquetes necesarios, Simple-CDD descarga los archivos necesarios desde el servidor mencionado en <literal>server</literal> y los reúne en un repositorio parcial (que luego le proveerá a debian-cd);"

msgid "the custom packages mentioned in <literal>local_packages</literal> are also integrated into this local mirror;"
msgstr "también se integrarán a este repositorio local los paquetes personalizados mencionados en <literal>local_packages</literal>;"

msgid "debian-cd is then executed (within a default location that can be configured with the <literal>debian_cd_dir</literal> variable), with the list of packages to integrate;"
msgstr "luego ejecutará debian-cd (con una ubicación predeterminada que puede configurar con la variable <literal>debian_cd_dir</literal>) con la lista de paquetes a integrar;"

msgid "once debian-cd has prepared its directory, Simple-CDD applies some changes to this directory:"
msgstr "una vez que debian-cd preparó este directorio, Simple-CDD realiza algunos cambios al mismo:"

msgid "files containing the profiles are added in a <filename>simple-cdd</filename> subdirectory (that will end up on the CD-ROM);"
msgstr "agrega los archivos que contienen los perfiles en un subdirectorio <filename>simple-cdd</filename> (que serán incluidos en el CD-ROM);"

msgid "other files listed in the <literal>all_extras</literal> parameter are also added;"
msgstr "también se agregarán los demás archivos enumerados en el parámetro <literal>all_extras</literal>;"

msgid "the boot parameters are adjusted so as to enable the preseeding. Questions concerning language and country can be avoided if the required information is stored in the <literal>language</literal> and <literal>country</literal> variables."
msgstr "ajustará los parámetros de arranque para permitir presembrado. Puede evitar las preguntas sobre idioma y país si almacena la información necesaria en las variables <literal>language</literal> y <literal>country</literal>."

msgid "debian-cd then generates the final ISO image."
msgstr "luego debian-cd genera la imagen ISO final."

msgid "Generating an ISO Image"
msgstr "Generación de una imagen ISO"

#, fuzzy
#| msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-10-amd64-CD-1.iso</filename>."
msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-11-amd64-CD-1.iso</filename>."
msgstr "Una vez que escribimos un archivo de configuración y definimos nuestros perfiles, el paso restante es ejecutar <command>build-simple-cdd --conf simple-cdd.conf</command>. Luego de unos minutos tendremos la imagen necesaria en <filename>images/debian-10-amd64-CD-1.iso</filename>."

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>monitoring</primary>"
msgstr "<primary>Munin</primary>"

msgid "<primary>Munin</primary>"
msgstr "<primary>Munin</primary>"

msgid "<primary>Nagios</primary>"
msgstr "<primary>Nagios</primary>"

msgid "Monitoring is a generic term, and the various involved activities have several goals: on the one hand, following usage of the resources provided by a machine allows anticipating saturation and the subsequent required upgrades; on the other hand, alerting the administrator as soon as a service is unavailable or not working properly means that the problems that do happen can be fixed sooner."
msgstr "La monitorización es un término genérico, y las muchas actividades involucradas tiene varias objetivos: por un lado, seguir el uso de recursos provistos por una máquina permite anticipar saturación y la actualización necesaria que le seguirá; por el otro, alertar a los administradores tan pronto como un servicio no esté disponible o no fucione correctamente significa que se podrán solucionar más rápidamente aquellos problemas que sucedan."

msgid "<emphasis>Munin</emphasis> covers the first area, by displaying graphical charts for historical values of a number of parameters (used RAM, occupied disk space, processor load, network traffic, Apache/MySQL load, and so on). <emphasis>Nagios</emphasis> covers the second area, by regularly checking that the services are working and available, and sending alerts through the appropriate channels (e-mails, text messages, and so on). Both have a modular design, which makes it easy to create new plug-ins to monitor specific parameters or services."
msgstr "<emphasis>Munin</emphasis> cubre la primera área mostrando gráficos de los valores históricos de una cantidad de parámetros (RAM utilizada, espacio ocupado en disco, carga en el procesador, tráfico de red, carga de Apache/MySQL, etc.). <emphasis>Nagios</emphasis> cubre la segunda área, revisando regularmente que los servicios estén funcionando y disponibles, enviando alertas a través de los canales apropiados (correo, mensajes de texto, etc.). Ambos tienen un diseño modular, lo que permite crear nuevos plugins para monitorizar parámetros o servicios específicos."

msgid "<emphasis>ALTERNATIVE</emphasis> Zabbix, an integrated monitoring tool"
msgstr "<emphasis>ALTERNATIVA</emphasis> Zabbix, una herramienta de monitorización integrada"

msgid "<primary>Zabbix</primary>"
msgstr "<primary>Zabbix</primary>"

#, fuzzy
#| msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"https://www.zabbix.com/\" />"
msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"https://zabbix.com/\" />"
msgstr "Si bien Munin y Nagios son comunes, no son los únicos jugadores en el campo de la monitorización, y cada uno de ellos gestiona la mitad de la tarea (gráficos por un lado, alertas por otro). Zabbix, por su parte, integra ambas partes de la monitorización; también tiene una interfaz web para configurar los aspectos más comunes. Creció enormemente en los últimos años y ahora se le puede considerar un contendiente viable. En el servidor de monitorización se instalaría <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (o <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), y probablemente también <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> para disponer de una interfaz web. En las máquinas a monitorizar se instalaría <emphasis role=\"pkg\">zabbix-agent</emphasis> que enviaría los datos al servidor. <ulink type=\"block\" url=\"https://www.zabbix.com/\" />"

msgid "<emphasis>ALTERNATIVE</emphasis> Icinga, a Nagios fork"
msgstr "<emphasis>ALTERNATIVA</emphasis> Icinga, una bifurcación de Nagios"

msgid "<primary>Icinga</primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"https://www.icinga.org/\" />"
msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"https://icinga.com/\" />"
msgstr "Debido a divergencias en opiniones sobre el modelo de desarrollo de Nagios (que es controlado por una empresa), una cantidad de desarrolladores bifurcaron Nagios y utilizaron Icinga como su nuevo nombre. Icinga todavía es compatible — hasta ahora — con los plugins y configuraciones de Nagios, pero también agrega funcionalidad adicional. <ulink type=\"block\" url=\"https://www.icinga.org/\" />"

msgid "Setting Up Munin"
msgstr "Configuración de Munin"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary>grapher</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The purpose of Munin is to monitor many machines; therefore, it quite naturally uses a client/server architecture. The central host — the grapher — collects data from all the monitored hosts, and generates historical graphs."
msgstr "El propósito de Munin es monitorizar muchas máquinas; por lo tanto, naturalmente utiliza una arquitectura cliente/servidor. El equipo central — el graficador — recolecta datos de todos los equipos monitorizados y genera gráficos históricos."

msgid "Configuring Hosts To Monitor"
msgstr "Configuración de los equipos a monitorizar"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>server</primary><secondary>munin-node</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The first step is to install the <emphasis role=\"pkg\">munin-node</emphasis> package. The daemon installed by this package listens on port 4949 and sends back the data collected by all the active plugins. Each plugin is a simple program returning a description of the collected data as well as the latest measured value. Plugins are stored in <filename>/usr/share/munin/plugins/</filename>, but only those with a symbolic link in <filename>/etc/munin/plugins/</filename> are really used."
msgstr "El primer paso es instalar el paquete <emphasis role=\"pkg\">munin-node</emphasis>. El demonio que instala este paquete escucha en el puerto 4949 y envía los datos recolectados por todos los plugins activos. Cada plugin es un programa simple que devuelve una descripción de los datos recolectados y el último valor medido. Los plugins se almacenan en <filename>/usr/share/munin/plugins/</filename>, pero realmente sólo se utilizan aquellos con un enlace simbólico en <filename>/etc/munin/plugins/</filename>."

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/munin/</filename></secondary><see>Munin</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/plugins/</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary>plugins</secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this auto-configuration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the Plugin Gallery can be interesting even though not all plugins have comprehensive documentation. <ulink type=\"block\" url=\"https://gallery.munin-monitoring.org\" />"
msgstr ""

#, fuzzy
#| msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the Plugin Gallery<footnote><para><ulink type=\"block\" url=\"http://gallery.munin-monitoring.org\" /></para></footnote> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgid "However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgstr "Cuando instala el paquete, se determina un conjunto de plugins activos basados en el software disponible y la configuración actual del equipo. Sin embargo, esta configuración automática depende de una funcionalidad que debe proveer cada plugin, y generalmente es buena idea revisar y afinar el resultado a mano. Puede resultar interesante visitar la Galería de Plugins<footnote><para><ulink type=\"block\" url=\"http://gallery.munin-monitoring.org\" /></para></footnote>, incluso aunque no todos los plugins dipongan de documentación exhaustiva. Sin embargo, todos los plugins son scripts y la mayoría son bastante simples y están bien comentados. Revisar <filename>/etc/munin/plugins/</filename> es, por lo tanto, una buena forma de tener una idea de lo que hace cada plugin y determinar si alguno debe eliminarlo. Similarmente, activar un plugin interesante que se encuentre en <filename>/usr/share/munin/plugins/</filename> es simplemente cuestión de crear un enlace simbólico con <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Sepa que cuando el nombre de un plugin finaliza con un guión bajo «_», el plugin necesita un parámetro. Debe almacenar este parámetro en el nombre del enlace simbólico; por ejemplo, el plugin «if_» debe activarse con un enlace simbólico llamado <filename>if_eth0</filename> para monitorizar el tráfico de red en la interfaz eth0."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/munin-node.conf</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>munin-node.service</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>systemctl restart munin-node</command>."
msgstr "Una vez que configuró correctamente los plugins, debe actualizar el demonio de configuración para describir el control de acceso de los datos recolectados. Esto involucra directivas <literal>allow</literal> en el archivo <filename>/etc/munin/munin-node.conf</filename>. La configuración predeterminada es <literal>allow^127\\.0\\.0\\.1$</literal>, lo que sólo permite el acceso al equipo local. Un administrador usualmente agregará una línea similar que contenga la dirección IP del equipo graficador y luego reiniciará el demonio con <command>systemctl restart munin-node</command>."

msgid "<emphasis>GOING FURTHER</emphasis> Creating local plugins"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> Creación de plugins locales"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><command>munin-run</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://guide.munin-monitoring.org/en/latest/plugin/writing.html\" />"
msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"https://guide.munin-monitoring.org/en/latest/plugin/writing.html\" />"
msgstr "Munin incluye documentación detallada sobre cómo se deben comportar los plugins y cómo desarrollar plugins nuevos. <ulink type=\"block\" url=\"http://guide.munin-monitoring.org/en/latest/plugin/writing.html\" />"

msgid "A plugin is best tested when run in the same conditions as it would be when triggered by munin-node; this can be simulated by running <command>munin-run <replaceable>plugin</replaceable></command> as root. A potential second parameter given to this command (such as <literal>config</literal>) is passed to the plugin as a parameter."
msgstr "La mejor forma de probar un plugin es ejecutarlo en las mismas condiciones que lo haría munin-node; puede simularlo ejecutando <command>munin-run <replaceable>plugin</replaceable></command> como root. Puede proveer un posible segundo parámetro a este programa (como <literal>config</literal>) que será provisto como parámetro al plugin."

msgid "When a plugin is invoked with the <literal>config</literal> parameter, it must describe itself by returning a set of fields:"
msgstr "Cuando ejecuta un plugin con el parámetro <literal>config</literal>, debe describirse a sí mismo devolviendo un conjunto de campos:"

#, fuzzy
#| msgid ""
#| "<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
#| "</userinput><computeroutput>graph_title Load average\n"
#| "graph_args --base 1000 -l 0\n"
#| "graph_vlabel load\n"
#| "graph_scale no\n"
#| "graph_category system\n"
#| "load.label load\n"
#| "graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
#| "load.info 5 minute load average\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"

msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"https://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgstr "La especificación de la guía de referencia de plugins, disponible como parte de la guía de Munin, describe los varios campos disponibles. <ulink type=\"block\" url=\"https://munin.readthedocs.org/en/latest/reference/plugin.html\" />"

msgid "When invoked without a parameter, the plugin simply returns the last measured values; for instance, executing <command>sudo munin-run load</command> could return <literal>load.value 0.12</literal>."
msgstr "Cuando lo ejecuta sin parámetros, un plugin simplemente devuelve el último valor medido; por ejemplo, ejecutar <command>sudo munin-run load</command> podría devolver <literal>load.value 0.12</literal>."

msgid "Finally, when a plugin is invoked with the <literal>autoconf</literal> parameter, it should return “yes” (and a 0 exit status) or “no” (with a 1 exit status) according to whether the plugin should be enabled on this host."
msgstr "Finalmente, cuando ejecute un plugin con el parámetro <literal>autoconf</literal>, debería devolver «yes» (y un código de salida 0) o «no» (con un código de salida 1) según si el plugin debería estar activado en este equipo o no."

msgid "Configuring the Grapher"
msgstr "Configuración del graficador"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><command>munin-cron</command></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/munin.conf</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The “grapher” is simply the computer that aggregates the data and generates the corresponding graphs. The required software is in the <emphasis role=\"pkg\">munin</emphasis> package. The standard configuration runs <command>munin-cron</command> (once every 5 minutes), which gathers data from all the hosts listed in <filename>/etc/munin/munin.conf</filename> (only the local host is listed by default), saves the historical data in RRD files (<emphasis>Round Robin Database</emphasis>, a file format designed to store data varying in time) stored under <filename>/var/lib/munin/</filename> and generates an HTML page with the graphs in <filename>/var/cache/munin/www/</filename>."
msgstr "El «graficador» es simplemente el equipo que agrupa los datos y genera los gráficos correspondientes. El software necesario se encuentra en el paquete <emphasis role=\"pkg\">munin</emphasis>. La configuración estándar ejecuta <command>munin-cron</command> (una vez cada 5 minutos), mediante el que obtiene datos de todos los equipos enumerados en <filename>/etc/munin/munin.conf</filename> (de forma predeterminada sólo incluye al equipo local), guarda los datos históricos en archivos RRD (<emphasis>base de datos Round Robin</emphasis>: «Round Robin Database», un formato de archivo diseñado para almacenar datos que varían en el tiempo) almacenados en <filename>/var/lib/munin/</filename> y genera una página HTML con los gráficos en <filename>/var/cache/munin/www/</filename>."

msgid "All monitored machines must therefore be listed in the <filename>/etc/munin/munin.conf</filename> configuration file. Each machine is listed as a full section with a name matching the machine and at least an <literal>address</literal> entry giving the corresponding IP address."
msgstr "Por lo tanto, debe enumerar todas las máquinas monitorizadas en el archivo de configuración <filename>/etc/munin/munin.conf</filename>. Cada máquina es enumerada como una sección completa con el nombre que coincide con el equipo y al menos un elemento <literal>address</literal> que provee la dirección IP correspondiente."

msgid ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"
msgstr ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"

msgid "Sections can be more complex, and describe extra graphs that could be created by combining data coming from several machines. The samples provided in the configuration file are good starting points for customization."
msgstr "Las secciones pueden ser más complejas y describir gráficos adicionales que puede crear combinando datos de varias máquinas. Los ejemplos que provee el archivo de configuración son buenos puntos de partida para personalizar."

msgid "The last step is to publish the generated pages; this involves configuring a web server so that the contents of <filename>/var/cache/munin/www/</filename> are made available on a website. Access to this website will often be restricted, using either an authentication mechanism or IP-based access control. See <xref linkend=\"sect.http-web-server\" /> for the relevant details."
msgstr "El último paso es publicar las páginas generadas; esto involucra configurar un servidor web para que el contenido de <filename>/var/cache/munin/www/</filename> esté disponible en un sitio web. Generalmente restringirá el acceso a este sitio web, ya sea con un mecanismo de autenticación o un control de acceso basado en IP. Revise la <xref linkend=\"sect.http-web-server\" /> para los detalles relevantes."

msgid "Setting Up Nagios"
msgstr "Configuración de Nagios"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">nagios4</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">monitoring-plugins</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

msgid "Unlike Munin, Nagios does not necessarily require installing anything on the monitored hosts; most of the time, Nagios is used to check the availability of network services. For instance, Nagios can connect to a web server and check that a given web page can be obtained within a given time."
msgstr "A diferencia de Munin, Nagios no necesita instalar algo en los equipos monitorizados; la mayoría de las veces, se utiliza Nagios para revisar la disponibilidad de servicios de red. Por ejemplo, Nagios puede conectarse a un servidor web y revisar si puede obtener una página web dada en un tiempo especificado."

msgid "Installing"
msgstr "Instalación"

msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios4</emphasis> and <emphasis role=\"pkg\">monitoring-plugins</emphasis> packages. Installing the packages configures the web interface and the Apache server. The <literal>authz_groupfile</literal> and <literal>auth_digest</literal> Apache modules must be enabled, for that execute:"
msgstr "El primer paso para configurar Nagios es instalar los paquetes <emphasis role=\"pkg\">nagios4</emphasis> y <emphasis role=\"pkg\">monitoring-plugins</emphasis>. Al instalar los paquetes se configura la interfaz web y el servidor Apache. Los módulos de Apache <literal>authz_groupfile</literal> y <literal>auth_digest</literal> deben estar habilitados, para ello ejecute:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>a2enmod authz_groupfile</userinput>\n"
#| "<computeroutput>Considering dependency authz_core for authz_groupfile:\n"
#| "Module authz_core already enabled\n"
#| "Enabling module authz_groupfile.\n"
#| "To activate the new configuration, you need to run:\n"
#| "  systemctl restart apache2\n"
#| "# </computeroutput><userinput>a2enmod auth_digest\n"
#| "Considering dependency authn_core for auth_digest:\n"
#| "Module authn_core already enabled\n"
#| "Enabling module auth_digest.\n"
#| "To activate the new configuration, you need to run:\n"
#| "  systemctl restart apache2\n"
#| "</userinput><computeroutput># </computeroutput><userinput>systemctl restart apache2\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>a2enmod authz_groupfile\n"
"</userinput><computeroutput>Considering dependency authz_core for authz_groupfile:\n"
"Module authz_core already enabled\n"
"Module authz_core already enabled\n"
"Enabling module authz_groupfile.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>a2enmod auth_digest\n"
"</userinput><computeroutput>Considering dependency authn_core for auth_digest:\n"
"Module authn_core already enabled\n"
"Enabling module auth_digest.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>systemctl restart apache2\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>a2enmod authz_groupfile</userinput>\n"
"<computeroutput>Considering dependency authz_core for authz_groupfile:\n"
"Module authz_core already enabled\n"
"Enabling module authz_groupfile.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>a2enmod auth_digest\n"
"Considering dependency authn_core for auth_digest:\n"
"Module authn_core already enabled\n"
"Enabling module auth_digest.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl restart apache2\n"
"</userinput>"

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/nagios4/</filename></secondary><see>Nagios</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/hdigest.users</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Adding other users is a simple matter of inserting them in the <filename>/etc/nagios4/hdigest.users</filename> file."
msgstr "Añadir otros usuarios es tan fácil como incluirlos en el archivo <filename>/etc/nagios4/hdigest.users</filename>."

msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios4/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgstr "Apuntar un navegador a <literal>http://<replaceable>servidor</replaceable>/nagios4/</literal> mostrará la interfaz web; en particular verá que Nagios ya monitoriza algunos parámetros de la máquina en la que ejecuta. Sin embargo, algunas características interactivas como agregar comentarios a los equipos no funcionarán. Estas características están desactivadas en la configuración predeterminada de Nagios, la cual es muy restrictiva por cuestiones de seguridad."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/nagios.cfg</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "Enabling some features involves editing <filename>/etc/nagios4/nagios.cfg</filename>. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgstr "Para activar algunas funcionalidades deberemos editar el archivo <filename>/etc/nagios4/nagios.cfg</filename>. También necesitaremos configurar permisos de escritura al directorio que utiliza Nagios, ejecutando algo similar a:"

msgid ""
"<computeroutput># </computeroutput><userinput>systemctl stop nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios4/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl start nagios4\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>systemctl stop nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios4/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl start nagios4\n"
"</userinput>"

msgid "Configuring"
msgstr "Configuración"

msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios4/nagios.cfg</filename>."
msgstr "La interfaz web de Nagios es bastante agradable, pero no permite configuración ni puede utilizarla para agregar equipos o servicios a monitorizar. Se administra toda la configuración a través de archivos referenciados en el archivo de configuración central, <filename>/etc/nagios4/nagios.cfg</filename>."

msgid "These files should not be dived into without some understanding of the Nagios concepts. The configuration lists objects of the following types:"
msgstr "No debe adentrarse en estos archivos sin entender algunos conceptos de Nagios. La configuración enumera objetos de los siguientes tipos:"

msgid "a <emphasis>host</emphasis> is a machine to be monitored;"
msgstr "a «<emphasis>host</emphasis>» es una máquina a monitorizar;"

msgid "a <emphasis>hostgroup</emphasis> is a set of hosts that should be grouped together for display, or to factor some common configuration elements;"
msgstr "un «<emphasis>hostgroup</emphasis>» es un conjunto de equipos que deben ser agrupados para visualización o para abstraer algunos elementos de configuración en común;"

msgid "a <emphasis>service</emphasis> is a testable element related to a host or a host group. It will most often be a check for a network service, but it can also involve checking that some parameters are within an acceptable range (for instance, free disk space or processor load);"
msgstr "un «<emphasis>service</emphasis>» es un elemento a probar relacionado a un equipo o grupo. La mayoría de las veces será un chequeo de un servicio de red, pero también puede incluir revisar que algunos parámetros están dentro de un rango aceptable (por ejemplo, espacio libre en el disco o carga del procesador);"

msgid "a <emphasis>servicegroup</emphasis> is a set of services that should be grouped together for display;"
msgstr "un «<emphasis>servicegroup</emphasis>» es un conjunto de servicios que deben ser agrupados para visualización;"

msgid "a <emphasis>contact</emphasis> is a person who can receive alerts;"
msgstr "un «<emphasis>contact</emphasis>» es una persona que puede recibir alertas;"

msgid "a <emphasis>contactgroup</emphasis> is a set of such contacts;"
msgstr "un «<emphasis>contactgroup</emphasis>» es un conjunto de contactos;"

msgid "a <emphasis>timeperiod</emphasis> is a range of time during which some services have to be checked;"
msgstr "un «<emphasis>timeperiod</emphasis>» es un rango de tiempo durante el que se deben revisar algunos servicios;"

msgid "a <emphasis>command</emphasis> is the command line invoked to check a given service."
msgstr "un «<emphasis>command</emphasis>» es la línea de órdenes ejecutada para revisar un servicio dado."

msgid "According to its type, each object has a number of properties that can be customized. A full list would be too long to include, but the most important properties are the relations between the objects."
msgstr "Según su tipo, cada objeto tiene una cantidad de propiedades que podemos personalizar. Una lista completa sería demasiado extensa, pero las propiedades más importantes son las relaciones entre objetos."

msgid "A <emphasis>service</emphasis> uses a <emphasis>command</emphasis> to check the state of a feature on a <emphasis>host</emphasis> (or a <emphasis>hostgroup</emphasis>) within a <emphasis>timeperiod</emphasis>. In case of a problem, Nagios sends an alert to all members of the <emphasis>contactgroup</emphasis> linked to the service. Each member is sent the alert according to the channel described in the matching <emphasis>contact</emphasis> object."
msgstr "Un «<emphasis>service</emphasis>» utiliza un «<emphasis>command</emphasis>» para revisar el estado de una característica en un «<emphasis>host</emphasis>» (o «<emphasis>hostgroup</emphasis>») durante un «<emphasis>timeperiod</emphasis>». En caso de un problema, Nagios envía una alerta a todos los miembros de un «<emphasis>contactgroup</emphasis>» relacionado con el servicio. Se envía la alerta a cada miembro según el canal descripto en el objeto «<emphasis>contact</emphasis>» asociado."

msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios4/conf.d/</filename> are a good source of information on how they work."
msgstr "Un sistema de herencia permite compartir fácilmente un conjunto de propiedades entre varios objetos sin duplicar información. Lo que es más, la configuración inicial incluye algunos objetos estándar; en muchos casos, definir nuevos equipos, servicios y contactos es tan simple como derivar de los objetos genéricos proporcionados. Los archivos en <filename>/etc/nagios4/conf.d/</filename> son una buena fuente de información sobre cómo funcionan."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/conf.d/</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

msgid "The Falcot Corp administrators use the following configuration:"
msgstr "Los administradores de Falcot Corp utilizan la siguiente configuración:"

msgid "<filename>/etc/nagios4/conf.d/falcot.cfg</filename> file"
msgstr "Archivo <filename>/etc/nagios4/conf.d/falcot.cfg</filename>"

#, fuzzy
#| msgid ""
#| "define contact{\n"
#| "    name                            generic-contact\n"
#| "    service_notification_period     24x7\n"
#| "    host_notification_period        24x7\n"
#| "    service_notification_options    w,u,c,r\n"
#| "    host_notification_options       d,u,r\n"
#| "    service_notification_commands   notify-service-by-email\n"
#| "    host_notification_commands      notify-host-by-email\n"
#| "    register                        0 ; Template only\n"
#| "}\n"
#| "define contact{\n"
#| "    use             generic-contact\n"
#| "    contact_name    rhertzog\n"
#| "    alias           Raphael Hertzog\n"
#| "    email           hertzog@debian.org\n"
#| "}\n"
#| "define contact{\n"
#| "    use             generic-contact\n"
#| "    contact_name    rmas\n"
#| "    alias           Roland Mas\n"
#| "    email           lolando@debian.org\n"
#| "}\n"
#| "\n"
#| "define contactgroup{\n"
#| "    contactgroup_name     falcot-admins\n"
#| "    alias                 Falcot Administrators\n"
#| "    members               rhertzog,rmas\n"
#| "}\n"
#| "\n"
#| "define host{\n"
#| "    use                   generic-host ; Name of host template to use\n"
#| "    host_name             www-host\n"
#| "    alias                 www.falcot.com\n"
#| "    address               192.168.0.5\n"
#| "    contact_groups        falcot-admins\n"
#| "    hostgroups            debian-servers,ssh-servers\n"
#| "}\n"
#| "define host{\n"
#| "    use                   generic-host ; Name of host template to use\n"
#| "    host_name             ftp-host\n"
#| "    alias                 ftp.falcot.com\n"
#| "    address               192.168.0.6\n"
#| "    contact_groups        falcot-admins\n"
#| "    hostgroups            debian-servers,ssh-servers\n"
#| "}\n"
#| "\n"
#| "# 'check_ftp' command with custom parameters\n"
#| "define command{\n"
#| "    command_name          check_ftp2\n"
#| "    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
#| "}\n"
#| "\n"
#| "# Generic Falcot service\n"
#| "define service{\n"
#| "    name                  falcot-service\n"
#| "    use                   generic-service\n"
#| "    contact_groups        falcot-admins\n"
#| "    register              0\n"
#| "}\n"
#| "\n"
#| "# Services to check on www-host\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   HTTP\n"
#| "    check_command         check_http\n"
#| "}\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   HTTPS\n"
#| "    check_command         check_https\n"
#| "}\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   SMTP\n"
#| "    check_command         check_smtp\n"
#| "}\n"
#| "\n"
#| "# Services to check on ftp-host\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             ftp-host\n"
#| "    service_description   FTP\n"
#| "    check_command         check_ftp2\n"
#| "}"
msgid ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.12\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"
msgstr ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Sólo plantilla\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Nombre de la plantilla de host a utilizar\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Nombre de la plantilla de host a utilizar\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# orden 'check_ftp' con parámetros personalizados\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Servicio genérico de Falcot\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Servicios a chequear en www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Servicios a chequear en ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"

msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios4/conf.d/services_nagios2.cfg</filename>."
msgstr "Este archivo de configuración describe dos equipos monitorizados. El primero es el servidor web, y se realizan chequeos en los puertos HTTP (80) y HTTP seguro (443). Nagios también revisa que el servidor SMTP ejecute en el puerto 25. El segundo equipo es el servidor FTP y el chequeo incluye asegurarse que responda en menos de 20 segundos. Más allá de esta demora, se generará un «<emphasis>warning</emphasis>» («precaución»); más de 30 segundos generará una alerta crítica. La interfaz web también muestra que se monitoriza el servicio SSH: esto proviene de los equipos que pertenecen al «hostgroup» <literal>ssh-servers</literal>. El servicio estándar asociado está definido en <filename>/etc/nagios4/conf.d/services_nagios2.cfg</filename>."

msgid "Note the use of inheritance: an object is made to inherit from another object with the “use <replaceable>parent-name</replaceable>”. The parent object must be identifiable, which requires giving it a “name <replaceable>identifier</replaceable>” property. If the parent object is not meant to be a real object, but only to serve as a parent, giving it a “register 0” property tells Nagios not to consider it, and therefore to ignore the lack of some parameters that would otherwise be required."
msgstr "Verá cómo utilizamos herencia: un objeto hereda de otro objeto con la propiedad «use <replaceable>nombre-padre</replaceable>». Debemos poder identificar al objeto padre, lo que requiere incluir en él una propiedad «name <replaceable>identificador</replaceable>». Si no deseamos que el objeto padre sea un objeto real, sino que sólo sirva como padre, agregar una propiedad «register 0» le indica a Nagios que no lo considere y, por lo tanto, ignore la falta de algunos parámetros que serían obligatorios."

msgid "<emphasis>DOCUMENTATION</emphasis> List of object properties"
msgstr "<emphasis>DOCUMENTACIÓN</emphasis> Lista de propiedades de objetos"

#, fuzzy
#| msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation hosted on <ulink url=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/index.html\" />. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation hosted on <ulink url=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/\" />. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgstr "Puede obtener información más detallada sobre las muchas formas en las que puede configurar Nagios en la documentación alojada en <ulink url=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/index.html\" />. Esta incluye una lista de todos los tipos de objetos así como también las propiedades que pueden tener. También explica cómo crear nuevos plugins."

msgid "<emphasis>GOING FURTHER</emphasis> Remote tests with NRPE"
msgstr "<emphasis>YENDO MÁS ALLÁ</emphasis> Pruebas remotas con NRPE"

msgid "<primary>Nagios Remote Plugin Executor</primary><see>NRPE</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>NRPE</primary>"
msgstr "<primary>RAID</primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary>Nagios</primary><secondary><emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary>Nagios</primary><secondary><emphasis role=\"pkg\">nagios-nrpe-server</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios/nrpe.cfg</filename></secondary>"
msgstr "<primary>instalación</primary><secondary>automatizada</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>check_nrpe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <emphasis>check_nrpe</emphasis> command."
msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <command>check_nrpe</command> command."
msgstr "Muchos plugins de Nagios permiten chequear parámetros locales de un equipo; si muchas máquinas necesitan estos chequeos para que los recolecte una instalación central, necesita desplegar el plugin NRPE (<emphasis>ejecución remota de plugins de Nagios</emphasis>: «Nagios Remote Plugin Executor»). Necesitará instalar el paquete <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> en el servidor Nagios y el paquete <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> en los equipos sobre los que ejecutará los tests locales. Este último obtendrá su configuración del archivo <filename>/etc/nagios/nrpe.cfg</filename>. Este archivo debe enumerar las pruebas que puede iniciarse remotamente y las direcciones IP de las máquinas que puede ejecutarlas. Del lado de Nagios, activar estas pruebas remotas es tan simple como agregar los servicios apropiados utilizando el nuevo «command» <emphasis>check_nrpe</emphasis>."

#~ msgid "<primary><emphasis>VMWare</emphasis></primary>"
#~ msgstr "<primary><emphasis>VMWare</emphasis></primary>"

#~ msgid "<primary><emphasis>Bochs</emphasis></primary>"
#~ msgstr "<primary><emphasis>Bochs</emphasis></primary>"

#~ msgid "<primary><emphasis>QEMU</emphasis></primary>"
#~ msgstr "<primary><emphasis>QEMU</emphasis></primary>"

#~ msgid "<primary><emphasis>KVM</emphasis></primary>"
#~ msgstr "<primary><emphasis>KVM</emphasis></primary>"

#~ msgid "<primary><emphasis>LXC</emphasis></primary>"
#~ msgstr "<primary><emphasis>LXC</emphasis></primary>"

#~ msgid "<emphasis>DOCUMENTATION</emphasis> <command>xl</command> options"
#~ msgstr "<emphasis>DOCUMENTACIÓN</emphasis> Opciones de <command>xl</command>"

#~ msgid ""
#~ "#auto eth0\n"
#~ "#iface eth0 inet dhcp\n"
#~ "\n"
#~ "auto br0\n"
#~ "iface br0 inet dhcp\n"
#~ "  bridge-ports eth0"
#~ msgstr ""
#~ "#auto eth0\n"
#~ "#iface eth0 inet dhcp\n"
#~ "\n"
#~ "auto br0\n"
#~ "iface br0 inet dhcp\n"
#~ "  bridge-ports eth0"

#~ msgid "The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<filename>/var/lib/lxc/testlxc/config</filename>) and add a few <literal>lxc.network.*</literal> entries:"
#~ msgstr "El sistema de archivos recientemente creado ahora contiene un sistema Debian mínimo y, de forma predeterminada, el contenedor no tendrá interfaz de red (con el permiso de la interfaz local de loopback). Debido a que esta no es la configuración deseada, editaremos el archivo de configuración del contenedor (<filename>/var/lib/lxc/testlxc/config</filename>) y agregar algunos elementos <literal>lxc.network.*</literal>:"

#~ msgid "copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>"
#~ msgstr "copie el archivo de presembrado a <filename>/media/usbdisk/preseed.cfg</filename>"

#~ msgid "<primary>simple-cdd</primary>"
#~ msgstr "<primary>simple-cdd</primary>"

#~ msgid ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"
#~ msgstr ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"

#~ msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> and <emphasis role=\"pkg\">nagios3-doc</emphasis> packages. Installing the packages configures the web interface and creates a first <literal>nagiosadmin</literal> user (for which it asks for a password). Adding other users is a simple matter of inserting them in the <filename>/etc/nagios3/htpasswd.users</filename> file with Apache's <command>htpasswd</command> command. If no Debconf question was displayed during installation, <command>dpkg-reconfigure nagios3-cgi</command> can be used to define the <literal>nagiosadmin</literal> password."
#~ msgstr "El primer paso para configurar Nagios es instalar los paquetes <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> y <emphasis role=\"pkg\">nagios3-doc</emphasis>. La instalación de estos paquetes configurará la interfaz web y creará un primer usuario <literal>nagiosadmin</literal> (para el que pedirá una contraseña). Agregar otros usuarios es tan simple como agregarlos al archivo <filename>/etc/nagios3/htpasswd.users</filename> con el programa <command>htpasswd</command> de Apache. Si no se muestra ninguna pregunta Debconf durante su instalación, puede utilizar <command>dpkg-reconfigure nagios3-cgi</command> para definir la contraseña de <literal>nagiosadmin</literal>."

#~ msgid "the <emphasis role=\"distribution\">Jessie</emphasis> standard kernel does not allow limiting the amount of memory available to a container; the feature exists, and is built in the kernel, but it is disabled by default because it has a (slight) cost on overall system performance; however, enabling it is a simple matter of setting the <command>cgroup_enable=memory</command> kernel command-line option at boot time;"
#~ msgstr "el núcleo estándar en <emphasis role=\"distribution\">Jessie</emphasis> no permite limitar la cantidad de memoria disponible en un contenedor; la funcionalidad existe y está incorporada al núcleo pero desactivada de forma predeterminada porque tiene un (pequeño) costo en el rendimiento del sistema; sin embargo, puede activarla agregando la opción del núcleo <command>cgroup_enable=memory</command> en el momento de inicio;"

#~ msgid "The procedure requires setting up a <filename>yum.conf</filename> file containing the necessary parameters, including the path to the source RPM repositories, the path to the plugin configuration, and the destination folder. For this example, we will assume that the environment will be stored in <filename>/var/tmp/yum-bootstrap</filename>. The file <filename>/var/tmp/yum-bootstrap/yum.conf</filename> file should look like this:"
#~ msgstr "El procedimiento necesita que defina un archivo <filename>yum.conf</filename> que contenga los parámetros necesarios, entre ellos: la ruta a los repositorios RPM de origen, la ruta a la configuración de plugins y la carpeta de destino. Para este ejemplo, asumiremos que se almacenará el entorno en <filename>/var/tmp/yum-bootstrap</filename>. El archivo <filename>/var/tmp/yum-bootstrap/yum.conf</filename> debería verse de la siguiente forma:"

#~ msgid ""
#~ "[main]\n"
#~ "reposdir=/var/tmp/yum-bootstrap/repos.d\n"
#~ "pluginconfpath=/var/tmp/yum-bootstrap/pluginconf.d\n"
#~ "cachedir=/var/cache/yum\n"
#~ "installroot=/path/to/destination/domU/install\n"
#~ "exclude=$exclude\n"
#~ "keepcache=1\n"
#~ "#debuglevel=4  \n"
#~ "#errorlevel=4\n"
#~ "pkgpolicy=newest\n"
#~ "distroverpkg=centos-release\n"
#~ "tolerant=1\n"
#~ "exactarch=1\n"
#~ "obsoletes=1\n"
#~ "gpgcheck=1\n"
#~ "plugins=1\n"
#~ "metadata_expire=1800"
#~ msgstr ""
#~ "[main]\n"
#~ "reposdir=/var/tmp/yum-bootstrap/repos.d\n"
#~ "pluginconfpath=/var/tmp/yum-bootstrap/pluginconf.d\n"
#~ "cachedir=/var/cache/yum\n"
#~ "installroot=/ruta/destino/instalacion/domU/\n"
#~ "exclude=$exclude\n"
#~ "keepcache=1\n"
#~ "#debuglevel=4  \n"
#~ "#errorlevel=4\n"
#~ "pkgpolicy=newest\n"
#~ "distroverpkg=centos-release\n"
#~ "tolerant=1\n"
#~ "exactarch=1\n"
#~ "obsoletes=1\n"
#~ "gpgcheck=1\n"
#~ "plugins=1\n"
#~ "metadata_expire=1800"

#~| msgid "The <filename>/var/tmp/yum-bootstrap/repos.d</filename> directory should contain the descriptions of the RPM source repositories, just as in <filename>/etc/yum.repos.d</filename> in an already installed RPM-based system. Here is an example for a CentOS 6 installation:"
#~ msgid "The <filename>/var/tmp/yum-bootstrap/repos.d</filename> directory should contain the descriptions of the RPM source repositories in <filename>*.repo</filename> files, just as in <filename>/etc/yum.repos.d</filename> in an already installed RPM-based system. Here is an example for a CentOS 6 installation:"
#~ msgstr "El directorio <filename>/var/tmp/yum-bootstrap/repos.d</filename> debería contener las descripciones de los repositorios RPM de origen en los archivos <filename>*.repo</filename>, de la misma forma que el archivo <filename>/etc/yum.repos.d</filename> en un sistema basado en RPM ya instalado. Este es, como ejemplo, el archivo de una instalación de CentOS 6:"

#~ msgid ""
#~ "[base]\n"
#~ "name=CentOS-6 - Base\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "   \n"
#~ "[updates]\n"
#~ "name=CentOS-6 - Updates\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[extras]\n"
#~ "name=CentOS-6 - Extras\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[centosplus]\n"
#~ "name=CentOS-6 - Plus\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6"
#~ msgstr ""
#~ "[base]\n"
#~ "name=CentOS-6 - Base\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "   \n"
#~ "[updates]\n"
#~ "name=CentOS-6 - Updates\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[extras]\n"
#~ "name=CentOS-6 - Extras\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[centosplus]\n"
#~ "name=CentOS-6 - Plus\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6"

#~ msgid "Finally, <filename>pluginconf.d/installonlyn.conf</filename> file should contain the following:"
#~ msgstr "Finalmente, el archivo <filename>pluginconf.d/installonlyn.conf</filename> debería contener lo siguiente:"

#~ msgid ""
#~ "[main]\n"
#~ "enabled=1\n"
#~ "tokeep=5"
#~ msgstr ""
#~ "[main]\n"
#~ "enabled=1\n"
#~ "tokeep=5"

#~ msgid "Once all this is setup, make sure the <command>rpm</command> databases are correctly initialized, with a command such as <command>rpm --rebuilddb</command>. An installation of CentOS 6 is then a matter of the following:"
#~ msgstr "Una vez que todo esto está configurado, asegúrese que las bases de datos <command>rpm</command> están inicializadas correctamente, ejecutando algo como <command>rpm --rebuilddb</command>. Luego, puede instalar CentOS 6 de la siguiente forma:"

#~ msgid "<userinput>yum -c /var/tmp/yum-bootstrap/yum.conf -y install coreutils basesystem centos-release yum-basearchonly initscripts</userinput>"
#~ msgstr "<userinput>yum -c /var/tmp/yum-bootstrap/yum.conf -y install coreutils basesystem centos-release yum-basearchonly initscripts</userinput>"

#~ msgid "Xen is currently only available for the i386 and amd64 architectures. Moreover, it uses processor instructions that haven't always been provided in all i386-class computers. Note that most of the Pentium-class (or better) processors made after 2001 will work, so this restriction won't apply to very many situations."
#~ msgstr "Xen sólo está disponible actualmente para las arquitecturas i386 y amd64. Lo que es más, utiliza instrucciones de procesador que no siempre estuvieron disponibles en todos los equipos de clase i386. Sepa que funcionarán la mayoría de los procesadores de clase Pentium (o mejores) creados después de 2001, por lo que esta restricción no aplicará en demasiadas situaciones."

#~ msgid ""
#~ "# /etc/fstab: static file system information.\n"
#~ "[...]\n"
#~ "cgroup            /sys/fs/cgroup           cgroup    defaults        0       0"
#~ msgstr ""
#~ "# /etc/fstab: información estática del sistema de archivos.\n"
#~ "[...]\n"
#~ "cgroup            /sys/fs/cgroup           cgroup    defaults        0       0\n"

#~ msgid "<filename>/sys/fs/cgroup</filename> will then be mounted automatically at boot time; if no immediate reboot is planned, the filesystem should be manually mounted with <command>mount /sys/fs/cgroup</command>."
#~ msgstr "Se montará automáticamente <filename>/sys/fs/cgroup</filename> al iniciar; si no planea reiniciar en el futuro cercano debe montar manualmente el sistema de archivos ejecutando <command>mount /sys/fs/cgroup</command>."

#~ msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots; its configuration file, <filename>/etc/default/lxc</filename>, is relatively straightforward; note that the container configuration files need to be stored in <filename>/etc/lxc/auto/</filename>; many users may prefer symbolic links, such as can be created with <command>ln -s /var/lib/lxc/testlxc/config /etc/lxc/auto/testlxc.config</command>."
#~ msgstr "El paquete <emphasis role=\"pkg\">lxc</emphasis> contiene un script de inicialización que puede iniciar automáticamente uno o más contenedores cuando inicia el anfitrión; su archivo de configuración, <filename>/etc/default/lxc</filename> es bastante directo; sepa que necesita almacenar los archivos de configuración del contenedor en <filename>/etc/lxc/auto/</filename>; muchos usuarios prefieren enlaces simbólicos, que puede crear con <command>ln -s /var/lib/lxc/testlxc/config /etc/lxc/auto/testlxc.config</command>."

#~ msgid "<emphasis>BEWARE</emphasis> Bugs in default <literal>debian</literal> template"
#~ msgstr "<emphasis>CUIDADO</emphasis> Errores en la plantilla predeterminada de <literal>debian</literal>"

#~ msgid "The <command>/usr/share/lxc/templates/lxc-debian</command> template creation script provided in the initial <emphasis role=\"distribution\">Wheezy</emphasis> package (aka <emphasis role=\"pkg\">lxc</emphasis> 0.8.0~rc1-8+deb7u1) suffers from numerous problems. The most important one is that it relies on the <command>live-debconfig</command> program which is not available in <emphasis role=\"distribution\">Wheezy</emphasis> but only in newer versions of Debian. <ulink type=\"block\" url=\"http://bugs.debian.org/680469\" /> <ulink type=\"block\" url=\"http://bugs.debian.org/686747\" />"
#~ msgstr "El script de creación de plantillas <command>/usr/share/lxc/templates/lxc-debian</command> incluído en el paquete original en <emphasis role=\"distribution\">Wheezy</emphasis> (es decir: <emphasis role=\"pkg\">lxc</emphasis> 0.8.0~rc1-8+deb7u1) tiene varios problemas. El más importante es que depende del programa <command>live-debconfig</command> que no está disponible en <emphasis role=\"distribution\">Wheezy</emphasis> sino en versiones posteriores de Debian. <ulink type=\"block\" url=\"http://bugs.debian.org/680469\" /> <ulink type=\"block\" url=\"http://bugs.debian.org/686747\" />"

#~ msgid "At the time of writing, there was no good solution and no usable work-around, except to use an alternate template creation script. Further updates of lxc might fix this though. This section assumes that <command>/usr/share/lxc/templates/lxc-debian</command> matches the upstream provided script: <ulink type=\"block\" url=\"https://github.com/lxc/lxc/raw/master/templates/lxc-debian.in\" />"
#~ msgstr "Al momento de escribir estas líneas, no existía una solución decente o alternativa útil excepto utilizar otro script de creación de plantillas. Es posible que futuras actualizaciones de lxc lo corrijan. Esta sección asume que <command>/usr/share/lxc/templates/lxc-debian</command> coincide con el script que se provee en origen: <ulink type=\"block\" url=\"https://github.com/lxc/lxc/raw/master/templates/lxc-debian.in\" />"

#~ msgid "<primary><command>Bochs</command></primary>"
#~ msgstr "<primary><command>Bochs</command></primary>"

#~ msgid "<primary><command>QEMU</command></primary>"
#~ msgstr "<primary><command>QEMU</command></primary>"

#~ msgid "<primary><command>VirtualBox</command></primary>"
#~ msgstr "<primary><command>VirtualBox</command></primary>"

#~ msgid "A kernel with the appropriate patches allowing it to work on that hypervisor. In the 2.6.32 case relevant to <emphasis role=\"distribution\">Squeeze</emphasis>, the available hardware will dictate the choice among the various available <emphasis role=\"pkg\">xen-linux-system-2.6.32-5-xen-*</emphasis> packages."
#~ msgstr "Un núcleo con los parches apropiados que le permita funcionar en dicho hypervisor. En el caso de 2.6.32 relevante en <emphasis role=\"distribution\">Squeeze</emphasis>, el hardware disponible indicará la elección del paquete correcto entre los <emphasis role=\"pkg\">xen-linux-system-2.6.32-5-xen-*</emphasis> disponibles."

#~ msgid "If the Xen image is not meant to run Debian but another system, another potentially interesting option is <literal>--rpmstrap</literal>, to invoke <command>rpmstrap</command> in order to initialize a new RPM-based system (such as Fedora, CentOS or Mandriva). Other methods include <literal>--copy</literal>, to copy an image from an existing system, and <literal>--tar</literal>, to extract the system image from an archive."
#~ msgstr "Si no pretende que la imagen Xen ejecute Debian si no otro sistema operativo, otra opción posiblemente interesante es <literal>--rpmstrap</literal> que ejecutará <command>rpmstrap</command> para inicializar un nuevo sistema basado en RPM (como Fedora, CentOS o Mandriva). Otros métodos incluyen la opción <literal>--copy</literal> para copiar una imagen de un sistema existente y la opción <literal>--tar</literal> para extraer la imagen del sistema desde un compendio tar."

#~ msgid "Note also that the <command>lxc-debian</command> command as shipped in <emphasis role=\"distribution\">Squeeze</emphasis> unfortunately creates a <emphasis role=\"distribution\">Lenny</emphasis> system, and not a <emphasis role=\"distribution\">Squeeze</emphasis> system as one could expect. This problem can be worked around by simply installing a newer version of the package (starting from 0.7.3-1)."
#~ msgstr "Sepa también que el programa <command>lxc-debian</command> como está incluido en <emphasis role=\"distribution\">Squeeze</emphasis> lamentablemente crea un systema <emphasis role=\"distribution\">Lenny</emphasis> y no un sistema <emphasis role=\"distribution\">Squeeze</emphasis> como uno esperaría. Puede evitar este problema simplemente instalando una nueva versión del paquete (comenzando con 0.7.3-1)."

#~ msgid "The newly-created filesystem now contains a minimal Debian system, adapted to the aforementioned “simple” network configuration. In the “rich” configuration, the <filename>/var/lib/lxc/testlxc/rootfs/etc/network/interfaces</filename> file will need some modifications; more important, though, is that the network interface that the container sees must not be the host's physical interface. This can be configured by adding a few <literal>lxc.network.*</literal> entries to the container's configuration file, <filename>/var/lib/lxc/testlxc/config</filename>:"
#~ msgstr "El sistema de archivos recientemente creado ahora contiene un sistema Debian mínimo, adaptado a la configuración de red «simple» que ya mencionamos. En la configuración «enriquecida», necesitará modificar el archivo <filename>/var/lib/lxc/testlxc/rootfs/etc/network/interfaces</filename>; lo que es más importante, sin embargo, es que la interfaz de red que ve el contenedor no debe ser la interfaz física del anfitrión. Puede configurar esto agregando unas pocas líneas <literal>lxc.network.*</literal> al archivo de configuración del contenedor, <filename>/var/lib/lxc/testlxc/config</filename>:"

#~ msgid "<computeroutput># </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
#~ msgstr "<computeroutput># </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"

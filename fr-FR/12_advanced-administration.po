# Roland Mas <lolando@debian.org>, 2015.
msgid ""
msgstr ""
"Project-Id-Version: The Debian Administrator's Handbook\n"
"POT-Creation-Date: 2022-07-30 18:23+0200\n"
"PO-Revision-Date: 2021-07-22 04:32+0000\n"
"Last-Translator: Frédéric Daniel Luc Lehobey <weblate@lehobey.net>\n"
"Language-Team: French <https://hosted.weblate.org/projects/debian-handbook/12_advanced-administration/fr/>\n"
"Language: fr-FR\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=n > 1;\n"
"X-Generator: Weblate 4.7.2-dev\n"

msgid "RAID"
msgstr "RAID"

msgid "LVM"
msgstr "LVM"

msgid "FAI"
msgstr "FAI"

msgid "Preseeding"
msgstr "Preseeding"

msgid "Monitoring"
msgstr "Supervision"

msgid "Virtualization"
msgstr "Virtualisation"

msgid "Xen"
msgstr "Xen"

msgid "LXC"
msgstr "LXC"

msgid "Advanced Administration"
msgstr "Administration avancée"

msgid "This chapter revisits some aspects we already described, with a different perspective: instead of installing one single computer, we will study mass-deployment systems; instead of creating RAID or LVM volumes at install time, we'll learn to do it by hand so we can later revise our initial choices. Finally, we will discuss monitoring tools and virtualization techniques. As a consequence, this chapter is more particularly targeting professional administrators, and focuses somewhat less on individuals responsible for their home network."
msgstr "Ce chapitre est l'occasion de revenir sur des aspects déjà abordés mais avec une nouvelle perspective : au lieu d'installer une machine, nous étudierons les solutions pour déployer un parc de machines ; au lieu de créer une partition RAID ou LVM avec les outils intégrés à l'installateur, nous apprendrons à le faire manuellement afin de pouvoir revenir sur les choix initiaux. Enfin, la découverte des outils de supervision et de virtualisation parachèvera ce chapitre avant tout destiné aux administrateurs professionnels plus qu'aux particuliers responsables de leur réseau familial."

msgid "RAID and LVM"
msgstr "RAID et LVM"

msgid "<primary>RAID</primary>"
msgstr "<primary>RAID</primary>"

msgid "<primary>LVM</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Logical Volume Manager</primary><see>LVM</see>"
msgstr "<primary>Logical Volume Manager</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>logical volume</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>raid volume</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>filesystem</primary><secondary>redundancy</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive reinstallation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive re-installation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgstr "Le <xref linkend=\"installation\" /> a présenté ces technologies en montrant comment l'installateur facilitait leur déploiement. Au-delà de cette étape cruciale, un bon administrateur doit pouvoir gérer l'évolution de ses besoins en espace de stockage sans devoir recourir à une coûteuse réinstallation. Il convient donc de maîtriser les outils qui servent à manipuler des volumes RAID et LVM."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>management</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former ensures the security and availability of the data in case of hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgstr "Ces deux techniques permettent d'abstraire les volumes à monter de leurs contreparties physiques (disques durs ou partitions), la première pour garantir la sécurité et la disponibilité des données en cas de panne matérielle en dupliquant les informations et la seconde pour gérer ses données à sa guise en faisant abstraction de la taille réelle de ses disques. Dans les deux cas, cela se traduit par de nouveaux périphériques de type bloc sur lesquels on pourra donc créer des systèmes de fichiers, ou des espaces de mémoire virtuelle, qui ne correspondent pas directement à un seul disque dur réel. Bien que ces deux systèmes aient des origines bien distinctes, leurs fonctionnalités se recoupent en partie ; c'est pourquoi ils sont souvent mentionnés ensemble."

msgid "<emphasis>PERSPECTIVE</emphasis> Btrfs combines LVM and RAID"
msgstr "<emphasis>PERSPECTIVE</emphasis> Btrfs combine LVM et RAID"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>mount</command></primary><secondary>Btrfs</secondary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary>Btrfs</primary>"
msgstr "<primary>preseed</primary>"

#, fuzzy
#| msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a new filesystem, initially developed at Oracle, that purports to combine the featuresets of LVM and RAID and much more. It is mostly functional, and although it is still tagged “experimental” because its development is incomplete (some features aren't implemented yet), it has already seen some use in production environments. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"
msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a filesystem, initially developed at Oracle, that purports to combine the feature sets of LVM and RAID and much more. <ulink type=\"block\" url=\"https://btrfs.wiki.kernel.org/\" />"
msgstr "Alors que LVM et RAID sont deux sous-systèmes distincts du noyau qui viennent s'intercaler entre les périphériques blocs des disques et les systèmes de fichiers, <emphasis>btrfs</emphasis> est un nouveau système de fichiers initié par Oracle qui combine les fonctionnalités de LVM et RAID, et plus encore. Bien que fonctionnel, et même s'il est encore marqué d'un sceau « expérimental » (il n'est pas terminé, certaines fonctionnalités ne sont pas encore implémentées), il a déjà fait l'objet de quelques déploiements en production. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"

msgid "Among the noteworthy features are the ability to take a snapshot of a filesystem tree at any point in time. This snapshot copy doesn't initially use any disk space, the data only being duplicated when one of the copies is modified. The filesystem also handles transparent compression of files, and checksums ensure the integrity of all stored data."
msgstr "Parmi les fonctionnalités les plus intéressantes, on peut noter la possibilité de capturer l'état d'une arborescence à un instant donné. Cette copie ne consomme initialement pas d'espace disque, chaque fichier n'étant réellement dupliqué que lorsque l'une des deux copies est modifiée. Il gère également la compression (transparente) des fichiers et des sommes de contrôle pour s'assurer de l'intégrité de toutes les données stockées."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>filesystem</primary><secondary>snapshot</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "In both the RAID and LVM cases, the kernel provides a block device file, similar to the ones corresponding to a hard disk drive or a partition. When an application, or another part of the kernel, requires access to a block of such a device, the appropriate subsystem routes the block to the relevant physical layer. Depending on the configuration, this block can be stored on one or several physical disks, and its physical location may not be directly correlated to the location of the block in the logical device."
msgstr "À la fois pour RAID et pour LVM, le noyau fournit un fichier de périphérique accessible en mode bloc (donc de la même manière qu'un disque dur ou une partition de celui-ci). Lorsqu'une application, ou une autre partie du noyau, a besoin d'accéder à un bloc de ce périphérique, le sous-système correspondant se charge d'effectuer le routage de ce bloc vers la couche physique appropriée, ce bloc pouvant être stocké sur un ou plusieurs disques, à un endroit non directement corrélé avec l'emplacement demandé dans le périphérique logique."

msgid "Software RAID"
msgstr "RAID logiciel"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>Software RAID</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "<primary>Redundant Array of Independent Disks</primary><see>RAID</see>"
msgstr ""

#, fuzzy
#| msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss and ensure availability in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgstr "RAID signifie <foreignphrase>Redundant Array of Independent Disks</foreignphrase> (ensemble redondant de disques indépendants). Ce système a vu le jour pour fournir une protection contre les pannes de disques durs. Le principe général est simple : les données sont stockées sur un ensemble de plusieurs disques physiques au lieu d'être concentrées sur un seul, avec un certain degré (variable) de redondance. Selon ce niveau de redondance, en cas de panne subite d'un de ces disques, les données peuvent être reconstruites à l'identique à partir des disques qui restent opérationnels, ce qui évite de les perdre."

msgid "<emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?"
msgstr "<emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> ou <foreignphrase>inexpensive</foreignphrase> ?"

#, fuzzy
#| msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavour of cheapness."
msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavor of cheapness."
msgstr "Le « I » de RAID signifiait à l'origine <foreignphrase>inexpensive</foreignphrase> (« bon marché »), car le RAID permet d'obtenir une nette augmentation de la sécurité des données sans investir dans des disques hors de prix. Peut-être pour des considérations d'image, on a tendance à préférer <foreignphrase>independent</foreignphrase>, qui n'a pas la connotation de bricolage associée au bon marché."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>Hardware RAID</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>degraded</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>reconstruction</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID can be implemented either by dedicated hardware (RAID modules integrated into SCSI or SATA controller cards) or by software abstraction (the kernel). Whether hardware or software, a RAID system with enough redundancy can transparently stay operational when a disk fails; the upper layers of the stack (applications) can even keep accessing the data in spite of the failure. Of course, this “degraded mode” can have an impact on performance, and redundancy is reduced, so a further disk failure can lead to data loss. In practice, therefore, one will strive to only stay in this degraded mode for as long as it takes to replace the failed disk. Once the new disk is in place, the RAID system can reconstruct the required data so as to return to a safe mode. The applications won't notice anything, apart from potentially reduced access speed, while the array is in degraded mode or during the reconstruction phase."
msgstr "Le RAID peut être mis en œuvre par du matériel dédié (soit des modules RAID intégrés à des cartes pour contrôleur SCSI, soit directement sur la carte mère) ou par l'abstraction logicielle (le noyau). Qu'il soit matériel ou logiciel, un système RAID disposant de suffisamment de redondance peut, en cas de défaillance d'un disque, rester opérationnel en toute transparence, le niveau supérieur (les applications) pouvant même continuer à accéder aux données sans interruption malgré la panne. Évidemment, ce « mode dégradé » peut avoir des implications en termes de performances et il réduit la quantité de redondance du système ; une deuxième panne simultanée peut aboutir à la perte des données. Il est donc d'usage de ne rester en mode dégradé que le temps de se procurer un remplaçant pour le disque défaillant. Une fois qu'il est mis en place, le système RAID peut reconstruire les données qui doivent y être présentes, de manière à revenir à un état sécurisé. Le tout se fait bien entendu de manière invisible pour les applications, hormis les baisses éventuelles de performances pendant la durée du mode dégradé et la phase de reconstruction qui s'ensuit."

msgid "When RAID is implemented by hardware, its configuration generally happens within the BIOS setup tool, and the kernel will consider a RAID array as a single disk, which will work as a standard physical disk, although the device name may be different (depending on the driver)."
msgstr "Lorsque le RAID est implémenté par le matériel, c'est le setup du BIOS qui permet généralement de le configurer et le noyau Linux va considérer le volume RAID comme un seul disque, fonctionnant comme un disque standard, à ceci près que le nom du périphérique peut différer."

msgid "We only focus on software RAID in this book."
msgstr "Nous ne traiterons que du RAID logiciel dans ce livre."

msgid "Different RAID Levels"
msgstr "Différents niveaux de RAID"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>level</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID is actually not a single system, but a range of systems identified by their levels; the levels differ by their layout and the amount of redundancy they provide. The more redundant, the more failure-proof, since the system will be able to keep working with more failed disks. The counterpart is that the usable space shrinks for a given set of disks; seen the other way, more disks will be needed to store a given amount of data."
msgstr "On distingue plusieurs niveaux de RAID, différant dans l'agencement des données et le degré de redondance qu'ils proposent. Plus la redondance est élevée, plus la résistance aux pannes sera forte, puisque le système pourra continuer à fonctionner avec un plus grand nombre de disques en panne ; la contrepartie est que le volume utile de données devient plus restreint (ou, pour voir les choses différemment, qu'il sera nécessaire d'avoir plus de disques pour stocker la même quantité de données)."

msgid "Linear RAID"
msgstr "RAID linéaire"

#, fuzzy
#| msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That's about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That is about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgstr "Bien que le sous-système RAID du noyau permette la mise en œuvre de « RAID linéaire », il ne s'agit pas à proprement parler de RAID, puisqu'il n'y a aucune redondance. Le noyau se contente d'agréger plusieurs disques les uns à la suite des autres et de les proposer comme un seul disque virtuel (en fait, un seul périphérique bloc). C'est à peu près sa seule utilité et il n'est que rarement utilisé seul (voir plus loin), d'autant que l'absence de redondance signifie que la défaillance d'un seul disque rend la totalité des données inaccessibles."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>linear</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID-0"
msgstr "RAID-0"

msgid "This level doesn't provide any redundancy either, but disks aren't simply stuck on end one after another: they are divided in <emphasis>stripes</emphasis>, and the blocks on the virtual device are stored on stripes on alternating physical disks. In a two-disk RAID-0 setup, for instance, even-numbered blocks of the virtual device will be stored on the first physical disk, while odd-numbered blocks will end up on the second physical disk."
msgstr "Ici non plus, aucune redondance n'est proposée, mais les disques ne sont plus simplement mis bout à bout : ils sont en réalité découpés en <foreignphrase>stripes</foreignphrase> (bandes), ces bandes étant alors intercalées dans le disque logique. Ainsi, dans le cas de RAID-0 à deux disques, les blocs impairs du volume virtuel seront stockés sur le premier disque et les blocs pairs sur le second."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>stripes</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. However, RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. The disks are utilized entirely by the RAID device, so they should have the same size not to lose performance."
msgstr "Le but de ce système n'est pas d'augmenter la fiabilité, puisqu'ici aussi un seul disque en panne rend inaccessible la totalité des données, mais d'améliorer les performances : lors d'un accès séquentiel à de grandes quantités de données contiguës, le noyau pourra lire (ou écrire) en parallèle depuis les deux (ou plus...) disques qui composent l'ensemble, ce qui augmente le débit. L'usage du RAID-0 a tendance à disparaître au profit de LVM, qui sera abordé par la suite."

msgid "RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>0</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID-1"
msgstr "RAID-1"

msgid "This level, also known as “RAID mirroring”, is both the simplest and the most widely used setup. In its standard form, it uses two physical disks of the same size, and provides a logical volume of the same size again. Data are stored identically on both disks, hence the “mirror” nickname. When one disk fails, the data is still available on the other. For really critical data, RAID-1 can of course be set up on more than two disks, with a direct impact on the ratio of hardware cost versus available payload space."
msgstr "Aussi connu sous le nom de « RAID miroir », c'est le système RAID le plus simple. Il utilise en général deux disques physiques de tailles identiques et fournit un volume logique de la même taille. Les données sont stockées à l'identique sur les deux disques, d'où l'appellation de « miroir ». En cas de panne d'un disque, les données restent accessibles sur l'autre. Pour les données vraiment critiques, on peut utiliser le RAID-1 sur plus de deux disques, au prix de devoir multiplier le rapport entre le coût des disques et la quantité de données utiles."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>1</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>mirror</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "<emphasis>NOTE</emphasis> Disks and cluster sizes"
msgstr "<emphasis>NOTE</emphasis> Taille des disques, taille de l'ensemble"

msgid "If two disks of different sizes are set up in a mirror, the bigger one will not be fully used, since it will contain the same data as the smallest one and nothing more. The useful available space provided by a RAID-1 volume therefore matches the size of the smallest disk in the array. This still holds for RAID volumes with a higher RAID level, even though redundancy is stored differently."
msgstr "Si deux disques de tailles différentes sont groupés dans un volume en RAID miroir, le plus gros disque ne sera pas intégralement utilisé, puisqu'il contiendra exactement les mêmes données que le plus petit (et rien de plus). La capacité utile du volume RAID-1 est donc la plus petite des tailles des disques qui le composent. Il en va de même pour les volumes RAID de niveau plus élevé, même si la redondance est répartie différemment."

msgid "It is therefore important, when setting up RAID arrays (except for RAID-0 and “linear RAID”), to only assemble disks of identical, or very close, sizes, to avoid wasting resources."
msgstr "On veillera donc à n'assembler dans un même volume RAID (sauf RAID-0 et linéaire) que des disques de même taille (ou de tailles très proches), afin d'éviter un gaspillage des ressources."

msgid "<emphasis>NOTE</emphasis> Spare disks"
msgstr "<emphasis>NOTE</emphasis> Disques de secours"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary>spare disk</primary>"
msgstr "<primary>preseed</primary>"

msgid "RAID levels that include redundancy allow assigning more disks than required to an array. The extra disks are used as spares when one of the main disks fails. For instance, in a mirror of two disks plus one spare, if one of the first two disks fails, the kernel will automatically (and immediately) reconstruct the mirror using the spare disk, so that redundancy stays assured after the reconstruction time. This can be used as another kind of safeguard for critical data."
msgstr "Dans les niveaux qui offrent une redondance, on peut affecter à un volume RAID plus de disques que nécessaire, qui serviront de secours en cas de défaillance d'un disque principal. Ainsi, il est possible de mettre en place un miroir de deux disques plus un de secours ; si l'un des deux premiers disques tombe, le noyau va automatiquement en reconstruire le contenu sur le disque de secours, de sorte que la redondance restera assurée (après le temps de reconstruction). Ceci est utile pour des données vraiment critiques."

msgid "One would be forgiven for wondering how this is better than simply mirroring on three disks to start with. The advantage of the “spare disk” configuration is that the spare disk can be shared across several RAID volumes. For instance, one can have three mirrored volumes, with redundancy assured even in the event of one disk failure, with only seven disks (three pairs, plus one shared spare), instead of the nine disks that would be required by three triplets."
msgstr "On peut s'interroger sur l'intérêt de cette possibilité, comparé par exemple à l'établissement d'un miroir sur trois disques. L'avantage de cette configuration est en fait que le disque de secours peut être partagé entre plusieurs volumes RAID. On peut ainsi avoir trois volumes miroir, avec l'assurance que les données seront toujours stockées en double même en cas de panne d'un disque, avec seulement 7 disques (trois paires, plus un disque de secours partagé) au lieu de 9 (trois triplets)."

msgid "This RAID level, although expensive (since only half of the physical storage space, at best, is useful), is widely used in practice. It is simple to understand, and it allows very simple backups: since both disks have identical contents, one of them can be temporarily extracted with no impact on the working system. Read performance is often increased since the kernel can read half of the data on each disk in parallel, while write performance isn't too severely degraded. In case of a RAID-1 array of N disks, the data stays available even with N-1 disk failures."
msgstr "Ce niveau de RAID, bien qu'onéreux (puisque seule la moitié, au mieux, de l'espace disque physique se retrouve utilisable), reste assez utilisé en pratique. Il est en effet conceptuellement simple et il permet de réaliser très simplement des sauvegardes (puisque les deux disques sont identiques, on peut en extraire temporairement un sans perturber le fonctionnement du système). Les performances en lecture sont généralement améliorées par rapport à un simple disque (puisque le système peut théoriquement lire la moitié des données sur chaque disque, en parallèle sur les deux), sans trop de perte en vitesse d'écriture. Dans le cas de RAID-1 à N disques, les données restent disponibles même en cas de panne de N-1 disques."

#, fuzzy
#| msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgid "<emphasis>CAUTION</emphasis> RAID is not Backup"
msgstr "<emphasis>ASTUCE</emphasis> RAID, disques et partitions"

#, fuzzy
#| msgid "<primary>debian-cd</primary>"
msgid "<primary>backup</primary>"
msgstr "<primary>debian-cd</primary>"

msgid "RAID systems are not backup mechanisms. While RAID increases the redundancy - and therefore the availability of a system - and protects against disk failures, backups are done to protect data from being altered, deleted, getting corrupted, etc., and to be able to restore them if necessary. To demonstrate this: If you remove one or all files by accident, a RAID will mirror this change, but it will not provide the means to restore the file(s). So while there is clearly an overlap, they are not the same and should be used in conjunction with each other."
msgstr ""

msgid "RAID-4"
msgstr "RAID-4"

msgid "This RAID level, not widely used, uses N disks to store useful data, and an extra disk to store redundancy information. If that disk fails, the system can reconstruct its contents from the other N. If one of the N data disks fails, the remaining N-1 combined with the “parity” disk contain enough information to reconstruct the required data."
msgstr "Ce niveau de RAID, assez peu usité, utilise N disques pour stocker les données utiles, et un disque supplémentaire pour des informations de redondance. Si ce disque tombe en panne, le système peut le reconstruire à l'aide des N autres. Si c'est un des N disques de données qui tombe, les N-1 restants et le disque de parité contiennent suffisamment d'informations pour reconstruire les données."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>4</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>parity</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID-4 isn't too expensive since it only involves a one-in-N increase in costs and has no noticeable impact on read performance, but writes are slowed down. Furthermore, since a write to any of the N disks also involves a write to the parity disk, the latter sees many more writes than the former, and its lifespan can shorten dramatically as a consequence. Data on a RAID-4 array is safe only up to one failed disk (of the N+1)."
msgstr "Le RAID-4 est peu onéreux (puisqu'il n'entraîne qu'un surcoût d'un disque pour N), n'a pas d'impact notable sur les performances en lecture, mais ralentit les écritures. De plus, comme chaque écriture sur un des N disques s'accompagne d'une écriture sur le disque de parité, celui-ci se voit confier beaucoup plus d'écritures que ceux-là et peut voir sa durée de vie considérablement réduite en conséquence. Il résiste au maximum à la panne d'un disque parmi les N+1."

msgid "RAID-5"
msgstr "RAID-5"

msgid "RAID-5 addresses the asymmetry issue of RAID-4: parity blocks are spread over all of the N+1 disks, with no single disk having a particular role."
msgstr "Le RAID-5 corrige ce dernier défaut du RAID-4, en répartissant les blocs de parité sur les N+1 disques, qui jouent à présent un rôle identique."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>5</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Read and write performance are identical to RAID-4. Here again, the system stays functional with up to one failed disk (of the N+1), but no more."
msgstr "Les performances en lecture et en écriture sont inchangées par rapport au RAID-4. Là encore, le système reste fonctionnel en cas de défaillance d'un disque parmi les N+1, mais pas plus."

msgid "RAID-6"
msgstr "RAID-6"

msgid "RAID-6 can be considered an extension of RAID-5, where each series of N blocks involves two redundancy blocks, and each such series of N+2 blocks is spread over N+2 disks."
msgstr "Le RAID-6 peut être considéré comme une extension du RAID-5, dans laquelle à chaque série de N blocs correspondent non plus un mais deux blocs de parité, qui sont ici aussi répartis sur les N+2 disques."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>6</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

# Recommended, as their is a certain homogeniety of hardware. If one disk is failing, the next for the same reason is likely to happen soon after. Ironically as mechanical tolerances improve, the risk of simultaneous failures increase.
msgid "This RAID level is slightly more expensive than the previous two, but it brings some extra safety since up to two drives (of the N+2) can fail without compromising data availability. The counterpart is that write operations now involve writing one data block and two redundancy blocks, which makes them even slower."
msgstr "Ce niveau de RAID, légèrement plus coûteux que les deux précédents, apporte également une protection supplémentaire puisqu'il conserve l'intégralité des données même en cas de panne simultanée de deux disques (sur N+2). La contrepartie est que les opérations d'écriture impliquent dorénavant l'écriture d'un bloc de données et de deux blocs de contrôle, ce qui les ralentit d'autant plus."

msgid "RAID-1+0"
msgstr "RAID-1+0"

# You will start getting into bandwidth limitations rather quickly, requiring additional controllers to spread the load.
#, fuzzy
#| msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there's no problem with that."
msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there is no problem with that."
msgstr "Il ne s'agit pas à proprement parler d'un niveau de RAID, mais d'un RAID à deux niveaux. Si l'on dispose de 2×N disques, on commence par les apparier en N volumes de RAID-1. Ces N volumes sont alors agrégés en un seul, soit par le biais de RAID linéaire, soit par du RAID-0, voire (de plus en plus fréquemment) par LVM. On sort dans ce dernier cas du RAID pur, mais cela ne pose pas de problème."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>1+0</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID-1+0 can survive multiple disk failures: up to N in the 2×N array described above, provided that at least one disk keeps working in each of the RAID-1 pairs."
msgstr "Le RAID-1+0 tolère la panne de disques multiples, jusqu'à N dans le cas d'un groupe de 2×N, à condition qu'au moins un disque reste fonctionnel dans chaque paire associée en RAID-1."

msgid "<emphasis>GOING FURTHER</emphasis> RAID-10"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> RAID-10"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>10</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "RAID-10 is generally considered a synonym of RAID-1+0, but a Linux specificity makes it actually a generalization. This setup allows a system where each block is stored on two different disks, even with an odd number of disks, the copies being spread out along a configurable model."
msgstr "« RAID-10 » est généralement considéré comme un synonyme pour « RAID-1+0 », mais une spécificité de Linux en fait en réalité une généralisation. On peut dans ce mode de fonctionnement obtenir un système où chaque bloc existe en double sur deux disques différents, malgré un nombre impair de disques, les copies étant réparties selon différents modèles en fonction de la configuration."

msgid "Performances will vary depending on the chosen repartition model and redundancy level, and of the workload of the logical volume."
msgstr "Les performances pourront varier en fonction du modèle et du niveau de redondance choisis, ainsi que du type d'activité sur le volume logique."

msgid "Obviously, the RAID level will be chosen according to the constraints and requirements of each application. Note that a single computer can have several distinct RAID arrays with different configurations."
msgstr "On choisira bien entendu le niveau de RAID en fonction des contraintes et des besoins spécifiques de chaque application. Notons qu'on peut constituer plusieurs volumes RAID distincts, avec des configurations différentes, sur le même ordinateur."

msgid "Setting up RAID"
msgstr "Mise en place du RAID"

msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>create</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Setting up RAID volumes requires the <emphasis role=\"pkg\">mdadm</emphasis> package; it provides the <command>mdadm</command> command, which allows creating and manipulating RAID arrays, as well as scripts and tools integrating it to the rest of the system, including the monitoring system."
msgstr "La mise en place de volumes RAID se fait grâce au paquet <emphasis role=\"pkg\">mdadm</emphasis> ; ce dernier contient la commande du même nom, qui permet de créer et manipuler des ensembles RAID, ainsi que les scripts et outils permettant l'intégration avec le système et la supervision."

msgid "Our example will be a server with a number of disks, some of which are already used, the rest being available to setup RAID. We initially have the following disks and partitions:"
msgstr "Prenons l'exemple d'un serveur sur lequel sont branchés un certain nombre de disques, dont certains sont occupés et d'autres peuvent être utilisés pour établir du RAID. On dispose initialement des disques et partitions suivants :"

msgid "the <filename>sdb</filename> disk, 4 GB, is entirely available;"
msgstr "le disque <filename>sdb</filename>, de 4 Go, est entièrement disponible ;"

msgid "the <filename>sdc</filename> disk, 4 GB, is also entirely available;"
msgstr "le disque <filename>sdc</filename>, de 4 Go, est également entièrement disponible ;"

msgid "on the <filename>sdd</filename> disk, only partition <filename>sdd2</filename> (about 4 GB) is available;"
msgstr "sur le disque <filename>sdd</filename>, seule la partition <filename>sdd2</filename> d'environ 4 Go est disponible ;"

msgid "finally, a <filename>sde</filename> disk, still 4 GB, entirely available."
msgstr "enfin, un disque <filename>sde</filename>, toujours de 4 Go, est entièrement disponible."

msgid "<emphasis>NOTE</emphasis> Identifying existing RAID volumes"
msgstr "<emphasis>NOTE</emphasis> Identifier les volumes RAID existants"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/proc</filename></primary><secondary><filename>/proc/mdstat</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The <filename>/proc/mdstat</filename> file lists existing volumes and their states. When creating a new RAID volume, care should be taken not to name it the same as an existing volume."
msgstr "Le fichier <filename>/proc/mdstat</filename> liste les volumes existants et leur état. On prendra soin lors de la création d'un nouveau volume RAID de ne pas essayer de le nommer avec le nom d'un volume existant."

msgid "We're going to use these physical elements to build two volumes, one RAID-0 and one mirror (RAID-1). Let's start with the RAID-0 volume:"
msgstr "Nous allons construire sur ces éléments physiques deux volumes, l'un en RAID-0, l'autre en miroir. Commençons par le RAID-0 :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md0 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0:\n"
#| "        Version : 1.2\n"
#| "  Creation Time : Wed May  6 09:24:34 2015\n"
#| "     Raid Level : raid0\n"
#| "     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 2\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:24:34 2015\n"
#| "          State : clean \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 0\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "     Chunk Size : 512K\n"
#| "\n"
#| "           Name : mirwiz:0  (local to host mirwiz)\n"
#| "           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
#| "         Events : 0\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       16        0      active sync   /dev/sdb\n"
#| "       1       8       32        1      active sync   /dev/sdc\n"
#| "# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
#| "<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
#| "Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
#| "Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
#| "Superblock backups stored on blocks: \n"
#| "        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
#| "\n"
#| "Allocating group tables: done                            \n"
#| "Writing inode tables: done                            \n"
#| "Creating journal (32768 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
#| "<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
#| "/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc\n"
"</userinput><computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0\n"
"</userinput><computeroutput>/dev/md0: 7.99GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0\n"
"</userinput><computeroutput>/dev/md0:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 01:54:24 2022\n"
"        Raid Level : raid0\n"
"        Array Size : 8378368 (7.99 GiB 8.58 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 01:54:24 2022\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"            Layout : -unknown-\n"
"        Chunk Size : 512K\n"
"\n"
"Consistency Policy : none\n"
"\n"
"              Name : debian:0  (local to host debian)\n"
"              UUID : a75ac628:b384c441:157137ac:c04cd98c\n"
"            Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8        0        0      active sync   /dev/sdb\n"
"       1       8       16        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0\n"
"</userinput><computeroutput>mke2fs 1.46.2 (28-Feb-2021)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 2094592 4k blocks and 524288 inodes\n"
"Filesystem UUID: ef077204-c477-4430-bf01-52288237bea0\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/raid-0\n"
"</userinput><computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0\n"
"</userinput><computeroutput># </computeroutput><userinput>df -h /srv/raid-0\n"
"</userinput><computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.8G   24K  7.4G   1% /srv/raid-0\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:24:34 2015\n"
"     Raid Level : raid0\n"
"     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:24:34 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"     Chunk Size : 512K\n"
"\n"
"           Name : mirwiz:0  (local to host mirwiz)\n"
"           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       16        0      active sync   /dev/sdb\n"
"       1       8       32        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
"Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
"Superblock backups stored on blocks: \n"
"        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0\n"
"</computeroutput>"

#, fuzzy
#| msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It's also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It is also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgstr "La commande <command>mdadm --create</command> requiert en arguments le nom du volume à créer (<filename>/dev/md*</filename>, MD signifiant <foreignphrase>Multiple Device</foreignphrase>), le niveau de RAID, le nombre de disques (qui prend tout son sens à partir du RAID-1 mais qui est systématiquement obligatoire) et les périphériques à utiliser. Une fois le périphérique créé, nous pouvons l'utiliser comme une partition normale, y créer un système de fichiers, le monter, etc. On notera que le fait que nous ayons créé un volume RAID-0 sur <filename>md0</filename> est une pure coïncidence et que le numéro d'un ensemble n'a pas à être corrélé avec le modèle de redondance (ou non) choisi. Il est également possible de créer des volumes RAID nommés, en indiquant à <command>mdadm</command> des noms de volume comme <filename>/dev/md/lineaire</filename> au lieu de <filename>/dev/md0</filename>."

msgid "Creation of a RAID-1 follows a similar fashion, the differences only being noticeable after the creation:"
msgstr "La création d'un volume RAID-1 se fait de manière similaire, les différences n'apparaissant qu'après sa création :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: Note: this array has metadata at the start and\n"
#| "    may not be suitable as a boot device.  If you plan to\n"
#| "    store '/boot' on this device please ensure that\n"
#| "    your boot-loader understands md/v1.x metadata, or use\n"
#| "    --metadata=0.90\n"
#| "mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
#| "Continue creating array? </computeroutput><userinput>y</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md1 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "        Version : 1.2\n"
#| "  Creation Time : Wed May  6 09:30:19 2015\n"
#| "     Raid Level : raid1\n"
#| "     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
#| "  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 2\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:30:40 2015\n"
#| "          State : clean, resyncing (PENDING) \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 0\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 0\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       1       8       64        1      active sync   /dev/sde\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "          State : clean\n"
#| "[...]\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde\n"
"</userinput><computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdc2) exceeds size (4189184K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y\n"
"</userinput><computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1\n"
"</userinput><computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:08:09 2022\n"
"             State : clean, resync\n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 13% complete\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 17\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       1       8       48        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:30:19 2015\n"
"     Raid Level : raid1\n"
"     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
"  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:30:40 2015\n"
"          State : clean, resyncing (PENDING) \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       1       8       64        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"

msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgstr "<emphasis>ASTUCE</emphasis> RAID, disques et partitions"

#, fuzzy
#| msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions, and do not require full disks."
msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions as well, and do not require full disks."
msgstr "Comme on peut le constater sur notre exemple, il n'est nullement nécessaire d'utiliser des disques entiers pour faire du RAID, on peut très bien n'en utiliser que certaines partitions."

msgid "A few remarks are in order. First, <command>mdadm</command> notices that the physical elements have different sizes; since this implies that some space will be lost on the bigger element, a confirmation is required."
msgstr "Plusieurs remarques ici. Premièrement, <command>mdadm</command> constate que les deux éléments physiques n'ont pas la même taille ; comme cela implique que de l'espace sera perdu sur le plus gros des deux éléments, une confirmation est nécessaire."

msgid "More importantly, note the state of the mirror. The normal state of a RAID mirror is that both disks have exactly the same contents. However, nothing guarantees this is the case when the volume is first created. The RAID subsystem will therefore provide that guarantee itself, and there will be a synchronization phase as soon as the RAID device is created. After some time (the exact amount will depend on the actual size of the disks…), the RAID array switches to the “active” or “clean” state. Note that during this reconstruction phase, the mirror is in a degraded mode, and redundancy isn't assured. A disk failing during that risk window could lead to losing all the data. Large amounts of critical data, however, are rarely stored on a freshly created RAID array before its initial synchronization. Note that even in degraded mode, the <filename>/dev/md1</filename> is usable, and a filesystem can be created on it, as well as some data copied on it."
msgstr "La deuxième remarque, plus importante, concerne l'état du miroir. Les deux disques sont en effet censés avoir, en fonctionnement normal, un contenu rigoureusement identique. Comme rien ne garantit que ce soit le cas à la création du volume, le système RAID va s'en assurer. Il y a donc une phase de synchronisation, automatique, dès la création du périphérique RAID. Si l'on patiente quelques instants (qui varient selon la taille des disques...), on obtient finalement un état « actif » ou « propre ». Il est à noter que durant cette étape de reconstruction du miroir, l'ensemble RAID est en mode dégradé et que la redondance n'est pas assurée. Une panne de l'un des disques pendant cette fenêtre sensible peut donc aboutir à la perte de l'intégralité des données. Il est cependant rare que de grandes quantités de données critiques soient placées sur un volume RAID fraîchement créé avant que celui-ci ait eu le temps de se synchroniser. On notera également que même en mode dégradé, le périphérique <filename>/dev/md1</filename> est utilisable (pour créer le système de fichiers et commencer à copier des données, éventuellement)."

msgid "<emphasis>TIP</emphasis> Starting a mirror in degraded mode"
msgstr "<emphasis>ASTUCE</emphasis> Démarrer un miroir en mode dégradé"

msgid "Sometimes two disks are not immediately available when one wants to start a RAID-1 mirror, for instance because one of the disks one plans to include is already used to store the data one wants to move to the array. In such circumstances, it is possible to deliberately create a degraded RAID-1 array by passing <filename>missing</filename> instead of a device file as one of the arguments to <command>mdadm</command>. Once the data have been copied to the “mirror”, the old disk can be added to the array. A synchronization will then take place, giving us the redundancy that was wanted in the first place."
msgstr "Lorsque l'on souhaite sécuriser des données présentes sur un disque non RAID et les migrer vers un volume RAID-1, il n'est pas toujours possible de disposer à la fois de l'ancien disque et des deux nouveaux en même temps (souvent parce que le disque existant va être intégré dans le miroir). On peut alors délibérément créer le volume RAID-1 en mode dégradé, en passant <filename>missing</filename> en argument à <command>mdadm</command> à la place d'un des deux périphériques à utiliser. Une fois que les données auront été copiées vers le « miroir », le disque historique pourra être intégré au volume. Une synchronisation aura alors lieu, à la suite de quoi les données seront stockées de manière redondante."

msgid "<emphasis>TIP</emphasis> Setting up a mirror without synchronization"
msgstr "<emphasis>ASTUCE</emphasis> Mise en place d'un miroir sans synchronisation"

msgid "RAID-1 volumes are often created to be used as a new disk, often considered blank. The actual initial contents of the disk is therefore not very relevant, since one only needs to know that the data written after the creation of the volume, in particular the filesystem, can be accessed later."
msgstr "Lorsque l'on crée un volume RAID-1, c'est généralement pour l'utiliser comme un disque neuf, donc a priori vierge. Le contenu initial de ce disque importe peu, puisque l'on n'a besoin que de savoir que les données écrites seront bien restituées (en particulier le système de fichiers)."

msgid "One might therefore wonder about the point of synchronizing both disks at creation time. Why care whether the contents are identical on zones of the volume that we know will only be read after we have written to them?"
msgstr "Il peut donc sembler superflu de synchroniser les deux disques d'un miroir lors de sa création. À quoi sert d'avoir un contenu identique sur des zones du volume dont on sait qu'on ne se servira qu'après y avoir écrit ?"

msgid "Fortunately, this synchronization phase can be avoided by passing the <literal>--assume-clean</literal> option to <command>mdadm</command>. However, this option can lead to surprises in cases where the initial data will be read (for instance if a filesystem is already present on the physical disks), which is why it isn't enabled by default."
msgstr "Il est heureusement possible de se passer de cette phase de synchronisation, grâce à l'option <literal>--assume-clean</literal> de <command>mdadm</command>. Cette option pouvant amener à des résultats surprenants dans les cas d'usage où les données initiales seront utilisées (par exemple si un système de fichiers est déjà présent), elle n'est pas active par défaut."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>failing</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Now let's see what happens when one of the elements of the RAID-1 array fails. <command>mdadm</command>, in particular its <literal>--fail</literal> option, allows simulating such a disk failure:"
msgstr "Voyons à présent ce qui se passe en cas de panne d'un élément de l'ensemble RAID-1. <command>mdadm</command> permet de simuler cette défaillance, grâce à son option <literal>--fail</literal> :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Update Time : Wed May  6 09:39:39 2015\n"
#| "          State : clean, degraded \n"
#| " Active Devices : 1\n"
#| "Working Devices : 1\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 19\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       0        0        2      removed\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde\n"
"</userinput><computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:15:34 2022\n"
"             State : clean, degraded \n"
"    Active Devices : 1\n"
"   Working Devices : 1\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       -       0        0        1      removed\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:39:39 2015\n"
"          State : clean, degraded \n"
" Active Devices : 1\n"
"Working Devices : 1\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       0        0        2      removed\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "The contents of the volume are still accessible (and, if it is mounted, the applications don't notice a thing), but the data safety isn't assured anymore: should the <filename>sdd</filename> disk fail in turn, the data would be lost. We want to avoid that risk, so we'll replace the failed disk with a new one, <filename>sdf</filename>:"
msgstr "Le contenu du volume reste accessible (et, s'il est monté, les applications ne s'aperçoivent de rien), mais la sécurité des données n'est plus assurée : si le disque <filename>sdd</filename> venait à tomber en panne, les données seraient perdues. Pour éviter ce risque, nous allons remplacer ce disque par un disque neuf, <filename>sdf</filename> :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
#| "<computeroutput>mdadm: added /dev/sdf\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 3\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:48:49 2015\n"
#| "          State : clean, degraded, recovering \n"
#| " Active Devices : 1\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 1\n"
#| "\n"
#| " Rebuild Status : 28% complete\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 26\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      spare rebuilding   /dev/sdf\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Update Time : Wed May  6 09:49:08 2015\n"
#| "          State : clean \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 41\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      active sync   /dev/sdf\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:25:34 2022\n"
"             State : clean, degraded, recovering \n"
"    Active Devices : 1\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 1\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 47% complete\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 39\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:25:34 2022\n"
"             State : clean\n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      active sync   /dev/sdf\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"   Raid Devices : 2\n"
"  Total Devices : 3\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:48:49 2015\n"
"          State : clean, degraded, recovering \n"
" Active Devices : 1\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 1\n"
"\n"
" Rebuild Status : 28% complete\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:49:08 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "Here again, the kernel automatically triggers a reconstruction phase during which the volume, although still accessible, is in a degraded mode. Once the reconstruction is over, the RAID array is back to a normal state. One can then tell the system that the <filename>sde</filename> disk is about to be removed from the array, so as to end up with a classical RAID mirror on two disks:"
msgstr "Ici encore, nous avons une phase de reconstruction, déclenchée automatiquement, pendant laquelle le volume, bien qu'accessible, reste en mode dégradé. Une fois qu'elle est terminée, le RAID revient dans son état normal. On peut alors signaler au système que l'on va retirer le disque <filename>sde</filename> de l'ensemble, pour se retrouver avec un miroir classique sur deux disques :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      active sync   /dev/sdf</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde\n"
"</userinput><computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      active sync   /dev/sdf\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf</computeroutput>"

msgid "From then on, the drive can be physically removed when the server is next switched off, or even hot-removed when the hardware configuration allows hot-swap. Such configurations include some SCSI controllers, most SATA disks, and external drives operating on USB or Firewire."
msgstr "Le disque pourra alors être démonté physiquement lors d'une extinction de la machine. Dans certaines configurations matérielles, les disques peuvent même être remplacés à chaud, ce qui permet de se passer de cette extinction. Parmi ces configurations, on trouvera certains contrôleurs SCSI, la plupart des systèmes SATA et les disques externes sur bus USB ou Firewire."

msgid "Backing up the Configuration"
msgstr "Sauvegarde de la configuration"

#, fuzzy
#| msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. In reality this leads to the RAID starting from the individual disks alternately - distributing the data also alternately, depending on which disk started the RAID in degraded mode Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgstr "La plupart des métadonnées concernant les volumes RAID sont sauvegardées directement sur les disques qui les composent, de sorte que le noyau peut détecter les différents ensembles avec leurs composants et les assembler automatiquement lors du démarrage du système. Cela dit, il convient de sauvegarder cette configuration, car cette détection n'est pas infaillible et aura tendance à faillir précisément en période sensible. Si dans notre exemple la panne du disque <filename>sde</filename> était réelle, et si on redémarrait le système sans le retirer, ce disque pourrait, à la faveur du redémarrage, « retomber en marche ». Le noyau aurait alors trois éléments physiques, chacun prétendant représenter la moitié du même volume RAID. Une autre source de confusion peut subvenir si l'on consolide des volumes RAID de deux serveurs sur un seul. Si ces ensembles étaient en fonctionnement normal avant le déplacement des disques, le noyau saura reconstituer les paires correctement. Mais pour peu que les disques déplacés soient agrégés en un <filename>/dev/md1</filename> et qu'il existe également un <filename>md1</filename> sur le serveur consolidé, l'un des miroirs sera contraint de changer de nom."

msgid "Backing up the configuration is therefore important, if only for reference. The standard way to do it is by editing the <filename>/etc/mdadm/mdadm.conf</filename> file, an example of which is listed here:"
msgstr "Il est donc important de sauvegarder la configuration, ne serait-ce qu'à des fins de référence. Pour cela, on éditera le fichier <filename>/etc/mdadm/mdadm.conf</filename>, dont un exemple est donné ci-dessous:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/mdadm/mdadm.conf</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "<command>mdadm</command> configuration file"
msgstr "Fichier de configuration de mdadm"

#, fuzzy
#| msgid ""
#| "# mdadm.conf\n"
#| "#\n"
#| "# Please refer to mdadm.conf(5) for information about this file.\n"
#| "#\n"
#| "\n"
#| "# by default (built-in), scan all partitions (/proc/partitions) and all\n"
#| "# containers for MD superblocks. alternatively, specify devices to scan, using\n"
#| "# wildcards if desired.\n"
#| "DEVICE /dev/sd*\n"
#| "\n"
#| "# auto-create devices with Debian standard permissions\n"
#| "CREATE owner=root group=disk mode=0660 auto=yes\n"
#| "\n"
#| "# automatically tag new arrays as belonging to the local system\n"
#| "HOMEHOST &lt;system&gt;\n"
#| "\n"
#| "# instruct the monitoring daemon where to send mail alerts\n"
#| "MAILADDR root\n"
#| "\n"
#| "# definitions of existing MD arrays\n"
#| "ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "\n"
#| "# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100\n"
#| "# by mkconf 3.2.5-3"
msgid ""
"<![CDATA[# mdadm.conf\n"
"#\n"
"# !NB! Run update-initramfs -u after updating this file.\n"
"# !NB! This will ensure that initramfs has an uptodate copy.\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST <system>\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md/0  metadata=1.2 UUID=a75ac628:b384c441:157137ac:c04cd98c name=debian:0\n"
"ARRAY /dev/md/1  metadata=1.2 UUID=2dfb7fd5:e09e0527:0b5a905a:8334adb8 name=debian:1\n"
"# This configuration was auto-generated on Mon, 28 Feb 2022 01:53:48 +0100 by mkconf\n"
"]]>"
msgstr ""
"# mdadm.conf\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# auto-create devices with Debian standard permissions\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
"\n"
"# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100\n"
"# by mkconf 3.2.5-3"

msgid "One of the most useful details is the <literal>DEVICE</literal> option, which lists the devices where the system will automatically look for components of RAID volumes at start-up time. In our example, we replaced the default value, <literal>partitions containers</literal>, with an explicit list of device files, since we chose to use entire disks and not only partitions, for some volumes."
msgstr "Une des informations les plus souvent utiles est l'option <literal>DEVICE</literal>, qui spécifie l'ensemble des périphériques sur lesquels le système va chercher automatiquement des composants de volumes RAID au démarrage. Nous avons ici remplacé la valeur implicite, <literal>partitions containers</literal>, par une liste explicite de fichiers de périphérique ; nous avons en effet choisi d'utiliser des disques entiers, et non simplement des partitions, pour certains volumes."

msgid "The last two lines in our example are those allowing the kernel to safely pick which volume number to assign to which array. The metadata stored on the disks themselves are enough to re-assemble the volumes, but not to determine the volume number (and the matching <filename>/dev/md*</filename> device name)."
msgstr "Les deux dernières lignes de notre exemple sont celles qui permettent au noyau de choisir en toute sécurité quel numéro de volume associer à quel ensemble. Les méta-informations stockées sur les disques sont en effet suffisantes pour reconstituer les volumes, mais pas pour en déterminer le numéro (donc le périphérique <filename>/dev/md*</filename> correspondant)."

msgid "Fortunately, these lines can be generated automatically:"
msgstr "Fort heureusement, ces lignes peuvent être générées automatiquement :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
#| "<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?\n"
"</userinput><computeroutput>ARRAY /dev/md/0  metadata=1.2 UUID=a75ac628:b384c441:157137ac:c04cd98c name=debian:0\n"
"ARRAY /dev/md/1  metadata=1.2 UUID=2dfb7fd5:e09e0527:0b5a905a:8334adb8 name=debian:1\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"

msgid "The contents of these last two lines doesn't depend on the list of disks included in the volume. It is therefore not necessary to regenerate these lines when replacing a failed disk with a new one. On the other hand, care must be taken to update the file when creating or deleting a RAID array."
msgstr "Le contenu de ces deux dernières lignes ne dépend pas de la liste des disques qui composent les volumes. On pourra donc se passer de les régénérer si l'on remplace un disque défectueux par un neuf. En revanche, il faudra prendre soin de les mettre à jour après chaque création ou suppression de volume."

msgid "LVM, the <emphasis>Logical Volume Manager</emphasis>, is another approach to abstracting logical volumes from their physical supports, which focuses on increasing flexibility rather than increasing reliability. LVM allows changing a logical volume transparently as far as the applications are concerned; for instance, it is possible to add new disks, migrate the data to them, and remove the old disks, without unmounting the volume."
msgstr "LVM, ou <foreignphrase>Logical Volume Manager</foreignphrase>, est une autre approche servant à abstraire les volumes logiques des disques physiques. Le but principal n'était pas ici de gagner en fiabilité des données mais en souplesse d'utilisation. LVM permet en effet de modifier dynamiquement un volume logique, en toute transparence du point de vue des applications. Par exemple, on peut ainsi ajouter de nouveaux disques, migrer les données dessus et récupérer les anciens disques ainsi libérés, sans démonter le volume."

msgid "LVM Concepts"
msgstr "Concepts de LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>concept</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>PV</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Physical Volume</primary><see>PV</see>"
msgstr "<primary>Logical Volume Manager</primary>"

msgid "This flexibility is attained by a level of abstraction involving three concepts."
msgstr "LVM manipule trois types de volumes pour atteindre cette flexibilité."

msgid "First, the PV (<emphasis>Physical Volume</emphasis>) is the entity closest to the hardware: it can be partitions on a disk, or a full disk, or even any other block device (including, for instance, a RAID array). Note that when a physical element is set up to be a PV for LVM, it should only be accessed via LVM, otherwise the system will get confused."
msgstr "Premièrement, les PV ou <foreignphrase>physical volumes</foreignphrase> sont les entités les plus proches du matériel : il peut s'agir de partitions sur des disques ou de disques entiers, voire de n'importe quel périphérique en mode bloc (y compris, par exemple, un volume RAID). Attention, lorsqu'un élément physique est initialisé en PV pour LVM, il ne faudra plus l'utiliser directement, sous peine d'embrouiller le système."

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>VG</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Volume Group</primary><see>VG</see>"
msgstr "<primary>Logical Volume Manager</primary>"

#, fuzzy
#| msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there's no risk of using them directly."
msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there is no risk of using them directly."
msgstr "Les PV sont alors regroupés en VG <foreignphrase>(volume groups)</foreignphrase>, que l'on peut considérer comme des disques virtuels (et extensibles, comme on le verra). Les VG sont abstraits et ne disposent pas de fichier spécial dans <filename>/dev/</filename>, aucun risque donc de les utiliser directement."

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>LV</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Logical Volume</primary><see>LV</see>"
msgstr "<primary>Logical Volume Manager</primary>"

msgid "The third kind of object is the LV (<emphasis>Logical Volume</emphasis>), which is a chunk of a VG; if we keep the VG-as-disk analogy, the LV compares to a partition. The LV appears as a block device with an entry in <filename>/dev</filename>, and it can be used as any other physical partition can be (most commonly, to host a filesystem or swap space)."
msgstr "Enfin, les LV <foreignphrase>(logical volumes)</foreignphrase> sont des subdivisions des VG, que l'on peut comparer à des partitions sur les disques virtuels que les VG représentent. Ces LV deviennent des périphériques, que l'on peut utiliser comme toute partition physique (par exemple pour y établir des systèmes de fichiers)."

msgid "The important thing is that the splitting of a VG into LVs is entirely independent of its physical components (the PVs). A VG with only a single physical component (a disk for instance) can be split into a dozen logical volumes; similarly, a VG can use several physical disks and appear as a single large logical volume. The only constraint, obviously, is that the total size allocated to LVs can't be bigger than the total capacity of the PVs in the volume group."
msgstr "Il faut bien réaliser que la subdivision d'un groupe de volumes en LV est entièrement décorrélée de sa composition physique (les PV). On peut ainsi avoir un VG subdivisé en une douzaine de volumes logiques tout en ne comportant qu'un volume physique (un disque, par exemple), ou au contraire un seul gros volume logique réparti sur plusieurs disques physiques ou partitions. La seule contrainte, bien entendu, est que la somme des tailles des LV d'un groupe ne doive pas excéder la capacité totale des PV qui le composent."

# You can also have a volume group that is on a RAID0 device. This is excellent for temporary files or swap space that is needed for high performance applications.
msgid "It often makes sense, however, to have some kind of homogeneity among the physical components of a VG, and to split the VG into logical volumes that will have similar usage patterns. For instance, if the available hardware includes fast disks and slower disks, the fast ones could be clustered into one VG and the slower ones into another; chunks of the first one can then be assigned to applications requiring fast data access, while the second one will be kept for less demanding tasks."
msgstr "En revanche, il est souvent utile de grouper dans un même VG des éléments physiques présentant des caractéristiques similaires et de subdiviser ce VG en volumes logiques qui seront utilisés de manière comparable également. Par exemple, si l'on dispose de disques rapides et de disques lents, on pourra regrouper les rapides dans un VG et les plus lents dans un autre. Les subdivisions logiques du premier VG pourront alors être affectées à des tâches nécessitant de bonnes performances, celles du second étant réservées aux tâches qui peuvent se contenter de vitesses médiocres."

msgid "In any case, keep in mind that an LV isn't particularly attached to any one PV. It is possible to influence where the data from an LV are physically stored, but this possibility isn't required for day-to-day use. On the contrary: when the set of physical components of a VG evolves, the physical storage locations corresponding to a particular LV can be migrated across disks (while staying within the PVs assigned to the VG, of course)."
msgstr "Dans tous les cas, il faut également garder à l'esprit qu'un LV n'est pas accroché à un PV ou un autre. Même si on peut influencer l'emplacement physique où les données d'un LV sont écrites, en fonctionnement normal c'est une information qui n'a pas d'intérêt particulier. Au contraire : lors d'une modification des composants physiques d'un groupe, les LV peuvent être amenés à se déplacer (tout en restant, bien entendu, confinés aux PV qui composent ce groupe)."

msgid "Setting up LVM"
msgstr "Mise en place de LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>setup</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Let us now follow, step by step, the process of setting up LVM for a typical use case: we want to simplify a complex storage situation. Such a situation usually happens after some long and convoluted history of accumulated temporary measures. For the purposes of illustration, we'll consider a server where the storage needs have changed over time, ending up in a maze of available partitions split over several partially used disks. In more concrete terms, the following partitions are available:"
msgstr "Nous allons suivre pas à pas une utilisation typique de LVM, pour simplifier une situation complexe. De telles situations sont souvent le résultat d'un historique chargé, où des solutions temporaires se sont accumulées au fil du temps. Considérons donc pour notre exemple un serveur dont les besoins en stockage ont varié et pour lequel on se retrouve avec une configuration complexe de partitions disponibles, morcelées sur différents disques hétéroclites et partiellement utilisés. Concrètement, on dispose des partitions suivantes :"

msgid "on the <filename>sdb</filename> disk, a <filename>sdb2</filename> partition, 4 GB;"
msgstr "sur le disque <filename>sdb</filename>, une partition <filename>sdb2</filename> de 4 Go ;"

msgid "on the <filename>sdc</filename> disk, a <filename>sdc3</filename> partition, 3 GB;"
msgstr "sur le disque <filename>sdc</filename>, une partition <filename>sdc3</filename> de 3 Go ;"

msgid "the <filename>sdd</filename> disk, 4 GB, is fully available;"
msgstr "le disque <filename>sdd</filename>, de 4 Go, est entièrement disponible ;"

msgid "on the <filename>sdf</filename> disk, a <filename>sdf1</filename> partition, 4 GB; and a <filename>sdf2</filename> partition, 5 GB."
msgstr "sur le disque <filename>sdf</filename>, une partition <filename>sdf1</filename> de 4 Go et une <filename>sdf2</filename> de 5 Go."

msgid "In addition, let's assume that disks <filename>sdb</filename> and <filename>sdf</filename> are faster than the other two."
msgstr "On notera de plus que les disques <filename>sdb</filename> et <filename>sdf</filename> ont de meilleures performances que les deux autres."

msgid "Our goal is to set up three logical volumes for three different applications: a file server requiring 5 GB of storage space, a database (1 GB) and some space for back-ups (12 GB). The first two need good performance, but back-ups are less critical in terms of access speed. All these constraints prevent the use of partitions on their own; using LVM can abstract the physical size of the devices, so the only limit is the total available space."
msgstr "Le but de la manœuvre est de mettre en place trois volumes logiques distincts, pour trois applications séparées : un serveur de fichiers (qui nécessite 5 Go), une base de données (1 Go) et un emplacement pour les sauvegardes (12 Go). Les deux premières ont de forts besoins de performance, mais pas la troisième, qui est moins critique. Ces contraintes empêchent l'utilisation des partitions isolément ; l'utilisation de LVM permet de s'affranchir des limites imposées par leurs tailles individuelles, pour n'être limité que par leur capacité totale."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">lvm2</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create PV</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The required tools are in the <emphasis role=\"pkg\">lvm2</emphasis> package and its dependencies. When they're installed, setting up LVM takes three steps, matching the three levels of concepts."
msgstr "Le prérequis est le paquet <emphasis role=\"pkg\">lvm2</emphasis> (et ses dépendances). Lorsque ce paquet est installé, la mise en place de LVM se fait en trois étapes, correspondant aux trois couches de LVM."

msgid "First, we prepare the physical volumes using <command>pvcreate</command>:"
msgstr "Commençons par préparer les volumes physiques à l'aide de <command>pvcreate</command> :"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>pvcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>pvdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
#| "# </computeroutput><userinput>pvdisplay</userinput>\n"
#| "<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
#| "  --- NEW Physical volume ---\n"
#| "  PV Name               /dev/sdb2\n"
#| "  VG Name               \n"
#| "  PV Size               4.00 GiB\n"
#| "  Allocatable           NO\n"
#| "  PE Size               0   \n"
#| "  Total PE              0\n"
#| "  Free PE               0\n"
#| "  Allocated PE          0\n"
#| "  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
#| "\n"
#| "# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
#| "  Physical volume \"/dev/sdd\" successfully created\n"
#| "  Physical volume \"/dev/sdf1\" successfully created\n"
#| "  Physical volume \"/dev/sdf2\" successfully created\n"
#| "# </computeroutput><userinput>pvdisplay -C</userinput>\n"
#| "<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
#| "  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
#| "  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
#| "  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
#| "  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
#| "  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay\n"
"</userinput><computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               yK0K6K-clbc-wt6e-qk9o-aUh9-oQqC-k1T71B\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n"
"  Physical volume \"/dev/sdd\" successfully created.\n"
"  Physical volume \"/dev/sdf1\" successfully created.\n"
"  Physical volume \"/dev/sdf2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay -C\n"
"</userinput><computeroutput>  PV         VG Fmt  Attr PSize PFree\n"
"  /dev/sdb2     lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3     lvm2 ---  3.00g 3.00g\n"
"  /dev/sdd      lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1     lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf2     lvm2 ---  5.00g 5.00g\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
"  Physical volume \"/dev/sdd\" successfully created\n"
"  Physical volume \"/dev/sdf1\" successfully created\n"
"  Physical volume \"/dev/sdf2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay -C</userinput>\n"
"<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
"  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
"  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
"  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
"</computeroutput>"

msgid "So far, so good; note that a PV can be set up on a full disk as well as on individual partitions of it. As shown above, the <command>pvdisplay</command> command lists the existing PVs, with two possible output formats."
msgstr "Rien de bien sorcier jusqu'à présent ; on remarquera que l'on peut établir un PV aussi bien sur un disque entier que sur des partitions. Comme on le constate, la commande <command>pvdisplay</command> est capable de lister les PV déjà établis, sous deux formes."

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>vgcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>vgdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "Now let's assemble these physical elements into VGs using <command>vgcreate</command>. We'll gather only PVs from the fast disks into a <filename>vg_critical</filename> VG; the other VG, <filename>vg_normal</filename>, will also include slower elements."
msgstr "Constituons à présent des groupes de volumes (VG) à partir de ces éléments physiques, à l'aide de la commande <command>vgcreate</command>. Nous allons placer dans le VG <filename>vg_critique</filename> uniquement des PV appartenant à des disques rapides ; le deuxième VG, <filename>vg_normal</filename>, contiendra des éléments physiques plus lents."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create VG</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
#| "<computeroutput>  No volume groups found\n"
#| "# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay</userinput>\n"
#| "<computeroutput>  --- Volume group ---\n"
#| "  VG Name               vg_critical\n"
#| "  System ID             \n"
#| "  Format                lvm2\n"
#| "  Metadata Areas        2\n"
#| "  Metadata Sequence No  1\n"
#| "  VG Access             read/write\n"
#| "  VG Status             resizable\n"
#| "  MAX LV                0\n"
#| "  Cur LV                0\n"
#| "  Open LV               0\n"
#| "  Max PV                0\n"
#| "  Cur PV                2\n"
#| "  Act PV                2\n"
#| "  VG Size               8.09 GiB\n"
#| "  PE Size               4.00 MiB\n"
#| "  Total PE              2071\n"
#| "  Alloc PE / Size       0 / 0   \n"
#| "  Free  PE / Size       2071 / 8.09 GiB\n"
#| "  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
#| "\n"
#| "# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
#| "<computeroutput>  Volume group \"vg_normal\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay -C</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
#| "  vg_critical   2   0   0 wz--n-  8.09g  8.09g\n"
#| "  vg_normal     3   0   0 wz--n- 12.30g 12.30g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1\n"
"</userinput><computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay\n"
"</userinput><computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               7.99 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2046\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2046 / 7.99 GiB\n"
"  VG UUID               JgFWU3-emKg-9QA1-stPj-FkGX-mGFb-4kzy1G\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2\n"
"</userinput><computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C\n"
"</userinput><computeroutput><![CDATA[  VG          #PV #LV #SN Attr   VSize   VFree  \n"
"  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n"
"  vg_normal     3   0   0 wz--n- <11.99g <11.99g\n"
"]]></computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  No volume groups found\n"
"# </computeroutput><userinput>vgcreate vg_critique /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critique\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critique\n"
"  System ID\n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               8,09 GiB\n"
"  PE Size               4,00 MiB\n"
"  Total PE              2071\n"
"  Alloc PE / Size       0 / 0\n"
"  Free  PE / Size       2071 / 8,09 GiB\n"
"  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree\n"
"  vg_critique   2   0   0 wz--n-  8,09g  8,09g\n"
"  vg_normal     3   0   0 wz--n- 12,30g 12,30g\n"
"</computeroutput>"

msgid "Here again, commands are rather straightforward (and <command>vgdisplay</command> proposes two output formats). Note that it is quite possible to use two partitions of the same physical disk into two different VGs. Note also that we used a <filename>vg_</filename> prefix to name our VGs, but it is nothing more than a convention."
msgstr "Ici encore, les commandes sont relativement simples (et <command>vgdisplay</command> présente deux formats de sortie). Notons que rien n'empêche de placer deux partitions d'un même disque physique dans deux VG différents ; le préfixe <filename>vg_</filename> utilisé ici est une convention, mais n'est pas obligatoire."

#, fuzzy
#| msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB, respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgstr "Nous disposons maintenant de deux « disques virtuels », respectivement d'environ 8 Go et 12 Go. Nous pouvons donc les subdiviser en « partitions virtuelles » (des LV). Cette opération passe par la commande <command>lvcreate</command>, dont la syntaxe est un peu plus complexe :"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create LV</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>lvcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>lvdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_files\" created\n"
#| "# </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput>  --- Logical volume ---\n"
#| "  LV Path                /dev/vg_critical/lv_files\n"
#| "  LV Name                lv_files\n"
#| "  VG Name                vg_critical\n"
#| "  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
#| "  LV Write Access        read/write\n"
#| "  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
#| "  LV Status              available\n"
#| "  # open                 0\n"
#| "  LV Size                5.00 GiB\n"
#| "  Current LE             1280\n"
#| "  Segments               2\n"
#| "  Allocation             inherit\n"
#| "  Read ahead sectors     auto\n"
#| "  - currently set to     256\n"
#| "  Block device           253:0\n"
#| "\n"
#| "# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_base\" created\n"
#| "# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_backups\" created\n"
#| "# </computeroutput><userinput>lvdisplay -C</userinput>\n"
#| "<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_base    vg_critical -wi-a---  1.00g                                           \n"
#| "  lv_files   vg_critical -wi-a---  5.00g                                           \n"
#| "  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>lvdisplay\n"
"</userinput><computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical\n"
"</userinput><computeroutput>  Logical volume \"lv_files\" created.\n"
"# </computeroutput><userinput>lvdisplay\n"
"</userinput><computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                Nr62xe-Zu7d-0u3z-Yyyp-7Cj1-Ej2t-gw04Xd\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time debian, 2022-03-01 00:17:46 +0100\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical\n"
"</userinput><computeroutput>  Logical volume \"lv_base\" created.\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal\n"
"</userinput><computeroutput>  Rounding up size to full physical extent 11.98 GiB\n"
"  Rounding up size to full physical extent 11.98 GiB\n"
"  Logical volume \"lv_backups\" created.\n"
"# </computeroutput><userinput>lvdisplay -C\n"
"</userinput><computeroutput>  LV         VG          Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_base    vg_critical -wi-a-----  1.00g                                                    \n"
"  lv_files   vg_critical -wi-a-----  5.00g                                                    \n"
"  lv_backups vg_normal   -wi-a----- 11.98g             \n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
"<computeroutput>  Logical volume \"lv_backups\" created\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"

msgid "Two parameters are required when creating logical volumes; they must be passed to the <command>lvcreate</command> as options. The name of the LV to be created is specified with the <literal>-n</literal> option, and its size is generally given using the <literal>-L</literal> option. We also need to tell the command what VG to operate on, of course, hence the last parameter on the command line."
msgstr "Deux informations sont obligatoires lors de la création des volumes logiques et doivent être passées sous forme d'options à <command>lvcreate</command>. Le nom du LV à créer est spécifié par l'option <literal>-n</literal> et sa taille est en général spécifiée par <literal>-L</literal>. Évidemment, il faut également expliciter à l'intérieur de quel groupe de volumes on souhaite créer le LV, d'où le dernier paramètre de la ligne de commande."

msgid "<emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> options"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Options de <command>lvcreate</command>"

msgid "The <command>lvcreate</command> command has several options to allow tweaking how the LV is created."
msgstr "La commande <command>lvcreate</command> propose plusieurs options influençant la manière dont le LV est créé."

msgid "Let's first describe the <literal>-l</literal> option, with which the LV's size can be given as a number of blocks (as opposed to the “human” units we used above). These blocks (called PEs, <emphasis>physical extents</emphasis>, in LVM terms) are contiguous units of storage space in PVs, and they can't be split across LVs. When one wants to define storage space for an LV with some precision, for instance to use the full available space, the <literal>-l</literal> option will probably be preferred over <literal>-L</literal>."
msgstr "Notons tout d'abord l'existence de l'option <literal>-l</literal>, qui permet de spécifier la taille du LV à créer non pas en unités « humaines » comme nous l'avons fait dans nos exemples, mais en nombre de blocs. Ces blocs (appelés <acronym>PE</acronym>, pour <foreignphrase>physical extents</foreignphrase>) sont des unités contiguës de stockage, qui font partie des PV et qui ne sont pas divisibles entre LV. Si l'on souhaite définir précisément l'espace alloué à un LV, par exemple pour utiliser totalement l'espace disponible sur un VG, on pourra préférer utiliser <literal>-l</literal> plutôt que <literal>-L</literal>."

#, fuzzy
#| msgid "It's also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgid "It is also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgstr "Il est également possible de spécifier qu'un LV devra être physiquement stocké sur un PV plutôt qu'un autre (tout en restant dans les PV affectés au groupe, bien entendu). Ainsi, si l'on sait que le disque <filename>sdb</filename> est plus rapide que <filename>sdf</filename>, on pourra placer le LV <filename>lv_base</filename> dessus (donc privilégier les performances de la base de données par rapport à celles du serveur de fichiers). La ligne de commande deviendra alors : <command>lvcreate -n lv_base -L 1G vg_critique /dev/sdb2</command>. Il est à noter que cette commande peut échouer si le PV spécifié n'a plus assez de blocs libres pour contenir le LV demandé. Dans notre exemple, il faudrait probablement créer <filename>lv_base</filename> avant <filename>lv_fichiers</filename> pour éviter cet inconvénient — ou libérer de l'espace sur <filename>sdb2</filename> grâce à la commande <command>pvmove</command>."

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>pvmove</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "Logical volumes, once created, end up as block device files in <filename>/dev/mapper/</filename>:"
msgstr "Les volumes logiques, une fois créés, sont représentés par des fichiers de périphériques situés dans <filename>/dev/mapper/</filename> :"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/dev</filename></primary><secondary><filename>/dev/mapper/</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>device</primary><secondary>block</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
#| "<computeroutput>total 0\n"
#| "crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
#| "# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
#| "<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
#| "brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
#| "brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"crw------- 1 root root 10, 236 Mar  1 00:17 control\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:19 vg_critical-lv_base -> ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:17 vg_critical-lv_files -> ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:19 vg_normal-lv_backups -> ../dm-2 ]]>\n"
"# </computeroutput><userinput>ls -l /dev/dm-*\n"
"</userinput><computeroutput>brw-rw---- 1 root disk 253, 0 Mar  1 00:17 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Mar  1 00:19 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Mar  1 00:19 /dev/dm-2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"

#, fuzzy
#| msgid "<emphasis>NOTE</emphasis> Autodetecting LVM volumes"
msgid "<emphasis>NOTE</emphasis> Auto-detecting LVM volumes"
msgstr "<emphasis>NOTE</emphasis> Détection automatique des volumes LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>lvm2-activation.service</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>vgchange</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "When the computer boots, the <filename>lvm2-activation</filename> systemd service unit executes <command>vgchange -aay</command> to “activate” the volume groups: it scans the available devices; those that have been initialized as physical volumes for LVM are registered into the LVM subsystem, those that belong to volume groups are assembled, and the relevant logical volumes are started and made available. There is therefore no need to edit configuration files when creating or modifying LVM volumes."
msgstr "Lors du démarrage de l'ordinateur, le service systemd <filename>lvm2-activation</filename> lance la commande <command>vgchange -aay</command> pour activer les groupes de volumes. Pour cela, il effectue un balayage des périphériques disponibles ; ceux qui ont été préparés comme des volumes physiques pour LVM sont alors répertoriés auprès du sous-système LVM, ceux qui font partie de groupes de volumes sont assemblés et les volumes logiques sont mis en fonctionnement. Il n'est donc pas nécessaire de modifier des fichiers de configuration lors de la création ou de l'altération de volumes LVM."

msgid "Note, however, that the layout of the LVM elements (physical and logical volumes, and volume groups) is backed up in <filename>/etc/lvm/backup</filename>, which can be useful in case of a problem (or just to sneak a peek under the hood)."
msgstr "On notera cependant que la disposition des divers éléments impliqués dans LVM est stockée dans <filename>/etc/lvm/backup</filename>, ce qui peut servir en cas de problème, ou bien pour aller voir ce qui se passe sous le capot."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/lvm/backup</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "To make things easier, convenience symbolic links are also created in directories matching the VGs:"
msgstr "Pour faciliter les choses, des liens symboliques sont également créés automatiquement dans des répertoires correspondant aux VG :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
#| "<computeroutput>total 0\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
#| "# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
#| "<computeroutput>total 0\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:19 lv_base -> ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:17 lv_files -> ../dm-0 ]]>\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:19 lv_backups -> ../dm-2 ]]>\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"

msgid "The LVs can then be used exactly like standard partitions:"
msgstr "On peut dès lors utiliser les LV tout comme on utiliserait des partitions classiques :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
#| "<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
#| "Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
#| "Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
#| "[...]\n"
#| "Creating journal (32768 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
#| "<computeroutput>[...]\n"
#| "/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
#| "/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
#| "/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups\n"
"</userinput><computeroutput>mke2fs 1.46.2 (28-Feb-2021)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 3140608 4k blocks and 786432 inodes\n"
"Filesystem UUID: 7eaf0340-b740-421e-96b2-942cdbf29cb3\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/backups\n"
"</userinput><computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups\n"
"</userinput><computeroutput># </computeroutput><userinput>df -h /srv/backups\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   24K   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab\n"
"</userinput><computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
"Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
"[...]\n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"

msgid "From the applications' point of view, the myriad small partitions have now been abstracted into one large 12 GB volume, with a friendlier name."
msgstr "On s'est ainsi abstrait, du point de vue applicatif, de la myriade de petites partitions, pour se retrouver avec une seule partition de 12 Go."

msgid "LVM Over Time"
msgstr "LVM au fil du temps"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>lvresize</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>resize2fs</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>resize</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>resize LV</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Even though the ability to aggregate partitions or physical disks is convenient, this is not the main advantage brought by LVM. The flexibility it brings is especially noticed as time passes, when needs evolve. In our example, let's assume that new large files must be stored, and that the LV dedicated to the file server is too small to contain them. Since we haven't used the whole space available in <filename>vg_critical</filename>, we can grow <filename>lv_files</filename>. For that purpose, we'll use the <command>lvresize</command> command, then <command>resize2fs</command> to adapt the filesystem accordingly:"
msgstr "Cette capacité à agréger des partitions ou des disques physiques n'est pas le principal attrait de LVM. La souplesse offerte se manifeste surtout au fil du temps, lorsque les besoins évoluent. Supposons par exemple que de nouveaux fichiers volumineux doivent être stockés et que le LV dévolu au serveur de fichiers ne suffise plus. Comme nous n'avons pas utilisé l'intégralité de l'espace disponible dans <filename>vg_critique</filename>, nous pouvons étendre <filename>lv_fichiers</filename>. Nous allons pour cela utiliser la commande <command>lvresize</command> pour étendre le LV, puis <command>resize2fs</command> pour ajuster le système de fichiers en conséquence :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 5.00g\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
#| "# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
#| "  Logical volume lv_files successfully resized\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 7.00g\n"
#| "# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
#| "<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
#| "Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
#| "old_desc_blocks = 1, new_desc_blocks = 1\n"
#| "The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
#| "\n"
#| "# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  4.9G  4.2G  485M  90% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files\n"
"</userinput><computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 5.00g                                                    \n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 7.99g 1.99g\n"
"# </computeroutput><userinput>lvresize -L 6G vg_critical/lv_files\n"
"</userinput><computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\n"
"  Logical volume vg_critical/lv_files successfully resized.\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files\n"
"</userinput><computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 6.00g                                                    \n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files\n"
"</userinput><computeroutput>resize2fs 1.46.2 (28-Feb-2021)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1572864 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.9G  4.2G  1.5G  75% /srv/files\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
"# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
"  Logical volume lv_files successfully resized\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 7.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"

msgid "<emphasis>CAUTION</emphasis> Resizing filesystems"
msgstr "<emphasis>ATTENTION</emphasis> Retaillage de systèmes de fichiers"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>filesystem</primary><secondary>resize</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It's rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It is rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgstr "En l'état actuel des choses, tous les systèmes de fichiers ne peuvent pas être retaillés en ligne et le retaillage d'un volume peut donc nécessiter de démonter le système de fichiers au préalable et le remonter par la suite. Bien entendu, si l'on souhaite réduire l'espace occupé par un LV, il faudra d'abord réduire le système de fichiers ; lors d'un élargissement, le volume logique devra être étendu avant le système de fichiers. C'est somme toute fort logique : la taille du système de fichiers ne doit à aucun moment dépasser celle du périphérique bloc sur laquelle il repose, qu'il s'agisse d'une partition physique ou d'un volume logique."

msgid "The ext3, ext4 and xfs filesystems can be grown online, without unmounting; shrinking requires an unmount. The reiserfs filesystem allows online resizing in both directions. The venerable ext2 allows neither, and always requires unmounting."
msgstr "Dans le cas du système ext3 ou ext4, on peut procéder à l'extension (mais pas la réduction) du système sans le démonter. Le système xfs est dans le même cas. reiserfs permet le retaillage dans les deux sens. ext2 n'en permet aucun et nécessite toujours un démontage."

msgid "We could proceed in a similar fashion to extend the volume hosting the database, only we've reached the VG's available space limit:"
msgstr "On pourrait procéder de même pour étendre le volume qui héberge la base de données, mais on arrive à la limite de la capacité du VG :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/\n"
"</userinput><computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  974M  883M   25M  98% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput>  VG          #PV #LV #SN Attr   VSize VFree   \n"
"  vg_critical   2   2   0 wz--n- 7.99g 1016.00m\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Sys. de fichiers      Tail. Uti. Disp. Uti% Monté sur\n"
"/dev/mapper/vg_critique-lv_base\n"
"                     1008M  854M  104M  90% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critique</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critique   2   2   0 wz--n- 8,09g 92,00m</computeroutput>"

#, fuzzy
#| msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb1</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb3</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgstr "Qu'à cela ne tienne, LVM permet également d'ajouter des volumes physiques à des groupes de volumes existants. Par exemple, on a pu s'apercevoir que la partition <filename>sdb1</filename>, qui jusqu'à présent était utilisée en dehors de LVM, contenait uniquement des archives qui ont pu être déplacées dans <filename>lv_sauvegardes</filename>. On peut donc l'intégrer au groupe de volumes pour récupérer l'espace disponible. Ceci passe par la commande <command>vgextend</command>. Il faut bien entendu préparer la partition comme un volume physique au préalable. Une fois le VG étendu, on peut suivre le même cheminement que précédemment pour étendre le système de fichiers:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
#| "# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb3\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdb3\" successfully created.\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb3\n"
"</userinput><computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput><![CDATA[  VG          #PV #LV #SN Attr   VSize   VFree \n"
"  vg_critical   3   2   0 wz--n- <12.99g <5.99g ]]>\n"
"# </computeroutput><userinput>lvresize -L 2G vg_critical/lv_base\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_base\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/\n"
"</userinput><computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  886M  991M  48% /srv/base\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"

msgid "<emphasis>GOING FURTHER</emphasis> Advanced LVM"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> LVM avancé"

#, fuzzy
#| msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance, to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgstr "Il existe des utilisations plus avancées de LVM, dans lesquelles de nombreux détails peuvent être spécifiés. On peut par exemple influer sur la taille des blocs composant les volumes logiques sur les volumes physiques et leur disposition. Il est également possible de déplacer ces blocs entre les PV. Ce peut être pour jouer sur la performance, ou plus prosaïquement pour vider un PV lorsqu'on souhaite sortir le disque correspondant du groupe de volumes (par exemple pour l'affecter à un autre VG, ou le sortir complètement de LVM). Les pages de manuel des différentes commandes, bien que non traduites en français, sont généralement claires. Un bon point d'entrée est la page <citerefentry><refentrytitle>lvm</refentrytitle><manvolnum>8</manvolnum></citerefentry>."

msgid "RAID or LVM?"
msgstr "RAID ou LVM ?"

msgid "RAID and LVM both bring indisputable advantages as soon as one leaves the simple case of a desktop computer with a single hard disk where the usage pattern doesn't change over time. However, RAID and LVM go in two different directions, with diverging goals, and it is legitimate to wonder which one should be adopted. The most appropriate answer will of course depend on current and foreseeable requirements."
msgstr "Les apports de RAID et LVM sont indéniables dès que l'on s'éloigne d'un poste bureautique simple, à un seul disque dur, dont l'utilisation ne change pas dans le temps. Cependant, RAID et LVM constituent deux directions différentes, chacune ayant sa finalité, et l'on peut se demander lequel de ces systèmes adopter. La réponse dépendra des besoins, présents et futurs."

msgid "There are a few simple cases where the question doesn't really arise. If the requirement is to safeguard data against hardware failures, then obviously RAID will be set up on a redundant array of disks, since LVM doesn't really address this problem. If, on the other hand, the need is for a flexible storage scheme where the volumes are made independent of the physical layout of the disks, RAID doesn't help much and LVM will be the natural choice."
msgstr "Dans certains cas simples, la question ne se pose pas vraiment. Si l'on souhaite immuniser des données contre des pannes de matériel, on ne pourra que choisir d'installer du RAID sur un ensemble redondant de disques, puisque LVM ne répond pas à cette problématique. Si au contraire il s'agit d'assouplir un schéma de stockage et de rendre des volumes indépendants de l'agencement des disques qui les composent, RAID ne pourra pas aider et l'on choisira donc LVM."

msgid "<emphasis>NOTE</emphasis> If performance matters…"
msgstr "<emphasis>NOTE</emphasis> Si les performances comptent…"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>SSD</primary>"
msgstr "<primary>RAID</primary>"

msgid "<primary>Solid State Drives</primary><see>SSD</see>"
msgstr ""

#, fuzzy
#| msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgstr "Si les performances des entrées-sorties sont importantes, notamment en termes de temps d'accès, il faut savoir que l'usage de LVM et/ou de RAID peut avoir un impact qui influerea sur votre choix. Toutefois, ces différences de performance sont vraiment mineures et ne seront mesurables que dans quelques cas. Si les performances comptent, le meilleur gain que l'on puisse obtenir viendra de l'usage de disques non rotatifs (<indexterm><primary>SSD</primary></indexterm><foreignphrase>Solid-State Drives</foreignphrase> ou SSD) ; leur coût au mégaoctet est plus élevé que celui des disques durs standards et leur capacité généralement inférieure, mais ils fournissent d'excellentes performances pour des accès aléatoires aux données. Si le cas d'usage inclut de nombreuses lectures/écritures réparties sur tout le système de fichiers, comme pour des bases de données sur lesquelles des requêtes complexes sont fréquemment exécutées, alors le bénéfice apporté par le SSD dépasse largement ce qui peut être gagné en privilégiant LVM sur RAID ou l'inverse. Dans ces situations, le choix doit être déterminé par d'autres considérations que la vitesse pure, puisque la problématique des performances est plus facilement gérée par l'usage de SSD."

msgid "The third notable use case is when one just wants to aggregate two disks into one volume, either for performance reasons or to have a single filesystem that is larger than any of the available disks. This case can be addressed both by a RAID-0 (or even linear-RAID) and by an LVM volume. When in this situation, and barring extra constraints (for instance, keeping in line with the rest of the computers if they only use RAID), the configuration of choice will often be LVM. The initial set up is barely more complex, and that slight increase in complexity more than makes up for the extra flexibility that LVM brings if the requirements change or if new disks need to be added."
msgstr "Un troisième cas est celui où l'on souhaite juste agréger deux disques en un, que ce soit pour des raisons de performance ou pour disposer d'un seul système de fichiers plus gros que chacun des disques dont on dispose. Ce problème peut être résolu aussi bien par la mise en place d'un ensemble RAID-0 (voire linéaire) que par un volume LVM. Dans cette situation, à moins de contraintes spécifiques (par exemple pour rester homogène avec le reste du parc informatique qui n'utilise que RAID), on choisira le plus souvent LVM. La mise en place initiale est légèrement plus complexe, mais elle est un bien maigre investissement au regard de la souplesse offerte par la suite, si les besoins changent ou si l'on désire ajouter des disques."

msgid "Then of course, there is the really interesting use case, where the storage system needs to be made both resistant to hardware failure and flexible when it comes to volume allocation. Neither RAID nor LVM can address both requirements on their own; no matter, this is where we use both at the same time — or rather, one on top of the other. The scheme that has all but become a standard since RAID and LVM have reached maturity is to ensure data redundancy first by grouping disks in a small number of large RAID arrays, and to use these RAID arrays as LVM physical volumes; logical partitions will then be carved from these LVs for filesystems. The selling point of this setup is that when a disk fails, only a small number of RAID arrays will need to be reconstructed, thereby limiting the time spent by the administrator for recovery."
msgstr "Vient enfin le cas le plus intéressant, celui où l'on souhaite concilier de la tolérance de pannes et de la souplesse dans l'allocation des volumes. Chacun des deux systèmes étant insuffisant pour répondre à ces besoins, on va devoir faire appel aux deux en même temps — ou plutôt, l'un après l'autre. L'usage qui se répand de plus en plus depuis la maturité des deux systèmes est d'assurer la sécurité des données d'abord, en groupant des disques redondants dans un petit nombre de volumes RAID de grande taille, et d'utiliser ces volumes RAID comme des éléments physiques pour LVM pour y découper des partitions qui seront utilisées pour des systèmes de fichiers. Ceci permet, en cas de défaillance d'un disque, de n'avoir qu'un petit nombre d'ensembles RAID à reconstruire, donc de limiter le temps d'administration."

#, fuzzy
#| msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited videoclips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited video clips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgstr "Prenons un exemple concret : le département des relations publiques de Falcot SA a besoin d'une station de travail adaptée au montage vidéo. En revanche, les contraintes de budget du département ne permettent pas d'investir dans du matériel neuf entièrement haut de gamme. Le choix est fait de privilégier le matériel spécifiquement graphique (écran et carte vidéo) et de rester dans le générique pour les éléments de stockage. Or la vidéo numérique, comme on le sait, a des besoins en stockage particuliers : les données à stocker sont volumineuses et la vitesse de lecture et d'écriture de ces données est importante pour les performances du système (plus que le temps d'accès moyen, par exemple). Il faut donc répondre à ces contraintes avec du matériel générique, en l'occurrence deux disques durs SATA de 300 Go, tout en garantissant la disponibilité du système et de certaines des données. Les films montés doivent en effet rester disponibles, mais les fichiers bruts non encore montés sont moins critiques, puisque les <foreignphrase>rushes</foreignphrase> restent sur les bandes."

msgid "RAID-1 and LVM are combined to satisfy these constraints. The disks are attached to two different SATA controllers to optimize parallel access and reduce the risk of a simultaneous failure, and they therefore appear as <filename>sda</filename> and <filename>sdc</filename>. They are partitioned identically along the following scheme:"
msgstr "Le choix se porte donc sur une solution combinant RAID-1 et LVM. Les deux disques sont montés sur deux contrôleurs SATA différents (afin d'optimiser les accès parallèles et limiter les risques de panne simultanée) et apparaissent donc comme <filename>sda</filename> et <filename>sdc</filename>. Le schéma de partitionnement, identique sur les deux disques, sera le suivant :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
#| "<computeroutput>\n"
#| "Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
#| "Units: sectors of 1 * 512 = 512 bytes\n"
#| "Sector size (logical/physical): 512 bytes / 512 bytes\n"
#| "I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
#| "Disklabel type: dos\n"
#| "Disk identifier: 0x00039a9f\n"
#| "\n"
#| "Device    Boot     Start       End   Sectors Size Id Type\n"
#| "/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
#| "/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
#| "/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
#| "/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
#| "/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
#| "/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>sfdisk -l /dev/sda\n"
"</userinput><computeroutput>Disk /dev/sda: 894.25 GiB, 960197124096 bytes, 1875385008 sectors\n"
"Disk model: SAMSUNG MZ7LM960\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: gpt\n"
"Disk identifier: BB14C130-9E9A-9A44-9462-6226349CA012\n"
"\n"
"Device         Start        End   Sectors   Size Type\n"
"/dev/sda1        2048       4095      2048     1M BIOS boot\n"
"/dev/sda2        4096  100667391 100663296    48G Linux RAID\n"
"/dev/sda3   100667392  134221823  33554432    16G Linux RAID\n"
"/dev/sda4   134221824  763367423 629145600   300G Linux RAID\n"
"/dev/sda5   763367424 1392513023 629145600   300G Linux RAID\n"
"/dev/sda6  1392513024 1875384974 482871951 230.3G Linux LVM\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"

msgid "The first partitions of both disks are BIOS boot partitions."
msgstr ""

#, fuzzy
#| msgid "The first partitions of both disks (about 1 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgid "The next two partitions <filename>sda2</filename> and <filename>sdc2</filename> (about 48 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgstr "La première partition de chaque disque (environ 1 Go) est utilisée pour assembler un volume RAID-1, <filename>md0</filename>. Ce miroir est utilisé directement pour stocker le système de fichiers racine."

#, fuzzy
#| msgid "The <filename>sda2</filename> and <filename>sdc2</filename> partitions are used as swap partitions, providing a total 2 GB of swap space. With 1 GB of RAM, the workstation has a comfortable amount of available memory."
msgid "The <filename>sda3</filename> and <filename>sdc3</filename> partitions are assembled into a RAID-0 volume, <filename>md1</filename>, and used as swap partition, providing a total 32 GB of swap space. Modern systems can provide plenty of RAM and our system won't need hibernation. So with this amount added, our system will unlikely run out of memory."
msgstr "Les partitions <filename>hda2</filename> et <filename>hdc2</filename> sont utilisées comme partitions d'échange, pour fournir un total de 2 Go de mémoire d'échange. Avec 1 Go de mémoire vive, la station de travail dispose ainsi d'une quantité confortable de mémoire."

#, fuzzy
#| msgid "The <filename>sda5</filename> and <filename>sdc5</filename> partitions, as well as <filename>sda6</filename> and <filename>sdc6</filename>, are assembled into two new RAID-1 volumes of about 100 GB each, <filename>md1</filename> and <filename>md2</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 200 GB of safe space."
msgid "The <filename>sda4</filename> and <filename>sdc4</filename> partitions, as well as <filename>sda5</filename> and <filename>sdc5</filename>, are assembled into two new RAID-1 volumes of about 300 GB each, <filename>md2</filename> and <filename>md3</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 600 GB of safe space."
msgstr "Les partitions <filename>hda5</filename> et <filename>hdc5</filename> d'une part, <filename>hda6</filename> et <filename>hdc6</filename> d'autre part sont utilisées pour monter deux nouveaux volumes RAID-1 de 100 Go chacun, <filename>md1</filename> et <filename>md2</filename>. Ces deux miroirs sont initialisés comme des volumes physiques LVM et affectés au groupe de volumes <filename>vg_raid</filename>. Ce groupe de volumes dispose ainsi d'environ 200 Go d'espace sécurisé."

#, fuzzy
#| msgid "The remaining partitions, <filename>sda7</filename> and <filename>sdc7</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 200 GB of space."
msgid "The remaining partitions, <filename>sda6</filename> and <filename>sdc6</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 460 GB of space."
msgstr "Les partitions restantes, <filename>hda7</filename> et <filename>hdc7</filename>, sont directement initialisées comme des volumes physiques et affectées à un autre VG, <filename>vg_vrac</filename>, qui pourra contenir presque 200 Go de données."

msgid "Once the VGs are created, they can be partitioned in a very flexible way. One must keep in mind that LVs created in <filename>vg_raid</filename> will be preserved even if one of the disks fails, which will not be the case for LVs created in <filename>vg_bulk</filename>; on the other hand, the latter will be allocated in parallel on both disks, which allows higher read or write speeds for large files."
msgstr "Une fois les VG établis, il devient possible de les découper de manière très flexible, en gardant à l'esprit que les LV qui seront créés dans <filename>vg_raid</filename> seront préservés même en cas de panne d'un des deux disques, à l'opposé des LV pris sur <filename>vg_vrac</filename> ; en revanche, ces derniers seront alloués en parallèle sur les deux disques, ce qui permettra des débits élevés lors de la lecture ou de l'écriture de gros fichiers."

#, fuzzy
#| msgid "We will therefore create the <filename>lv_usr</filename>, <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgid "We will therefore create the <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgstr "On créera donc des volumes logiques <filename>lv_usr</filename>, <filename>lv_var</filename>, <filename>lv_home</filename> sur <filename>vg_raid</filename>, pour y accueillir les systèmes de fichiers correspondants ; on y créera également un <filename>lv_films</filename>, d'une taille conséquente, pour accueillir les films déjà montés. L'autre VG sera quant à lui utilisé pour un gros <filename>lv_rushes</filename>, qui accueillera les données directement sorties des caméras numériques, et un <filename>lv_tmp</filename>, pour les fichiers temporaires. Pour l'espace de travail, la question peut se poser : certes, il est important qu'il offre de bonnes performances, mais doit-on pour autant courir le risque de perdre le travail effectué si un disque défaille alors qu'un montage n'est pas fini ? C'est un choix à faire ; en fonction de ce choix, on créera le LV dans l'un ou l'autre VG."

#, fuzzy
#| msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications. Should new software be installed later on (for editing audio clips, for instance), the LV hosting <filename>/usr/</filename> can be grown painlessly."
msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications."
msgstr "On dispose ainsi à la fois d'une certaine redondance des données importantes et d'une grande flexibilité dans la répartition de l'espace disponible pour les différentes applications. Si de nouveaux logiciels doivent être installés par la suite (pour des montages sonores, par exemple), on pourra aisément agrandir l'espace utilisé par <filename>/usr/</filename>."

msgid "<emphasis>NOTE</emphasis> Why three RAID-1 volumes?"
msgstr "<emphasis>NOTE</emphasis> Pourquoi trois volumes RAID-1 ?"

msgid "We could have set up one RAID-1 volume only, to serve as a physical volume for <filename>vg_raid</filename>. Why create three of them, then?"
msgstr "On aurait fort bien pu se contenter d'un seul volume RAID-1, qui servirait de volume physique pour <filename>vg_raid</filename>. Pourquoi donc en avoir créé trois ?"

msgid "The rationale for the first split (<filename>md0</filename> vs. the others) is about data safety: data written to both elements of a RAID-1 mirror are exactly the same, and it is therefore possible to bypass the RAID layer and mount one of the disks directly. In case of a kernel bug, for instance, or if the LVM metadata become corrupted, it is still possible to boot a minimal system to access critical data such as the layout of disks in the RAID and LVM volumes; the metadata can then be reconstructed and the files can be accessed again, so that the system can be brought back to its nominal state."
msgstr "La décision de séparer le premier miroir des deux autres est motivée par des considérations de sécurité des données. En effet, en RAID-1, les données écrites sur les disques sont strictement identiques ; il est donc possible de monter un seul disque directement, sans passer par la couche RAID. En cas de problème dans le noyau, par exemple, ou de corruption des métadonnées LVM, on peut ainsi démarrer un système minimal (sans les applications ou les données) et récupérer des données cruciales, par exemple l'agencement des disques dans les volumes RAID, et surtout dans les ensembles LVM ; on peut ainsi reconstruire les métadonnées et récupérer les fichiers, ce qui permettra de remettre le système dans un état opérationnel."

#, fuzzy
#| msgid "The rationale for the second split (<filename>md1</filename> vs. <filename>md2</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md2</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md2</filename> and integrate its components <filename>sda6</filename> and <filename>sdc6</filename> into the bulk VG (which grows by 200 GB instead of 100 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgid "The rationale for the second split (<filename>md2</filename> vs. <filename>md3</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md3</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md3</filename> and integrate its components <filename>sda5</filename> and <filename>sdc5</filename> into the bulk VG (which grows by 600 GB instead of 300 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgstr "Le pourquoi de la séparation entre <filename>md1</filename> et <filename>md2</filename> est plus subjectif et découle d'une certaine prudence. En effet, lors de l'assemblage de la station de travail, les besoins exacts ne sont pas forcément connus précisément ; ou ils peuvent changer au fil du temps. Dans notre cas, il est difficile de connaître à l'avance les besoins en stockage pour les films montés et les épreuves de tournage. Si un film à monter nécessite de très grandes quantités de <foreignphrase>rushes</foreignphrase> et que le groupe de volumes dédié aux données sécurisées est occupé à moins de 50%, on pourra envisager d'en récupérer une partie. Pour cela, on pourra soit sortir l'un des composants de <filename>vg_raid</filename> et le réaffecter directement à <filename>vg_vrac</filename> (si l'opération est temporaire et si l'on peut vivre avec la perte de performances induite), soit abandonner le RAID-1 sur <filename>md2</filename> et intégrer <filename>hda6</filename> et <filename>hdc6</filename> dans le VG non sécurisé (si l'on a besoin des performances — on récupère d'ailleurs dans ce cas 200 Go d'espace, au lieu des 100 Go du miroir) ; il suffira alors d'élargir le volume logique en fonction des besoins."

msgid "<primary>virtualization</primary>"
msgstr "<primary>virtualisation</primary>"

msgid "Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security."
msgstr "La virtualisation est une des évolutions majeures de ces dernières années en informatique. Ce terme regroupe différentes abstractions simulant des machines virtuelles de manière plus ou moins indépendante du matériel. On peut ainsi obtenir, sur un seul ordinateur physique, plusieurs systèmes fonctionnant en même temps et de manière isolée. Les applications sont multiples et découlent souvent de cette isolation des différentes machines virtuelles : on peut par exemple se créer plusieurs environnements de test selon différentes configurations, ou héberger des services distincts sur des machines (virtuelles) distinctes pour des raisons de sécurité."

msgid "There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:"
msgstr "Il existe différentes mises en œuvre pour la virtualisation, chacune ayant ses avantages et ses inconvénients. Nous allons nous concentrer sur Xen, LXC et KVM, mais on peut aussi citer, à titre d'exemple :"

#, fuzzy
#| msgid "<primary>LXC</primary>"
msgid "<primary>Xen</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>VMWare</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Nagios</primary>"
msgid "<primary>Bochs</primary>"
msgstr "<primary>Nagios</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>QEMU</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary>VirtualBox</primary>"
msgstr "<primary>virtualisation</primary>"

msgid "<primary>KVM</primary>"
msgstr "<primary>KVM</primary>"

msgid "<primary>LXC</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"
msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"https://qemu.org/\" />"
msgstr "QEMU, qui émule en logiciel un ordinateur matériel complet ; bien que les performances soient nettement dégradées de ce fait, ceci permet de faire fonctionner dans l'émulateur des systèmes d'exploitation non modifiés, voire expérimentaux. On peut également émuler un ordinateur d'une architecture différente de celle de l'hôte : par exemple, un ordinateur <foreignphrase>arm</foreignphrase> sur un système <foreignphrase>amd64</foreignphrase>. QEMU est un logiciel libre. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"

msgid "Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64)."
msgstr "Bochs est une autre machine virtuelle libre mais elle n'émule que les architectures x86 (i386 et amd64)."

#, fuzzy
#| msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"
msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as “snapshotting“ a running virtual machine. <ulink type=\"block\" url=\"https://vmware.com/\" />"
msgstr "VMWare est une machine virtuelle propriétaire. C'est la plus ancienne et par conséquent une des plus connues. Elle fonctionne selon un mécanisme similaire à QEMU et dispose de fonctionnalités avancées comme la possibilité de faire des <foreignphrase>snapshots</foreignphrase> (copies instantanées d'une machine virtuelle en fonctionnement). <ulink type=\"block\" url=\"http://www.vmware.com/fr/\" />"

#, fuzzy
#| msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler. While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"
msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some “snapshotting“ and other interesting features. <ulink type=\"block\" url=\"https://www.virtualbox.org/\" />"
msgstr "VirtualBox est une machine virtuelle libre (bien que certains composants additionnels soient disponibles sous une licence propriétaire). Elle est malheureusement dans la section « contrib », parce qu'elle inclut certains fichiers précompilés qui ne peuvent être reconstruits sans un compilateur propriétaire. Elle est plus récente que VMWare et restreinte aux architectures i386 et amd64, mais elle dispose tout de même de fonctionnalités intéressantes comme la possibilité de faire des <foreignphrase>snapshots</foreignphrase>, ainsi que d'autres fonctionalités intéressantes. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"

#, fuzzy
#| msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgid "<emphasis>HARDWARE</emphasis> Virtualization support"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Virtualisation massive"

msgid "Some computers might not have hardware virtualization support; when they do, it should be enabled in the BIOS."
msgstr ""

msgid "To know if you have virtualization support enabled, you can check if the relevant flag is enabled with <command>grep</command>. If the following command for your processor returns some text, you already have virtualization support enabled:"
msgstr ""

msgid "For Intel processors you can execute <command>grep vmx /proc/cpuinfo</command> to check for Intel's Virtual Machine Extensions."
msgstr ""

msgid "For AMD processors you can execute <command>grep svm /proc/cpuinfo</command> to check for AMD's Secure Virtual Machine."
msgstr ""

msgid "If that is not the case, you can access the BIOS of your system and check for entries like “Intel Virtualization Technology”/“Intel VT-x” or “SVM mode” (AMD) - usually to be found in the CPU configuration in the Advanced section."
msgstr ""

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary>paravirtualization</primary>"
msgstr "<primary>virtualisation</primary>"

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary>hypervisor</primary>"
msgstr "<primary>libvirt</primary>"

msgid "Xen <indexterm><primary>Xen</primary></indexterm> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen."
msgstr "Xen<indexterm><primary>Xen</primary></indexterm> est une solution de « paravirtualisation », c'est-à-dire qu'elle insère entre le matériel et les systèmes supérieurs une fine couche d'abstraction, nommée « hyperviseur », dont le rôle est de répartir l'utilisation du matériel entre les différentes machines virtuelles qui fonctionnent dessus. Cependant, cet hyperviseur n'entre en jeu que pour une petite partie des instructions, le reste étant exécuté directement par le matériel pour le compte des différents systèmes. L'avantage est que les performances ne subissent pas de dégradation ; la contrepartie est que les noyaux des systèmes d'exploitation que l'on souhaite utiliser sur un hyperviseur Xen doivent être modifiés pour en tirer parti."

#, fuzzy
#| msgid "<primary>deployment</primary>"
msgid "<primary>dom0</primary>"
msgstr "<primary>déploiement</primary>"

#, fuzzy
#| msgid "<primary>deployment</primary>"
msgid "<primary>domU</primary>"
msgstr "<primary>déploiement</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>virtualization</primary><secondary>host</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>virtualization</primary><secondary>guest</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgid "Let's spend some time on terms. The hypervisor is the lowest layer, which runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgstr "Explicitons un peu de terminologie. Nous avons vu que l'hyperviseur était la couche logicielle la plus basse, qui vient s'intercaler directement entre le noyau et le matériel. Cet hyperviseur est capable de séparer le reste du logiciel en plusieurs <emphasis>domaines</emphasis>, correspondant à autant de machines virtuelles. Parmi ces domaines, le premier à être lancé, désigné sous l'appellation <emphasis>dom0</emphasis>, joue un rôle particulier, puisque c'est depuis ce domaine (et seulement celui-là) qu'on pourra contrôler l'exécution des autres. Ces autres domaines sont, quant à eux, appelés <emphasis>domU</emphasis>. Le terme « dom0 » correspond donc au système « hôte » d'autres mécanismes de virtualisation, « domU » correspondant aux « invités »."

msgid "<emphasis>CULTURE</emphasis> Xen and the various versions of Linux"
msgstr "<emphasis>CULTURE</emphasis> Xen et les différentes versions de Linux"

msgid "Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <emphasis>paravirt_ops</emphasis> or <emphasis>pv_ops</emphasis> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially."
msgstr "À l'origine, Xen a été développé sous forme d'un ensemble de <foreignphrase>patches</foreignphrase> à appliquer sur le noyau. Ces derniers n'ont jamais été intégrés au noyau officiel. Pour répondre aux besoins des différents systèmes de virtualisation émergents à l'époque (et notamment KVM), le noyau Linux s'est doté d'un ensemble de fonctions génériques facilitant la création de solutions de paravirtualisation. Cette interface est connue sous le nom <emphasis>paravirt_ops</emphasis> (ou <emphasis>pv_ops</emphasis>). Puisque les <foreignphrase>patches</foreignphrase> de Xen dupliquaient certaines de ces fonctionnalités, ils n'ont pas pu être acceptés officiellement."

#, fuzzy
#| msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgstr "Suite à ce revers, Xensource — la société éditrice de Xen — a décidé de modifier sa solution pour s'appuyer sur ce nouveau socle afin de pouvoir officiellement intégrer Xen au noyau Linux. Une grande partie du code a dû être réécrite. Et si la société disposait d'une version fonctionnelle s'appuyant sur paravirt_ops, l'intégration dans le noyau Linux a été très progressive. Cette intégration a été complétée dans Linux 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"

#, fuzzy
#| msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgstr "Puisque <emphasis role=\"distribution\">Jessie</emphasis> exploite la version 3.16 du noyau Linux, les paquets standards <emphasis role=\"pkg\">linux-image-686-pae</emphasis> et <emphasis role=\"pkg\">linux-image-amd64</emphasis> fournissent un hyperviseur Xen de manière native (alors que d'anciennes versions du noyau, comme celle dans <emphasis role=\"distribution\">Squeeze</emphasis>, nécessitaient d'intégrer le code fourni par XenSource). <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"

msgid "<emphasis>CULTURE</emphasis> Xen and non-Linux kernels"
msgstr "<emphasis>CULTURE</emphasis> Xen et les noyaux non Linux"

#, fuzzy
#| msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgstr "Xen requiert que les systèmes d'exploitation qu'il doit animer soient modifiés et tous n'ont pas, à ce titre, atteint la même maturité. Plusieurs sont entièrement fonctionnels, à la fois en dom0 et en domU : Linux 3.0 et suivants, NetBSD 4.0 et suivants et OpenSolaris. D'autres ne fonctionnent pour l'instant qu'en domU. Le statut de chaque système d'exploitation est détaillé sur le wiki du projet Xen : <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"

msgid "However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows)."
msgstr "Toutefois, si Xen peut s'appuyer sur les fonctions matérielles dédiées spécifiquement à la virtualisation, qui ne sont proposées que par les processeurs les plus récents, il est alors possible d'employer des systèmes d'exploitation non modifiés (dont Windows) en tant que domU."

msgid "<emphasis>NOTE</emphasis> Architectures compatible with Xen"
msgstr "<emphasis>NOTE</emphasis> Architectures compatibles avec Xen"

msgid "Xen is currently only available for the i386, amd64, arm64 and armhf architectures."
msgstr "Xen n'est actuellement disponible que pour les architectures i386, amd64, arm64 et armhf."

msgid "Using Xen under Debian requires three components:"
msgstr "Pour utiliser Xen sous Debian, trois composants sont nécessaires :"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-hypervisor</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>."
msgid "The hypervisor itself. According to the available hardware, the appropriate package providing <emphasis role=\"pkg\">xen-hypervisor</emphasis> will be either <emphasis role=\"pkg\">xen-hypervisor-4.14-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.14-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.14-arm64</emphasis>."
msgstr "L'hyperviseur proprement dit. Selon le type de matériel dont on dispose, on installera <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis> ou <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>."

#, fuzzy
#| msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <emphasis role=\"distribution\">Jessie</emphasis>."
msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 5.10 version present in <emphasis role=\"distribution\">Bullseye</emphasis>."
msgstr "Un noyau qui fonctionne sur cet hyperviseur. Tout noyau plus récent que 3.0 conviendra, y compris la version 3.16 présente dans <emphasis role=\"distribution\">Jessie</emphasis>."

msgid "The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <emphasis role=\"pkg\">libc6-xen</emphasis> package."
msgstr "Une bibliothèque standard modifiée pour tirer parti de Xen. Pour cela, on installera le paquet <emphasis role=\"pkg\">libc6-xen</emphasis> (valable uniquement sur architecture i386)."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-utils</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <emphasis role=\"pkg\">xen-linux-system-amd64</emphasis>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.4</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:"
msgid "The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.14</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the GRUB bootloader menu, so as to start the chosen kernel in a Xen dom0. Note, however, that this entry is not usually set to be the first one in the list, but it will be selected by default."
msgstr "Pour se simplifier la vie, on installera un des métapaquets auxiliaires, tel que <emphasis role=\"pkg\">xen-linux-system-amd64</emphasis>, qui dépend d'une combinaison réputée stable de versions de l'hyperviseur et du noyau. L'hyperviseur recommande également le paquet <emphasis role=\"pkg\">xen-utils-4.4</emphasis>, lequel contient les utilitaires permettant de contrôler l'hyperviseur depuis le dom0. Et ce dernier (tout comme le noyau Xen) recommande la bibliothèque standard modifiée. Lors de l'installation de ces paquets, les scripts de configuration créent une nouvelle entrée dans le menu du chargeur de démarrage Grub, permettant de démarrer le noyau choisi dans un dom0 Xen. Attention toutefois, cette nouvelle entrée n'est pas celle démarrée en standard. Pour lister les entrées correspondant à l'hyperviseur Xen en premier, vous pouvez exécuter ces commandes :"

msgid "Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps."
msgstr "Une fois cette installation effectuée, il convient de tester le fonctionnement du dom0 seul, donc de redémarrer le système avec l'hyperviseur et le noyau Xen. À part quelques messages supplémentaires sur la console lors de l'initialisation, le système démarre comme d'habitude."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-tools</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>xen-create-image</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/xen-tools/xen-tools.conf</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Now is the time to actually install useful systems on the domU systems, using the tools from <emphasis role=\"pkg\">xen-tools</emphasis>. This package provides the <command>xen-create-image</command> command, which largely automates the task. The only mandatory parameter is <literal>--hostname</literal>, giving a name to the domU; other options are important, but they can be stored in the <filename>/etc/xen-tools/xen-tools.conf</filename> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <command>xen-create-image</command> invocation. Important parameters of note include the following:"
msgstr "Il est donc temps de commencer à installer des systèmes sur les domU. Nous allons pour cela utiliser le paquet <emphasis role=\"pkg\">xen-tools</emphasis>. Ce paquet fournit la commande <command>xen-create-image</command>, qui automatise en grande partie la tâche. Son seul paramètre obligatoire est <literal>--hostname</literal>, qui donne un nom au domU ; d'autres options sont importantes, mais leur présence sur la ligne de commande n'est pas nécessaire parce qu'elles peuvent être placées dans le fichier de configuration <filename>/etc/xen-tools/xen-tools.conf</filename>. On prendra donc soin de vérifier la teneur de ce fichier avant de créer des images, ou de passer des paramètres supplémentaires à <command>xen-create-image</command> lors de son invocation. Notons en particulier :"

msgid "<literal>--memory</literal>, to specify the amount of RAM dedicated to the newly created system;"
msgstr "<literal>--memory</literal>, qui spécifie la quantité de mémoire vive à allouer au système créé ;"

msgid "<literal>--size</literal> and <literal>--swap</literal>, to define the size of the “virtual disks” available to the domU;"
msgstr "<literal>--size</literal> et <literal>--swap</literal>, qui définissent la taille des « disques virtuels » disponibles depuis le domU ;"

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary><command>debootstrap</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

#, fuzzy
#| msgid "<literal>--debootstrap</literal>, to cause the new system to be installed with <command>debootstrap</command>; in that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">jessie</emphasis>)."
msgid "<literal>--debootstrap-cmd</literal>, to specify the which debootstrap command is used. The default is <command>debootstrap</command> if debootstrap and cdebootstrap are installed. In that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">bullseye</emphasis>)."
msgstr "<literal>--debootstrap</literal>, qui spécifie que le système doit être installé avec <command>debootstrap</command> ; si l'on utilise cette option, il sera important de spécifier également <literal>--dist</literal> avec un nom de distribution (par exemple <emphasis role=\"distribution\">jessie</emphasis>)."

msgid "<emphasis>GOING FURTHER</emphasis> Installing a non-Debian system in a domU"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Installer autre chose que Debian dans un domU"

msgid "In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <literal>--kernel</literal> option."
msgstr "S'il s'agit d'installer un système non Linux, on n'oubliera pas de spécifier le noyau à utiliser par le domU, avec l'option <literal>--kernel</literal>."

msgid "<literal>--dhcp</literal> states that the domU's network configuration should be obtained by DHCP while <literal>--ip</literal> allows defining a static IP address."
msgstr "<literal>--dhcp</literal> spécifie que la configuration réseau du domU doit être obtenue par DHCP ; au contraire, <literal>--ip</literal> permet de spécifier une adresse IP statique."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>Xen</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <literal>--dir</literal> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <literal>--lvm</literal> option, followed by the name of a volume group; <command>xen-create-image</command> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive."
msgstr "Enfin, il faut choisir une méthode de stockage pour les images à créer (celles qui seront vues comme des disques durs dans le domU). La plus simple, déclenchée par l'option <literal>--dir</literal>, est de créer un fichier sur le dom0 pour chaque périphérique que l'on souhaite fournir au domU. L'autre possibilité, sur les systèmes utilisant LVM, est de passer par le biais de l'option <literal>--lvm</literal> le nom d'un groupe de volumes, dans lequel <command>xen-create-image</command> créera un nouveau volume logique ; ce volume logique sera rendu disponible au domU comme un disque dur."

msgid "<emphasis>NOTE</emphasis> Storage in the domU"
msgstr "<emphasis>NOTE</emphasis> Stockage dans les domU"

msgid "Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <command>xen-create-image</command>, however, so editing the Xen image's configuration file is in order after its initial creation with <command>xen-create-image</command>."
msgstr "On peut également exporter vers les domU des disques durs entiers, des partitions, des ensembles RAID ou des volumes logiques LVM préexistants. Ces opérations n'étant pas prises en charge par <command>xen-create-image</command>, il faudra pour les accomplir modifier manuellement le fichier de configuration de l'image Xen après sa création par <command>xen-create-image</command>."

msgid "Once these choices are made, we can create the image for our future Xen domU:"
msgstr "Lorsque ces choix sont faits, nous pouvons créer l'image de notre futur domU Xen :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
#| "<computeroutput>\n"
#| "[...]\n"
#| "General Information\n"
#| "--------------------\n"
#| "Hostname       :  testxen\n"
#| "Distribution   :  jessie\n"
#| "Mirror         :  http://ftp.debian.org/debian/\n"
#| "Partitions     :  swap            128Mb (swap)\n"
#| "                  /               2G    (ext3)\n"
#| "Image type     :  sparse\n"
#| "Memory size    :  128Mb\n"
#| "Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
#| "Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
#| "[...]\n"
#| "Logfile produced at:\n"
#| "         /var/log/xen-tools/testxen.log\n"
#| "\n"
#| "Installation Summary\n"
#| "---------------------\n"
#| "Hostname        :  testxen\n"
#| "Distribution    :  jessie\n"
#| "MAC Address     :  00:16:3E:8E:67:5C\n"
#| "IP-Address(es)  :  dynamic\n"
#| "RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
#| "Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=bullseye --role=udev\n"
"</userinput><computeroutput>\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  bullseye\n"
"Mirror         :  http://deb.debian.org/debian\n"
"Partitions     :  swap            512M  (swap)\n"
"                  /               2G    (ext4)\n"
"Image type     :  sparse\n"
"Memory size    :  256M\n"
"Bootloader     :  pygrub\n"
"\n"
"[...]\n"
"Logfile produced at:\n"
"\t /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  bullseye\n"
"MAC Address     :  00:16:3E:C2:07:EE\n"
"IP Address(es)  :  dynamic\n"
"SSH Fingerprint :  SHA256:K+0QjpGzZOacLZ3jX4gBwp0mCESt5ceN5HCJZSKWS1A (DSA)\n"
"SSH Fingerprint :  SHA256:9PnovvGRuTw6dUcEVzzPKTITO0+3Ki1Gs7wu4ke+4co (ECDSA)\n"
"SSH Fingerprint :  SHA256:X5z84raKBajUkWBQA6MVuanV1OcV2YIeD0NoCLLo90k (ED25519)\n"
"SSH Fingerprint :  SHA256:VXu6l4tsrCoRsXOqAwvgt57sMRj2qArEbOzHeydvV34 (RSA)\n"
"Root Password   :  FS7CUxsY3xkusv7EkbT9yae\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  jessie\n"
"Mirror         :  http://ftp.debian.org/debian/\n"
"Partitions     :  swap            128Mb (swap)\n"
"                  /               2G    (ext3)\n"
"Image type     :  sparse\n"
"Memory size    :  128Mb\n"
"Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
"Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  jessie\n"
"MAC Address     :  00:16:3E:8E:67:5C\n"
"IP-Address(es)  :  dynamic\n"
"RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
"Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
"</computeroutput>"

msgid "We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters."
msgstr "Nous disposons à présent d'une machine virtuelle, mais actuellement éteinte, qui n'occupe de la place que sur le disque dur du dom0. Nous pouvons bien entendu créer plusieurs images, avec des paramètres différents au besoin."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>network models</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgstr "Avant d'allumer ces machines virtuelles, il reste à définir la manière d'accéder aux domU. Il est possible de les considérer comme des machines isolées et de n'accéder qu'à leur console système, mais ce n'est guère pratique. La plupart du temps, on pourra se contenter de considérer les domU comme des serveurs distants et de les contacter comme à travers un réseau. Cependant, il serait peu commode de devoir ajouter une carte réseau pour chaque domU ! Xen permet donc de créer des interfaces virtuelles, que chaque domaine peut voir et présenter à l'utilisateur de la manière habituelle. Cependant, ces cartes, même virtuelles, doivent pour être utiles être raccordées à un réseau, même virtuel. Xen propose pour cela plusieurs modèles de réseau :"

msgid "The simplest model is the <emphasis>bridge</emphasis> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch."
msgstr "En mode pont <foreignphrase>(bridge)</foreignphrase>, toutes les cartes réseau eth0 (pour le dom0 comme pour les domU) se comportent comme si elles étaient directement branchées sur un commutateur Ethernet <foreignphrase>(switch)</foreignphrase>. C'est le mode le plus simple."

msgid "Then comes the <emphasis>routing</emphasis> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network."
msgstr "En mode routage, le dom0 est placé entre les domU et le réseau extérieur (physique) ; il joue un rôle de routeur."

msgid "Finally, in the <emphasis>NAT</emphasis> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0."
msgstr "En mode traduction d'adresse (NAT), le dom0 est également placé entre les domU et le reste du réseau ; cependant, les domU ne sont pas accessibles directement depuis l'extérieur, le trafic subissant de la traduction d'adresses sur le dom0."

msgid "These three networking nodes involve a number of interfaces with unusual names, such as <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> and <filename>xenbr0</filename>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model."
msgstr "Ces trois modes mettent en jeu un certain nombre d'interfaces aux noms inhabituels, comme <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> et <filename>xenbr0</filename>, qui sont mises en correspondance selon différents agencements par l'hyperviseur Xen, contrôlé par les utilitaires en espace utilisateur. Nous ne nous attarderons pas ici sur les modes NAT et routage, qui ne présentent d'intérêt que dans des cas particuliers."

#, fuzzy
#| msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.4</emphasis> package recommends it) to replace the existing eth0 entry:"
msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils</emphasis> package recommends it) to replace the existing <replaceable>eth0</replaceable> entry (be careful to use the correct network device name):"
msgstr "La configuration standard des paquets Debian de Xen n'effectue aucune modification à la configuration réseau du système. En revanche, le démon <command>xend</command> est configuré pour intégrer les cartes réseau virtuelles dans n'importe quel pont pré-existant (si plusieurs existent, c'est <filename>xenbr0</filename> qui est retenu). Il convient donc de mettre en place un pont dans <filename>/etc/network/interfaces</filename> (cela nécessite le paquet <emphasis role=\"pkg\">bridge-utils</emphasis> qui est recommandé par <emphasis role=\"pkg\">xen-utils-4.4</emphasis>) en remplaçant l'entrée existante pour eth0 :"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/network/interfaces</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">bridge-utils</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary><literal>xenbr0</literal></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid ""
#| "auto xenbr0\n"
#| "iface xenbr0 inet dhcp\n"
#| "    bridge_ports eth0\n"
#| "    bridge_maxwait 0\n"
#| "    "
msgid ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports <replaceable>eth0</replaceable>\n"
"    bridge_maxwait 0"
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>xl</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them."
msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them. You might need to increase the default memory by editing the variable memory from configuration file (in this case, <filename>/etc/xen/testxen.cfg</filename>). Here we have set it to 1024 (megabytes)."
msgstr "Après un redémarrage pour vérifier que le pont est bien créé de manière automatique, nous pouvons maintenant démarrer le domU grâce aux outils de contrôle de Xen, en particulier la commande <command>xl</command>. Cette commande permet d'effectuer différentes manipulations sur les domaines, notamment de les lister, de les démarrer et de les éteindre."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/xen/testxen.cfg</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   463     1     r-----      9.8\n"
#| "# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
#| "<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
#| "# </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   366     1     r-----     11.4\n"
#| "testxen                                      1   128     1     -b----      1.1</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl list\n"
"</userinput><computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  3918     2     r-----      35.1\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg\n"
"</userinput><computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list\n"
"</userinput><computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  2757     2     r-----      45.2\n"
"testxen                                      3  1024     1     r-----       1.3\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   463     1     r-----      9.8\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   366     1     r-----     11.4\n"
"testxen                                      1   128     1     -b----      1.1</computeroutput>"

msgid "<emphasis>TOOL</emphasis> Choice of toolstacks to manage Xen VM"
msgstr "<emphasis>OUTIL</emphasis> Outils disponibles pour gérer une VM Xen"

msgid "<primary><command>xm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "<primary><command>xe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "<primary><command>virsh</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">libvirt</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of libvirt and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of <emphasis role=\"pkg\">libvirt</emphasis> and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgstr "Dans les anciennes versions de Debian (jusqu'à la version 7), <command>xm</command> était l'outil de référence pour utiliser et manipuler les machines virtuelles Xen. Cet outil en ligne de commande a maintenant été remplacé par <command>xl</command>, qui est globalement compatible. Mais ce ne sont pas les seuls outils disponibles : il existe aussi <command>virsh</command> de la suite libvirt, et <command>xe</command> de l'offre commerciale XAPI de XenServer."

msgid "<emphasis>CAUTION</emphasis> Only one domU per image!"
msgstr "<emphasis>ATTENTION</emphasis> Un seul domU par image !"

#, fuzzy
#| msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is, however, quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgstr "On peut bien entendu démarrer plusieurs domU en parallèle, mais chacun devra utiliser son propre système, puisque chacun (mis à part la petite partie du noyau qui s'interface avec l'hyperviseur) se croit seul sur le matériel ; il n'est pas possible de partager un espace de stockage entre deux domU fonctionnant en même temps. On pourra cependant, si l'on n'a pas besoin de faire tourner plusieurs domU en même temps, réutiliser la même partition d'échange, par exemple, ou la même partition utilisée pour stocker <filename>/home/</filename>."

msgid "Note that the <filename>testxen</filename> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly."
msgstr "On notera que le domU <filename>testxen</filename> occupe de la mémoire vive réelle, qui est prise sur celle disponible pour le dom0 (il ne s'agit pas de mémoire vive simulée). Il sera donc important de bien dimensionner la mémoire vive d'une machine que l'on destine à héberger des instances Xen."

msgid "Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <filename>hvc0</filename> console, with the <command>xl console</command> command:"
msgstr "Voilà ! Notre machine virtuelle démarre. Pour y accéder, nous avons deux possibilités. La première est de s'y connecter « à distance », à travers le réseau (comme pour une machine réelle, cependant, il faudra probablement mettre en place une entrée dans le DNS, ou configurer un serveur DHCP). La seconde, qui peut être plus utile si la configuration réseau du domU était erronée, est d'utiliser la console <filename>hvc0</filename>. On utilisera pour cela la commande <command>xl console</command> :"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary><literal>hvc0</literal></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
#| "<computeroutput>[...]\n"
#| "\n"
#| "Debian GNU/Linux 8 testxen hvc0\n"
#| "\n"
#| "testxen login: </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 11 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 8 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"

msgid "One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> key combination."
msgstr "On peut ainsi ouvrir une session, comme si l'on était au clavier de la machine virtuelle. Pour détacher la console, on utilisera la combinaison de touches <keycombo action=\"simul\"><keycap>Ctrl</keycap> <keycap>]</keycap></keycombo>."

msgid "<emphasis>TIP</emphasis> Getting the console straight away"
msgstr "<emphasis>ASTUCE</emphasis> Obtenir la console tout de suite"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>console</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Sometimes one wishes to start a domU system and get to its console straight away; this is why the <command>xl create</command> command takes a <literal>-c</literal> switch. Starting a domU with this switch will display all the messages as the system boots."
msgstr "Si l'on souhaite démarrer un système dans un domU et accéder à sa console dès le début, on pourra passer l'option <literal>-c</literal> à la commande <command>xl create</command> ; on obtiendra alors tous les messages au fur et à mesure du démarrage du système."

#, fuzzy
#| msgid "<emphasis>TOOL</emphasis> OpenXenManager"
msgid "<emphasis>TOOL</emphasis> Graphical Xen managers"
msgstr "<emphasis>OUTIL</emphasis> OpenXenManager"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>manager</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <command>xl</command> command."
msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package), a graphical interface allowing remote management of Xen domains via Xen's API, is no longer provided by Debian due to the lack of upstream development. If you are looking for a replacement, <emphasis role=\"pkg\">virt-manager</emphasis> provides support to handle Xen VMs as well."
msgstr "OpenXenManager (dans le paquet <emphasis role=\"pkg\">openxenmanager</emphasis>) est une interface graphique qui permet la gestion distante de domaines Xen par l'intermédiaire de l'API Xen. Cet outil peut donc contrôler des domaines Xen distants. Il fournit la plupart des fonctionnalités de la commande <command>xl</command>."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">openxenmanager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtual-manager</emphasis></primary>"

msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtual-manager</emphasis></primary>"

msgid "Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <command>xl pause</command> and <command>xl unpause</command> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <command>xl save</command> and <command>xl restore</command> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system."
msgstr "Une fois que le domU est fonctionnel, on peut s'en servir comme d'un serveur classique (c'est un système GNU/Linux, après tout). Mais comme il s'agit d'une machine virtuelle, on dispose de quelques fonctions supplémentaires. On peut par exemple le mettre en pause temporairement, puis le débloquer, avec les commandes <command>xl pause</command> et <command>xl unpause</command>. Un domU en pause cesse de consommer de la puissance de processeur, mais sa mémoire lui reste allouée. La fonction de sauvegarde (<command>xl save</command>) et celle de restauration associée (<command>xl restore</command>) seront donc peut-être plus intéressantes. En effet, une sauvegarde d'un domU libère les ressources utilisées par ce domU, y compris la mémoire vive. Lors de la restauration (comme d'ailleurs après une pause), le domU ne s'aperçoit de rien de particulier, sinon que le temps a passé. Si un domU est toujours en fonctionnement lorsqu'on éteint le dom0, il sera automatiquement sauvegardé ; au prochain démarrage, il sera automatiquement restauré et remis en marche. Bien entendu, on aura les inconvénients que l'on peut constater par exemple lors de la suspension d'un ordinateur portable ; en particulier, si la suspension est trop longue, les connexions réseau risquent d'expirer. Notons en passant que Xen est pour l'instant incompatible avec une grande partie de la gestion de l'énergie ACPI, ce qui inclut la suspension <foreignphrase>(software suspend)</foreignphrase> du système hôte (dom0)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>ACPI</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>ACPI</primary>"
msgstr "<primary>RAID</primary>"

msgid "Halting or rebooting a domU can be done either from within the domU (with the <command>shutdown</command> command) or from the dom0, with <command>xl shutdown</command> or <command>xl reboot</command>."
msgstr "Pour éteindre ou redémarrer un domU, on pourra soit exécuter la commande <command>shutdown</command> à l'intérieur de ce domU, soit utiliser, depuis le dom0, <command>xl shutdown</command> ou <command>xl reboot</command>."

msgid "Most of the <command>xl</command> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> manual page."
msgstr "La plupart des sous-commandes de <command>xl</command> attendent un ou plusieurs arguments, souvent le nom du domU concerné. Ces arguments sont largement décrits dans la page de manuel <citerefentry><refentrytitle>xl</refentrytitle><manvolnum>1</manvolnum></citerefentry>."

msgid "<emphasis>GOING FURTHER</emphasis> Advanced Xen"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Xen avancé"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>documentation</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"
msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"https://xenproject.org/help/documentation/\" />"
msgstr "Xen offre de nombreuses fonctions avancées que que nous ne pouvons pas décrire dans ces quelques paragraphes. En particulier, le système est relativement dynamique et l'on peut modifier différents paramètres d'un domaine (mémoire allouée, disques durs rendus disponibles, comportement de l'ordonnanceur des tâches, etc.) pendant le fonctionnement de ce domaine. On peut même migrer un domU entre des machines, sans l'éteindre ni perdre les connexions réseau ! On se rapportera, pour ces aspects avancés, à la documentation de Xen. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>Linux Containers</primary><see>LXC</see>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>kernel</primary><secondary>control groups</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <emphasis>control groups</emphasis>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system."
msgstr "Bien qu'il soit utilisé pour construire des « machines virtuelles », LXC n'est pas à proprement parler une solution de virtualisation. C'est en réalité un système permettant d'isoler des groupes de processus sur un même système. Il tire parti pour cela d'un ensemble d'évolutions récentes du noyau Linux, regroupées sous le nom de <foreignphrase>control groups</foreignphrase>, et qui permettent de donner des visions différentes de certains aspects du système à des ensembles de processus appelés groupes. Parmi ces aspects du système figurent notamment les identifiants de processus, la configuration réseau et les points de montage. Un groupe de processus ainsi isolés n'aura pas accès aux autres processus du système et son accès au système de fichiers pourra être restreint à un sous-ensemble prédéfini. Il aura également accès à sa propre interface réseau, sa table de routage, éventuellement à un sous-ensemble des périphériques présents, etc."

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>container</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there is no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgstr "Si l'on tire parti de ces fonctions, on peut isoler de la sorte tout une famille de processus depuis le processus <command>init</command> et on obtient un ensemble qui se rapproche énormément d'une machine virtuelle. L'appellation officielle est « un conteneur » (ce qui donne son nom à LXC, pour <foreignphrase>LinuX Containers</foreignphrase>), mais la principale différence avec une machine virtuelle Xen ou KVM tient au fait qu'il n'y a pas de deuxième noyau ; le conteneur utilise le même noyau que la machine hôte. Cela présente des avantages comme des inconvénients : parmi les avantages, citons les excellentes performances grâce à l'absence d'hyperviseur et de noyau intermédiaire, le fait que le noyau peut avoir une vision globale des processus du système et peut donc ordonnancer leur exécution de manière plus efficace que si deux noyaux indépendants essayaient d'ordonnancer des ensembles de processus sans cette vision d'ensemble. Parmi les inconvénients, citons qu'il n'est pas possible d'avoir une machine virtuelle avec un noyau différent (qu'il s'agisse d'un autre système d'exploitation ou simplement d'une autre version de Linux)."

msgid "<emphasis>NOTE</emphasis> LXC isolation limits"
msgstr "<emphasis>NOTE</emphasis> Limites de l'isolation par LXC"

msgid "LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:"
msgstr "Contrairement au fonctionnement « habituel » des émulateurs ou des virtualiseurs plus lourds, les conteneurs LXC ne fournissent pas nécessairement une isolation totale. En particulier :"

msgid "since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;"
msgstr "Comme le noyau est partagé entre le système hôte et les conteneurs, il est possible d'accéder depuis les conteneurs aux messages du noyau, ce qui peut donner lieu à des fuites d'informations si des messages sont émis par un conteneur;"

msgid "for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;"
msgstr "Pour la même raison, si l'un des conteneurs est compromis et si une faille de sécurité du noyau est ainsi exposée, les autres conteneurs seront affectés aussi;"

msgid "on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers."
msgstr "La gestion des permissions sur les fichiers est faite par le noyau sur la base des identifiants numériques d'utilisateurs et de groupes ; ces identifiants ne correspondent pas nécessairement aux mêmes utilisateurs sur des conteneurs différents, il faudra donc garder cela à l'esprit si des systèmes de fichiers sont partagés entre plusieurs conteneurs et accessibles en écriture."

msgid "Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container."
msgstr "Comme il s'agit d'isolation et non de virtualisation complète, la mise en place de conteneurs LXC est un peu plus complexe que la simple utilisation de debian-installer sur une machine virtuelle. Après quelques préliminaires, il s'agira de mettre en place une configuration réseau, puis de créer le système qui sera amené à fonctionner dans le conteneur."

msgid "Preliminary Steps"
msgstr "Préliminaires"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">lxc</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains the tools required to run LXC, and must therefore be installed."
msgstr "Les utilitaires requis pour faire fonctionner LXC sont inclus dans le paquet <emphasis role=\"pkg\">lxc</emphasis>, qui doit donc être installé avant toute chose."

msgid "<primary><filename>/sys</filename></primary><secondary><filename>/sys/fs/cgroup</filename></secondary>"
msgstr ""

msgid "LXC also requires the <emphasis>control groups</emphasis> configuration system, which is a virtual filesystem to be mounted on <filename>/sys/fs/cgroup</filename>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration."
msgstr "LXC a également besoin du système de paramétrage des <foreignphrase>control groups</foreignphrase>. Ce dernier se présente comme un système de fichiers virtuels à monter dans <filename>/sys/fs/cgroup</filename>. Comme Debian 8 utilise par défaut systemd, qui a aussi besoin des <foreignphrase>control groups</foreignphrase>, cette opération est effectuée automatiquement au démarrage, et il n'est plus besoin de configuration supplémentaire."

msgid "Network Configuration"
msgstr "Configuration réseau"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary>network configuration</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgid "The goal of installing LXC is to set up virtual machines; while we could, of course, keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgstr "Nous cherchons à utiliser LXC pour mettre en place des machines virtuelles ; il est possible de les laisser isolées du réseau et de ne communiquer avec elles que par le biais du système de fichiers, mais il sera dans la plupart des cas pertinent de donner aux conteneurs un accès, au moins minimal, au réseau. Dans le cas typique, chaque conteneur aura une interface réseau virtuelle et la connexion au vrai réseau passera par un pont. Cette interface virtuelle peut être soit branchée sur l'interface physique de la machine hôte, auquel cas le conteneur est directement sur le réseau, soit branchée sur une autre interface virtuelle de l'hôte, qui pourra ainsi filtrer ou router le trafic de manière fine. Dans les deux cas, il faudra installer le paquet <emphasis role=\"pkg\">bridge-utils</emphasis>."

#, fuzzy
#| msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance, <literal>eth0</literal> or <literal>enp1s0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgstr "Dans le cas simple, il s'agit de modifier <filename>/etc/network/interfaces</filename> pour créer une interface <literal>br0</literal>, y déplacer la configuration de l'interface physique (<literal>eth0</literal> par exemple) et y ajouter le lien entre les deux. Ainsi, si le fichier de définitions des interfaces standard contient initialement des lignes comme celles-ci :"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>br</literal> interface</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary><literal>br</literal>, network interface</primary>"
msgstr "<primary>virtualisation</primary>"

msgid ""
"auto eth0\n"
"iface eth0 inet dhcp"
msgstr ""
"auto eth0\n"
"iface eth0 inet dhcp"

msgid "They should be disabled and replaced with the following:"
msgstr "Il faudra les désactiver et les remplacer par :"

#, fuzzy
#| msgid ""
#| "auto xenbr0\n"
#| "iface xenbr0 inet dhcp\n"
#| "    bridge_ports eth0\n"
#| "    bridge_maxwait 0\n"
#| "    "
msgid ""
"auto br0\n"
"iface br0 inet dhcp\n"
"    bridge-ports <replaceable>eth0</replaceable>"
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

msgid "The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <literal>eth0</literal> as well as the interfaces defined for the containers."
msgstr "Cette configuration aura un résultat similaire à celui qui serait obtenu si les conteneurs étaient des machines branchées sur le même réseau physique que la machine hôte. La configuration en « pont » s'occupe de faire transiter les trames Ethernet sur toutes les interfaces connectées au pont, c'est-à-dire l'interface physique <literal>eth0</literal> mais aussi les interfaces qui seront définies pour les conteneurs."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>tap</literal> interface</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgid "In cases where this configuration cannot be used (for instance, if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgstr "Si l'on ne souhaite pas utiliser cette configuration, par exemple parce qu'on ne dispose pas d'adresse IP publique à affecter aux conteneurs, on créera une interface virtuelle <emphasis>tap</emphasis> que l'on intègrera au pont. On aura alors une topologie de réseau similaire à ce que l'on aurait avec une deuxième carte réseau sur l'hôte, branchée sur un switch séparé, avec les conteneurs branchés sur ce même switch. L'hôte devra donc faire office de passerelle pour les conteneurs si l'on souhaite que ceux-ci puissent communiquer avec l'extérieur."

msgid "In addition to <emphasis role=\"pkg\">bridge-utils</emphasis>, this “rich” configuration requires the <emphasis role=\"pkg\">vde2</emphasis> package; the <filename>/etc/network/interfaces</filename> file then becomes:"
msgstr "Pour cette configuration riche, on installera, en plus de <emphasis role=\"pkg\">bridge-utils</emphasis>, le paquet <emphasis role=\"pkg\">vde2</emphasis> ; le fichier <filename>/etc/network/interfaces</filename> peut alors devenir :"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">vde2</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid ""
#| "# Interface eth0 is unchanged\n"
#| "auto eth0\n"
#| "iface eth0 inet dhcp\n"
#| "\n"
#| "# Virtual interface \n"
#| "auto tap0\n"
#| "iface tap0 inet manual\n"
#| "  vde2-switch -t tap0\n"
#| "\n"
#| "# Bridge for containers\n"
#| "auto br0\n"
#| "iface br0 inet static\n"
#| "  bridge-ports tap0\n"
#| "  address 10.0.0.1\n"
#| "  netmask 255.255.255.0"
msgid ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"    vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"    bridge-ports tap0\n"
"    address 10.0.0.1\n"
"    netmask 255.255.255.0"
msgstr ""
"# Interface eth0 inchangée\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Interface virtuelle\n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Pont pour les conteneurs\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"

msgid "The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <literal>br0</literal> interface."
msgstr "On pourra ensuite soit configurer le réseau de manière statique dans les conteneurs, soit installer sur l'hôte un serveur DHCP configuré pour répondre aux requêtes sur l'interface <literal>br0</literal>."

msgid "Setting Up the System"
msgstr "Mise en place du système"

#, fuzzy
#| msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> package includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgstr "Nous allons maintenant mettre en place le système de fichiers qui sera utilisé par le conteneur. Comme cette « machine virtuelle » ne fonctionnera pas directement sur le matériel, certains ajustements sont nécessaires par rapport à un système de fichiers classique, notamment en ce qui concerne le noyau, les périphériques et les consoles. Fort heureusement, le paquet <emphasis role=\"pkg\">lxc</emphasis> contient des scripts qui automatisent en grande partie cette mise en place. Ainsi, pour créer un conteneur Debian, on pourra utiliser les commandes suivantes (qui auront besoin des paquets <emphasis role=\"pkg\">debootstrap</emphasis> et <emphasis role=\"pkg\">rsync</emphasis>) :"

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
#| "</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
#| "Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
#| "Downloading debian minimal ...\n"
#| "I: Retrieving Release \n"
#| "I: Retrieving Release.gpg \n"
#| "[...]\n"
#| "Download complete.\n"
#| "Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
#| "[...]\n"
#| "Root password is 'sSiKhMzI', please change !\n"
#| "root@mirwiz:~# </computeroutput>\n"
#| "        "
msgid ""
"<computeroutput># </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"Root password is 'sSiKhMzI', please change !\n"
"root@mirwiz:~# </computeroutput>\n"
"        "

msgid "Note that the filesystem is initially created in <filename>/var/cache/lxc</filename>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required."
msgstr "On notera que le système de fichiers est initialement généré dans <filename>/var/cache/lxc</filename>, puis copié vers le répertoire de destination ; cela permet de créer d'autres systèmes de fichiers identiques beaucoup plus rapidement, puisque seule la copie sera nécessaire."

#, fuzzy
#| msgid "Note that the debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgid "Note that the Debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgstr "Signalons que le script de création de template Debian accepte une option <option>--arch</option> pour spécifier l'architecture du système à installer ainsi qu'une option <option>--release</option> si l'on souhaite une version de Debian autre que la version stable actuelle. Vous pouvez également définir la variable d'environnement <literal>MIRROR</literal> pour indiquer un miroir Debian local à utiliser."

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package further creates a bridge interface <literal>lxcbr0</literal>, which by default is used by all newly created containers via <filename>/etc/lxc/default.conf</filename> and the <filename>lxc-net</filename> service:"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/lxc/default.conf</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>lxc-net.service</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>veth</literal> interface</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "<primary><literal>veth</literal>, network interface</primary>"
msgstr ""

#, fuzzy
#| msgid ""
#| "lxc.network.type = veth\n"
#| "lxc.network.flags = up\n"
#| "lxc.network.link = br0\n"
#| "lxc.network.hwaddr = 4a:49:43:49:79:20"
msgid ""
"lxc.net.0.type = veth\n"
"lxc.net.0.link = lxcbr0\n"
"lxc.net.0.flags = up"
msgstr ""
"lxc.network.type = veth\n"
"lxc.network.flags = up\n"
"lxc.network.link = br0\n"
"lxc.network.hwaddr = 4a:49:43:49:79:20"

#, fuzzy
#| msgid "These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <literal>br0</literal> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated."
msgid "These entries mean, respectively, that a virtual interface will be created in every new container; that it will automatically be brought up when said container is started; and that it will be automatically connected to the <literal>lxcbr0</literal> bridge on the host. You will find these settings in the created container's configuration (<filename>/var/lib/lxc/testlxc/config</filename>), where also the device' MAC address will be specified in <literal>lxc.net.0.hwaddr</literal>. Should this last entry be missing or disabled, a random MAC address will be generated."
msgstr "Ces lignes signifient respectivement qu'une interface virtuelle sera créée dans le conteneur, qu'elle sera automatiquement activée au démarrage dudit conteneur, qu'elle sera automatiquement connectée au pont <literal>br0</literal> de l'hôte et qu'elle aura l'adresse MAC spécifiée. Si cette dernière instruction est absente, ou désactivée, une adresse aléatoire sera utilisée."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary>container configuration</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Another useful entry in that file is the setting of the hostname:"
msgstr "Une autre instruction utile est celle qui définit le nom d'hôte :"

#, fuzzy
#| msgid "lxc.utsname = testlxc"
msgid "lxc.uts.name = testlxc"
msgstr "lxc.utsname = testlxc"

msgid "The newly-created filesystem now contains a minimal Debian system and a network interface."
msgstr ""

msgid "Starting the Container"
msgstr "Lancement du conteneur"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-start</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-attach</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Now that our virtual machine image is ready, let's start the container:"
msgid "Now that our virtual machine image is ready, let's start the container with <command>lxc-start --name=testlxc</command>."
msgstr "Maintenant que notre image de machine virtuelle est prête, nous pouvons démarrer le conteneur :"

msgid "In LXC releases following 2.0.8, root passwords are not set by default. We can set one running <command>lxc-attach -n testlxc <replaceable>passwd</replaceable></command> if we want. We can login with:"
msgstr ""

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
#| "</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
#| "</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
#| "\n"
#| "testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
#| "Password: \n"
#| "Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
#| "\n"
#| "The programs included with the Debian GNU/Linux system are free software;\n"
#| "the exact distribution terms for each program are described in the\n"
#| "individual files in /usr/share/doc/*/copyright.\n"
#| "\n"
#| "Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
#| "permitted by applicable law.\n"
#| "root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
#| "<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
#| "root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
#| "root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
#| "root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
#| "root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
#| "root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
#| "root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
#| "root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
#| "root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
#| "root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
#| "root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
#| "root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
#| "root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
#| "root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
#| "root@testlxc:~# </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput><![CDATA[Connected to tty 1\n"
"Type <Ctrl+a q> to exit the console, <Ctrl+a Ctrl+a> to enter Ctrl+a itself\n"
"\n"
"Debian GNU/Linux 11 testlxc tty1\n"
"\n"
"testlxc login: ]]></computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 5.10.0-11-amd64 #1 SMP Debian 5.10.92-1 (2022-01-18) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"Last login: Wed Mar  9 01:45:21 UTC 2022 on console\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf\n"
"</userinput><computeroutput>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root           1  0.0  0.2  18964 11464 ?        Ss   01:36   0:00 /sbin/init\n"
"root          45  0.0  0.2  31940 10396 ?        Ss   01:37   0:00 /lib/systemd/systemd-journald\n"
"root          71  0.0  0.1  99800  5724 ?        Ssl  01:37   0:00 /sbin/dhclient -4 -v -i -pf /run/dhclient.eth0.pid [..]\n"
"root          97  0.0  0.1  13276  6980 ?        Ss   01:37   0:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups\n"
"root         160  0.0  0.0   6276  3928 pts/0    Ss   01:46   0:00 /bin/login -p --\n"
"root         169  0.0  0.0   7100  3824 pts/0    S    01:51   0:00  \\_ -bash\n"
"root         172  0.0  0.0   9672  3348 pts/0    R+   01:51   0:00      \\_ ps auxwf\n"
"root         164  0.0  0.0   5416  2128 pts/1    Ss+  01:49   0:00 /sbin/agetty -o -p -- \\u --noclear [...]\n"
"root@testlxc:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
"</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
"root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
"root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
"root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
"root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
"root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
"root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
"root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
"root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
"root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
"root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
"root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
"root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
"root@testlxc:~# </computeroutput>"

msgid "We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<filename>/var/lib/lxc/testlxc/rootfs</filename>). We can exit the console with <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."
msgstr "Nous voilà ainsi dans le conteneur, d'où nous n'avons accès qu'aux processus lancés depuis le conteneur lui-même et qu'au sous-ensemble dédié du système de fichiers (<filename>/var/lib/lxc/testlxc/rootfs</filename>). Nous pouvons quitter la console avec la combinaison de touches <keycombo action=\"simul\"><keycap>Ctrl</keycap> <keycap>a</keycap></keycombo> suivie de <keycombo><keycap>q</keycap></keycombo>."

#, fuzzy
#| msgid "Note that we ran the container as a background process, thanks to the <option>--daemon</option> option of <command>lxc-start</command>. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgid "Note that we ran the container as a background process, thanks to <command>lxc-start</command> starting using the <option>--daemon</option> option by default. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgstr "Notons que l'on a démarré le conteneur en tâche de fond, grâce à l'option <option>--daemon</option> de <command>lxc-start</command>. On pourra ensuite l'interrompre par <command>lxc-stop --name=testlxc</command>."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-stop</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <command>lxc-autostart</command> which starts containers whose <literal>lxc.start.auto</literal> option is set to 1). Finer-grained control of the startup order is possible with <literal>lxc.start.order</literal> and <literal>lxc.group</literal>: by default, the initialization script first starts containers which are part of the <literal>onboot</literal> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <literal>lxc.start.order</literal> option."
msgstr "Le paquet <emphasis role=\"pkg\">lxc</emphasis> contient un script d'initialisation qui peut démarrer automatiquement un ou plusieurs conteneurs lors du démarrage de l'ordinateur (il utilise <command>lxc-autostart</command>, qui démarre les conteneurs dont l'option <literal>lxc.start.auto</literal> est réglée à 1). Un contrôle plus fin de l'ordre de démarrage est possible, grâce aux options <literal>lxc.start.order</literal> et <literal>lxc.group</literal> : par défaut, le script d'initialisation démarre d'abord les conteneurs qui sont dans le groupe <literal>onboot</literal>, puis ceux qui ne font partie d'aucun groupe. Dans les deux cas, l'ordre de lancement au sein d'un groupe est contrôlé par l'option <literal>lxc.start.order</literal>."

msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Virtualisation massive"

msgid "Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <literal>tap</literal> and <literal>veth</literal> interfaces should be enough in many cases."
msgstr "Comme LXC est une solution d'isolation assez légère, elle peut être particulièrement adaptée à de l'hébergement massif de serveurs virtuels. Il faudra vraisemblablement utiliser une configuration réseau un peu plus avancée que celle utilisée dans cet exemple, mais la configuration avec interfaces <literal>tap</literal> et <literal>veth</literal> présentée plus haut suffira dans de nombreux cas."

msgid "It may also make sense to share part of the filesystem, such as the <filename>/usr</filename> and <filename>/lib</filename> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <literal>lxc.mount.entry</literal> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage."
msgstr "On pourra en outre vouloir partager une partie du système de fichiers, par exemple les sous-arborescences <filename>/usr</filename> et <filename>/lib</filename>, pour ne pas avoir à dupliquer les programmes installés s'ils sont communs à plusieurs conteneurs ; on ajoutera pour cela des entrées <literal>lxc.mount.entry</literal> dans le fichier de configuration des conteneurs. Un effet secondaire intéressant est qu'ils occuperont également moins de mémoire vive, vu que le noyau est capable de s'apercevoir que les programmes sont partagés. Le coût marginal d'un conteneur supplémentaire sera alors réduit à l'espace disque dédié (les données spécifiques à ce conteneur) et à quelques processus supplémentaires à gérer par le noyau."

msgid "We haven't described all the available options, of course; more comprehensive information can be obtained from the <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> and <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual pages and the ones they reference."
msgstr "Nous ne décrivons pas ici toutes les options disponibles ; pour plus d'informations, on se référera aux pages de manuel <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry>, <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> et celles référencées."

msgid "Virtualization with KVM"
msgstr "Virtualisation avec KVM"

msgid "<primary>Kernel-based Virtual Machine</primary><see>KVM</see>"
msgstr ""

msgid "KVM, which stands for <emphasis>Kernel-based Virtual Machine</emphasis>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <command>qemu-*</command> commands: it is still about KVM."
msgstr "KVM (<foreignphrase>Kernel-based Virtual Machine</foreignphrase>, « machine virtuelle basée sur le noyau ») est avant tout un module noyau facilitant la mise en place de machines virtuelles. L'application que l'on utilise pour démarrer et contrôler ces machines virtuelles est dérivée de QEMU. Ne vous étonnez donc pas si l'on fait appel à des commandes <command>qemu-*</command> dans cette section traitant de KVM ."

msgid "Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <filename>/proc/cpuinfo</filename>."
msgstr "Contrairement aux autres solutions de virtualisation, KVM a été intégré au noyau Linux dès ses débuts. Le choix de s'appuyer sur les jeux d'instructions dédiés à la virtualisation (Intel-VT ou AMD-V) permet à KVM d'être léger, élégant et peu gourmand en ressources. La contrepartie est qu'il ne fonctionne pas sur tous les ordinateurs, mais seulement ceux disposant de processeurs adaptés.  Pour les ordinateurs à base de x86, vous pouvez vérifier que votre processeur dispose des jeux d'instructions requis en cherchant « vmx » ou « svm » dans les drapeaux du processeur listés dans <filename>/proc/cpuinfo</filename>."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/proc</filename></primary><secondary><filename>/proc/cpuinfo</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization."
msgstr "Grâce à Red Hat soutenant activement son développement, KVM est plus ou moins devenu la référence pour la virtualisation sous Linux."

msgid "<primary><command>virt-install</command></primary>"
msgstr "<primary><command>virt-install</command></primary>"

#, fuzzy
#| msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The virtual <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgstr "Contrairement à des outils comme Virtualbox, KVM ne dispose pas en standard d'interface pour créer et gérer les machines virtuelles. Le paquet <emphasis role=\"pkg\">qemu-kvm</emphasis> se contente de fournir un exécutable du même nom (qui sert à démarrer une machine virtuelle) et un script de démarrage qui charge les modules nécessaires."

msgid "<primary>libvirt</primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>OpenVZ</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>UML</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <command>virtual-manager</command> is a graphical interface that uses libvirt to create and manage virtual machines."
msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare, and UML). <command>virt-manager</command> is a graphical interface that uses <emphasis>libvirt</emphasis> to create and manage virtual machines."
msgstr "Fort heureusement, Red Hat fournit aussi la solution à ce problème puisqu'ils développent <emphasis>libvirt</emphasis> et les outils associés <emphasis>virtual-manager</emphasis>. libvirt est une bibliothèque qui permet de gérer des machines virtuelles de manière uniforme, quelle que soit la technologie de virtualisation employée. À l'heure actuelle, libvirt gère QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare et UML. <command>virtual-manager</command> est une interface graphique exploitant libvirt pour créer et gérer des machines virtuelles."

msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>daemon</primary><secondary>libvirtd</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>libvirtd.service</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "We first install the required packages, with <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-bin</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgid "We first install the required packages, with <command>apt-get install libvirt-clients libvirt-daemon-system qemu-kvm virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-daemon-system</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. <emphasis role=\"pkg\">libvirt-clients</emphasis> provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgstr "Installons donc tous les paquets requis avec <command>apt-get install qemu-kvm libvirt-bin virtinst virtual-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-bin</emphasis> fournit le démon <command>libvirtd</command> qui sert à gérer des machines virtuelles (éventuellement à distance) et qui va mettre en route les machines virtuelles requises au démarrage du serveur. En outre, le paquet fournit <command>virsh</command>, un outil en ligne de commande qui permet de contrôler les machines virtuelles gérées par <command>libvirtd</command>."

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>virt-viewer</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "The <emphasis role=\"pkg\">virtinst</emphasis> package provides <command>virt-install</command>, which allows creating virtual machines from the command line. Finally, <emphasis role=\"pkg\">virt-viewer</emphasis> allows accessing a VM's graphical console."
msgstr "<emphasis role=\"pkg\">virtinst</emphasis> fournit quant à lui <command>virt-install</command> qui sert à créer des machines virtuelles depuis la ligne de commande. Enfin, <emphasis role=\"pkg\">virt-viewer</emphasis> permet d'accéder à la console graphique d'une machine virtuelle."

msgid "Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <xref linkend=\"sect.lxc.network\" />)."
msgstr "Tout comme avec Xen ou LXC, la configuration la plus courante pour des serveurs publics consiste à configurer un pont dans lequel seront intégrées les interfaces réseau des machines virtuelles (voir <xref linkend=\"sect.lxc.network\" />)."

msgid "Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network."
msgstr "Alternativement, la configuration par défaut employée par KVM est d'attribuer une adresse privée à la machine virtuelle (dans la plage 192.168.122.0/24) et de faire du NAT pour que la machine ait un accès au réseau extérieur."

msgid "The rest of this section assumes that the host has an <literal>eth0</literal> physical interface and a <literal>br0</literal> bridge, and that the former is connected to the latter."
msgstr "Dans la suite de cette section, nous supposerons qu'un pont <literal>br0</literal> a été configuré et que l'interface réseau physique <literal>eth0</literal> y a été intégrée."

msgid "Installation with <command>virt-install</command>"
msgstr "Installation avec <command>virt-install</command>"

msgid "Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line."
msgstr "La création d'une machine virtuelle est très similaire à l'installation d'une machine normale, sauf que toutes les caractéristiques de la machine sont décrites par une ligne de commande à rallonge."

msgid "Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <xref linkend=\"sect.remote-desktops\" /> for details), which will allow us to control the installation process."
msgstr "En pratique, cela veut dire que nous allons utiliser l'installateur Debian en démarrant sur un lecteur de DVD-Rom virtuel associé à une image ISO d'un DVD Debian. La machine virtuelle exportera la console graphique via le protocole VNC (voir explications en <xref linkend=\"sect.remote-desktops\" />) et nous pourrons donc contrôler le déroulement de l'installation par ce biais."

#, fuzzy
#| msgid "We first need to tell libvirtd where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgid "We first need to tell <command>libvirtd</command> where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgstr "En préalable, nous allons indiquer à <command>libvirtd</command> l'emplacement où nous allons stocker les images disques. Ce n'est nécessaire que si l'on souhaite utiliser un autre emplacement que celui par défaut (<filename>/var/lib/libvirt/images/</filename>)."

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
#| "<computeroutput>Pool srv-kvm created\n"
#| "\n"
#| "root@mirwiz:~# </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mkdir /srv/kvm\n"
"</userinput><computeroutput># </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm\n"
"</userinput><computeroutput>Pool srv-kvm created\n"
"\n"
"# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"

msgid "<emphasis>TIP</emphasis> Add your user to the libvirt group"
msgstr "<emphasis>ASTUCE</emphasis> Ajouter un utilisateur au groupe libvirt"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>group</primary><secondary><literal>libvirt</literal></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yourself to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgstr "Tous les exemples de cette section supposent que les commandes sont exécutées sous l'identité de root. En pratique, pour contrôler le démon libvirt local, il est nécessaire d'être soit root, soit un membre du groupe <literal>libvirt</literal> (ce qui n'est pas le cas par défaut). Ainsi, pour éviter d'utiliser les droits de root trop souvent, il est possible d'ajouter un utilisateur au groupe <literal>libvirt</literal>, ce qui permettra d'utiliser les différentes commandes sous l'identité de cet utilisateur."

msgid "Let us now start the installation process for the virtual machine, and have a closer look at <command>virt-install</command>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed."
msgstr "Lançons maintenant l'installation de la machine virtuelle et regardons de plus près la signification des options les plus importantes de <command>virt-install</command>. Cette commande va enregistrer la machine virtuelle et ses paramètres auprès de <command>libvirtd</command>, puis la démarrer une première fois afin que l'on puisse effectuer l'installation."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
#| "               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
#| "               --name testkvm            <co id=\"virtinst.name\"></co>\n"
#| "               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
#| "               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
#| "               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
#| "               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
#| "               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
#| "               --os-type linux           <co id=\"virtinst.os\"></co>\n"
#| "               --os-variant debianwheezy\n"
#| "</userinput><computeroutput>\n"
#| "Starting install...\n"
#| "Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
#| "Creating domain...                    |    0 B     00:00\n"
#| "Guest installation complete... restarting guest.\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --memory 2048             <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-11.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n"
"               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debiantesting\n"
"</userinput><computeroutput>\n"
"\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'\n"
"\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
"               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debianwheezy\n"
"</userinput><computeroutput>\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
"Creating domain...                    |    0 B     00:00\n"
"Guest installation complete... restarting guest.\n"
"</computeroutput>"

msgid "The <literal>--connect</literal> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<literal>/system</literal>) from others (<literal>/session</literal>)."
msgstr "L'option <literal>--connect</literal> permet d'indiquer l'hyperviseur à gérer. L'option prend la forme d'une URL indiquant à la fois la technologie de virtualisation (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, etc.) et la machine hôte (qui est laissée vide lorsque l'hôte est la machine locale). En outre, dans le cas de QEMU/KVM, chaque utilisateur peut gérer des machines virtuelles qui fonctionneront donc avec des droits limités et le chemin de l'URL permet de distinguer les machines « systèmes » (<literal>/system</literal>) des autres (<literal>/session</literal>)."

msgid "Since KVM is managed the same way as QEMU, the <literal>--virt-type kvm</literal> allows specifying the use of KVM even though the URL looks like QEMU."
msgstr "KVM se gérant de manière similaire à QEMU, l'option <literal>--virt-type kvm</literal> précise que l'on souhaite employer KVM même si l'URL de connexion précise indique QEMU."

msgid "The <literal>--name</literal> option defines a (unique) name for the virtual machine."
msgstr "L'option <literal>--name</literal> définit l'identifiant (unique) que l'on attribue à la machine virtuelle."

#, fuzzy
#| msgid "The <literal>--ram</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgid "The <literal>--memory</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgstr "L'option <literal>--ram</literal> définit la quantité de mémoire vive à allouer à la machine virtuelle (en Mo)."

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary><literal>qcow2</literal></primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>raw</literal>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space."
msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>qcow2</literal>) allows starting with a small file that only grows when the virtual machine starts actually using space."
msgstr "L'option <literal>--disk</literal> indique l'emplacement du fichier image qui va représenter le disque dur de notre machine virtuelle. Si le fichier n'existe pas, il est créé en respectant la taille en Go indiquée dans le paramètre <literal>size</literal>. Le paramètre <literal>format</literal> permet de stocker le disque virtuel de différentes manières. Le format par défaut (<literal>raw</literal>) est un fichier de la taille exacte du disque, copie exacte de son contenu. Le format retenu ici est un peu plus avancé (et spécifique à QEMU) et permet de démarrer avec un petit fichier dont la taille augmente au fur et à mesure que l'espace disque est réellement utilisé par la machine virtuelle."

msgid "The <literal>--cdrom</literal> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <literal>/dev/cdrom</literal>)."
msgstr "L'option <literal>--cdrom</literal> indique où trouver l'image ISO du CD-Rom qui va servir à démarrer l'installateur. On peut aussi bien indiquer un chemin local d'un fichier ISO, une URL où l'image peut être récupérée, ou encore un périphérique bloc correspondant à un vrai lecteur de CD-Rom (i.e. <literal>/dev/cdrom</literal>)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>virbr</literal> interface</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>libvirt</primary><secondary><literal>virbr</literal></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "<primary><literal>virbr</literal>, network interface</primary>"
msgstr ""

msgid "The <literal>--network</literal> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24)."
msgstr "L'option <literal>--network</literal> indique comment la carte réseau virtuelle doit s'intégrer dans la configuration réseau de l'hôte. Le comportement par défaut (que nous forçons ici) est de l'intégrer dans tout pont <foreignphrase>(bridge)</foreignphrase> pré-existant. En l'absence de pont, la machine virtuelle n'aura accès au LAN que par du NAT et obtient donc une adresse dans un sous-réseau privé (192.168.122.0/24)."

msgid "The default network configuration, which contains the definition for a <literal>virbr0</literal> bridge interface, can be edited using <command>virsh net-edit default</command> and started via <command>virsh net-start default</command> if not already done automatically during system start."
msgstr ""

#, fuzzy
#| msgid "<literal>--vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, the <literal>--vnclisten=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgid "<literal>--graphics vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, <literal>--graphics vnc,listen=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgstr "<literal>--vnc</literal> demande que la console graphique soit mise à disposition par VNC. Par défaut, le serveur VNC associé n'écoute que sur l'interface locale (<literal>localhost</literal>). Si le client VNC est exécuté depuis une autre machine, il faudra mettre en place un tunnel SSH pour établir la connexion (voir <xref linkend=\"sect.ssh-port-forwarding\" />). Alternativement, on peut passer l'option <literal>--vnclisten=0.0.0.0</literal> pour demander que le serveur VNC soit accessible depuis toutes les interfaces, mais dans ce cas vous avez intérêt à prévoir des règles adéquates dans le pare-feu."

msgid "The <literal>--os-type</literal> and <literal>--os-variant</literal> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there."
msgstr "Les options <literal>--os-type</literal> et <literal>--os-variant</literal> permettent d'optimiser quelques paramètres de la machine virtuelle en fonction des caractéristiques connues du système d'exploitation indiqué."

msgid "The full list of OS types can be shown using the <command>osinfo-query os</command> command from the <emphasis role=\"pkg\">libosinfo-bin</emphasis> package."
msgstr ""

msgid "At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <command>virt-viewer</command> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):"
msgstr "À ce stade, la machine virtuelle est démarrée et il faut se connecter à la console graphique pour effectuer l'installation. Si l'opération a été effectuée depuis un bureau graphique, la console graphique a été automatiquement lancée. Autrement, on peut en démarrer une sur un autre poste à l'aide de <command>virt-viewer</command> :"

msgid ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>serveur</replaceable>/system\n"
"</userinput><computeroutput>root@serveur's password: \n"
"root@serveur's password: </computeroutput>"

#, fuzzy
#| msgid "Installation with <command>virt-install</command>"
msgid "Connecting to installer session using <command>virt-viewer</command>"
msgstr "Installation avec <command>virt-install</command>"

msgid "When the installation process ends, the virtual machine is restarted, now ready for use."
msgstr "À la fin de l'installation, la machine virtuelle est redémarrée. Elle est désormais prête à l'emploi."

msgid "Managing Machines with <command>virsh</command>"
msgstr "Gestion des machines avec <command>virsh</command>"

msgid "Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <command>libvirtd</command> for the list of the virtual machines it manages:"
msgstr "L'installation étant terminée, il est temps de voir comment manipuler les machines virtuelles disponibles. La première commande permet de lister les machines gérées par <command>libvirtd</command> :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
#| " Id Name                 State\n"
#| "----------------------------------\n"
#| "  - testkvm              shut off\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  8 testkvm              shut off\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  - testkvm              shut off\n"
"</userinput>"

msgid "Let's start our test virtual machine:"
msgstr "Démarrons notre machine virtuelle de test :"

msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"

msgid "We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <command>vncviewer</command>):"
msgstr "Cherchons à obtenir les informations de connexion à la console graphique (le port d'affichage VNC renvoyé peut être passé en paramètre à <command>vncviewer</command>) :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
#| "</userinput><computeroutput>:0</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>127.0.0.1:0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>:0</computeroutput>"

msgid "Other available <command>virsh</command> subcommands include:"
msgstr "Parmi les autres commandes disponibles dans <command>virsh</command>, on trouve :"

msgid "<literal>reboot</literal> to restart a virtual machine;"
msgstr "<literal>reboot</literal> pour initier un redémarrage ;"

msgid "<literal>shutdown</literal> to trigger a clean shutdown;"
msgstr "<literal>shutdown</literal> pour arrêter proprement une machine virtuelle ;"

msgid "<literal>destroy</literal>, to stop it brutally;"
msgstr "<literal>destroy</literal> pour la stopper brutalement ;"

msgid "<literal>suspend</literal> to pause it;"
msgstr "<literal>suspend</literal> pour la mettre en pause ;"

msgid "<literal>resume</literal> to unpause it;"
msgstr "<literal>resume</literal> pour la sortir de pause ;"

msgid "<literal>autostart</literal> to enable (or disable, with the <literal>--disable</literal> option) starting the virtual machine automatically when the host starts;"
msgstr "<literal>autostart</literal> pour activer (ou désactiver lorsque l'option <literal>--disable</literal> est employée) le démarrage automatique d'une machine virtuelle au démarrage de l'hôte ;"

msgid "<literal>undefine</literal> to remove all traces of the virtual machine from <command>libvirtd</command>."
msgstr "<literal>undefine</literal> pour effacer toute trace de la machine virtuelle au sein de <command>libvirtd</command>."

msgid "All these subcommands take a virtual machine identifier as a parameter."
msgstr "Toutes ces commandes prennent en paramètre un identifiant de machine virtuelle."

#, fuzzy
#| msgid "Installing an RPM based system in Debian with yum"
msgid "Installing an RPM based chroot in Debian with yum"
msgstr "Installer un système basé sur RPM avec yum sur Debian"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>RPM</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary>chroot</primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>yum</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>rpm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility (available in the package of the same name)."
msgid "If a chroot is meant to run Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>. But if it is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility, available as <command>yum4</command> in the <emphasis role=\"pkg\">nextgen-yum4</emphasis> package, since the original program has been removed from Debian before the <emphasis role=\"distribution\">Bullseye</emphasis> release due to being unmaintained, outdated, and obsoleted by <command>dnf</command>."
msgstr "Si la machine virtuelle doit faire fonctionner Debian (ou une de ses dérivées), le système peut être initialisé avec <command>debootstrap</command>, comme décrit précédemment. En revanche si la machine virtuelle doit être installée avec un système basé sur RPM (comme Fedora, CentOS ou Scientific Linux), la mise en place sera faite avec l'outil <command>yum</command> (disponible dans le paquet de même nom)."

#, fuzzy
#| msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum</command> to extract the remaining set of packages. But since we call <command>yum</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum4</command> to extract the remaining set of packages. But since we call <command>yum4</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgstr "La procédure implique d'utiliser <command>rpm</command> pour extraire un ensemble de fichiers initiaux, notamment les fichiers de configuration de <command>yum</command>, puis d'appeler <command>yum</command> pour extraire le reste des paquets.  Mais comme <command>yum</command> est appelé depuis l'extérieur du chroot, il est nécessaire d'effectuer quelques changements temporaires.  Dans l'exemple ci-dessous, le chroot cible est situé dans <filename>/srv/centos</filename>."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
#| "</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
#| "</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
#| "rpm: However assuming you know what you are doing...\n"
#| "warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
#| "</userinput><computeroutput>[...]\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-9.2009.0.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-9.2009.0.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-9.2009.0.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum4 --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>chroot /srv/centos/\n"
"</userinput><computeroutput>[root@testsystem /]# </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"

msgid "Automated Installation"
msgstr "Installation automatisée"

msgid "<primary>deployment</primary>"
msgstr "<primary>déploiement</primary>"

msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The Falcot Corp administrators, like many administrators of large IT services, need tools to install (or reinstall) quickly, and automatically if possible, their new machines."
msgstr "Les administrateurs de Falcot SA, comme tous les administrateurs de parcs importants de machines, ont besoin d'outils pour installer (voire réinstaller) rapidement, et si possible automatiquement, leurs nouvelles machines."

msgid "These requirements can be met by a wide range of solutions. On the one hand, generic tools such as SystemImager handle this by creating an image based on a template machine, then deploy that image to the target systems; at the other end of the spectrum, the standard Debian installer can be preseeded with a configuration file giving the answers to the questions asked during the installation process. As a sort of middle ground, a hybrid tool such as FAI (<emphasis>Fully Automatic Installer</emphasis>) installs machines using the packaging system, but it also uses its own infrastructure for tasks that are more specific to massive deployments (such as starting, partitioning, configuration and so on)."
msgstr "Pour répondre à ces besoins, il y a différentes catégories de solutions : d'un côté, des outils génériques comme SystemImager qui gèrent cela en créant une image des fichiers d'une machine modèle qui peut ensuite être déployée sur les machines cibles ; de l'autre, debian-installer, l'installateur standard auquel on ajoute un fichier de configuration indiquant les réponses aux différentes questions posées au cours de l'installation. Entre les deux, on trouve un outil hybride comme FAI <foreignphrase>(Fully Automatic Installer)</foreignphrase> qui installe des machines en s'appuyant sur le système de paquetage, mais qui exploite sa propre infrastructure pour les autres tâches relevant du déploiement (démarrage, partitionnement, configuration, etc.)."

#, fuzzy
#| msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an i386 image won't fit on a powerpc machine, and so on)."
msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an amd64 image won't fit on a powerpc machine, and so on)."
msgstr "Chacune de ces solutions a des avantages et des inconvénients : SystemImager ne dépend pas d'un système de paquetage particulier et permet donc de gérer des parcs de machines exploitant plusieurs distributions de Linux. Il offre en outre un mécanisme de mise à jour du parc sans requérir une réinstallation. Mais pour que ces mises à jour soient fiables, il faut en contrepartie que les machines du parc ne changent pas indépendamment. Autrement dit, il n'est pas question que l'utilisateur puisse mettre à jour certains logiciels voire en installer de supplémentaires. De même, les mises à jour de sécurité, qui pourraient être automatisées, ne devront pas l'être puisque qu'elles devront transiter via l'image de référence. Enfin, cette solution nécessite un parc homogène de machines pour éviter de devoir jongler avec de trop nombreuses images. Il n'est pas question d'installer une image powerpc sur une machine i386 ."

#, fuzzy
#| msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly simple-CDD (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even the simple-CDD solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly <emphasis role=\"pkg\">simple-cdd</emphasis> (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even this solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgstr "Une installation automatisée avec debian-installer saura au contraire s'adapter aux spécificités des différentes machines : il récupérera le noyau et les logiciels dans les dépôts correspondants, détectera le matériel présent, partitionnera l'ensemble du disque pour exploiter tout l'espace disponible, installera le système Debian et configurera un chargeur de démarrage. En revanche, avec l'installateur standard, seules des versions Debian « standard » seront installées : c'est-à-dire le système de base plus les « tâches » que l'on aura présélectionnées. Impossible donc d'installer un profil très particulier comprenant des applications non empaquetées. Pour répondre à ces problématiques, il faut modifier l'installateur… Fort heureusement, ce dernier est très modulaire et des outils existent pour automatiser le plus gros de ce travail : il s'agit de simple-CDD (CDD est l'acronyme de <foreignphrase>Custom Debian Derivative</foreignphrase> — dérivée personnalisée de Debian). Même avec simple-CDD, cette solution ne répond qu'au besoin des installations initiales ; ce n'est pourtant pas jugé problématique puisque les outils APT permettent ensuite de déployer efficacement des mises à jour."

#, fuzzy
#| msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian), in order to focus more intently on debian-installer and simple-CDD, which are more interesting in a Debian-only context."
msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian, but available as a third-party package), in order to focus more intently on debian-installer and <emphasis role=\"pkg\">simple-cdd</emphasis>, which are more interesting in a Debian-only context."
msgstr "Nous n'aborderons que rapidement FAI et pas du tout SystemImager (qui ne fait plus partie de Debian), afin d'étudier plus en détail debian-installer et simple-CDD, les solutions les plus intéressantes dans un contexte où Debian est systématiquement employé."

msgid "Fully Automatic Installer (FAI)"
msgstr "Fully Automatic Installer (FAI)"

#, fuzzy
#| msgid "<primary>Fully Automatic Installer (FAI)</primary>"
msgid "<primary>Fully Automatic Installer</primary><see>FAI</see>"
msgstr "<primary><foreignphrase>Fully Automatic Installer (FAI)</foreignphrase></primary>"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>FAI</primary>"
msgstr "<primary>RAID</primary>"

msgid "<foreignphrase>Fully Automatic Installer</foreignphrase> is probably the oldest automated deployment system for Debian, which explains its status as a reference; but its very flexible nature only just compensates for the complexity it involves."
msgstr "<foreignphrase>Fully Automatic Installer</foreignphrase> est probablement la plus ancienne des solutions de déploiement automatisé de systèmes Debian. C'est pourquoi cet outil est très fréquemment cité ; mais sa grande souplesse compense difficilement sa relative complexité."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary>FAI</primary><secondary><emphasis role=\"pkg\">fai-server</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtual-manager</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary>FAI</primary><secondary><emphasis role=\"pkg\">fai-quickstart</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

msgid "FAI requires a server system to store deployment information and allow target machines to boot from the network. This server requires the <emphasis role=\"pkg\">fai-server</emphasis> package (or <emphasis role=\"pkg\">fai-quickstart</emphasis>, which also brings the required elements for a standard configuration)."
msgstr "Pour exploiter cette solution, il faut un système serveur qui va permettre de stocker les informations de déploiement et de démarrer les machines depuis le réseau. On y installera le paquet <emphasis role=\"pkg\">fai-server</emphasis> (ou <emphasis role=\"pkg\">fai-quickstart</emphasis> si on veut forcer l'installation de tous les éléments nécessaires pour une configuration relativement standard)."

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/fai/</filename></secondary><see>FAI</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><filename>/etc/fai/nfsroot.conf</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> according to <filename>/etc/fai/nfsroot.conf</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgstr "En ce qui concerne la définition des différents profils installables, FAI emploie une approche différente. Au lieu d'avoir une installation de référence que l'on se contente de dupliquer, FAI est un installateur à part entière mais qui est totalement paramétrable par un ensemble de fichiers et de scripts stockés sur le serveur : l'emplacement par défaut est <filename>/srv/fai/config/</filename>, mais ce répertoire n'existe pas, charge à l'administrateur donc de créer tous les fichiers nécessaires. En général, on s'inspirera des exemples que l'on trouve dans la documentation disponible dans le paquet <emphasis role=\"pkg\">fai-doc</emphasis> et plus particulièrement dans <filename>/usr/share/doc/fai-doc/examples/simple/</filename>."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai-setup</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai-cd</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgstr "Une fois ces profils totalement définis, il faut exécuter <command>fai-setup</command> pour régénérer les différents éléments nécessaires au démarrage d'une installation par FAI ; il s'agit essentiellement de préparer (ou mettre à jour) un système minimal (NFSROOT) qui est employé pendant l'installation. Alternativement, il est possible de générer un CD d'amorçage de l'installation avec <command>fai-cd</command>."

msgid "Creating all these configuration files requires some understanding of the way FAI works. A typical installation process is made of the following steps:"
msgstr "Avant d'être à même de créer tous ces fichiers de configuration, il convient d'avoir une bonne idée du fonctionnement de FAI. Une installation typique enchaîne les étapes suivantes :"

msgid "fetching a kernel from the network, and booting it;"
msgstr "récupération et démarrage du noyau par le réseau ;"

msgid "mounting the root filesystem from NFS;"
msgstr "montage du système racine par NFS (le <foreignphrase>nfsroot</foreignphrase> mentionné précédemment) ;"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "executing <command>/usr/sbin/fai</command>, which controls the rest of the process (the next steps are therefore initiated by this script);"
msgstr "exécution de <command>/usr/sbin/fai</command> qui contrôle le reste de l'installation (les étapes suivantes sont donc initiées par ce script) ;"

msgid "copying the configuration space from the server into <filename>/fai/</filename>;"
msgstr "récupération de l'espace de configuration depuis le serveur et mise à disposition dans <filename>/fai/</filename> ;"

msgid "running <command>fai-class</command>. The <filename>/fai/class/[0-9][0-9]*</filename> scripts are executed in turn, and return names of “classes” that apply to the machine being installed; this information will serve as a base for the following steps. This allows for some flexibility in defining the services to be installed and configured."
msgstr "appel de <command>fai-class</command>. Les scripts <filename>/fai/class/[0-9][0-9]*</filename> sont exécutés et retournent des noms de « classe » qui doivent être appliqués à la machine en cours d'installation ; cette information sera réutilisée par les différentes étapes à suivre. Il s'agit d'un moyen relativement souple de définir les services qui doivent être installés et configurés."

msgid "fetching a number of configuration variables, depending on the relevant classes;"
msgstr "récupération d'un certain nombre de variables de configuration en fonction des classes définies ;"

msgid "partitioning the disks and formatting the partitions, based on information provided in <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;"
msgstr "partitionnement des disques et formatage des partitions à partir des informations fournies dans <filename>/fai/disk_config/<replaceable>classe</replaceable></filename> ;"

msgid "mounting said partitions;"
msgstr "montage des partitions ;"

msgid "installing the base system;"
msgstr "installation d'un système de base ;"

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary>FAI</primary><secondary><command>fai-debconf</command></secondary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "preseeding the Debconf database with <command>fai-debconf</command>;"
msgstr "préconfiguration de la base Debconf avec <command>fai-debconf</command> ;"

msgid "fetching the list of available packages for APT;"
msgstr "téléchargement de la liste des paquets disponibles pour APT ;"

msgid "installing the packages listed in <filename>/fai/package_config/<replaceable>class</replaceable></filename>;"
msgstr "installation des logiciels listés dans les fichiers <filename>/fai/package_config/<replaceable>classe</replaceable></filename> ;"

msgid "executing the post-configuration scripts, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;"
msgstr "exécution des scripts de post-configuration <filename>/fai/scripts/<replaceable>classe</replaceable>/[0-9][0-9]*</filename> ;"

msgid "recording the installation logs, unmounting the partitions, and rebooting."
msgstr "enregistrement des logs de l'installation, démontage des partitions, redémarrage."

msgid "Preseeding Debian-Installer"
msgstr "Debian-installer avec préconfiguration"

msgid "<primary>preseed</primary>"
msgstr "<primary>preseed</primary>"

msgid "<primary>preconfiguration</primary>"
msgstr "<primary>préconfiguration</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>installation</primary><secondary>preseeding</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <emphasis>preseeding</emphasis>."
msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <foreignphrase>preseeding</foreignphrase>."
msgstr "En fin de compte, le meilleur outil pour installer des systèmes Debian devrait logiquement être l'installateur officiel de Debian. C'est pourquoi, dès la conception de debian-installer, il a été prévu de l'employer de manière automatique. Il s'appuie pour cela sur le mécanisme offert par <emphasis role=\"pkg\">debconf</emphasis>. Celui-ci permet d'une part de restreindre le nombre de questions posées, les autres obtenant automatiquement la réponse par défaut et d'autre part, de fournir séparément toutes les réponses afin que l'installation puisse être non interactive. Cette dernière fonctionnalité porte le nom de <foreignphrase>preseeding</foreignphrase>, que l'on traduira simplement par préconfiguration."

msgid "<emphasis>GOING FURTHER</emphasis> Debconf with a centralized database"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Debconf avec une base de données centralisée"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">debconf-doc</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Preseeding allows to provide a set of answers to Debconf questions at installation time, but these answers are static and do not evolve as time passes. Since already-installed machines may need upgrading, and new answers may become required, the <filename>/etc/debconf.conf</filename> configuration file can be set up so that Debconf uses external data sources (such as an LDAP directory server, or a remote file accessed via NFS or Samba). Several external data sources can be defined at the same time, and they complement one another. The local database is still used (for read-write access), but the remote databases are usually restricted to reading. The <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual page describes all the possibilities in detail (you need the <emphasis role=\"pkg\">debconf-doc</emphasis> package)."
msgstr "La préconfiguration permet de fournir un ensemble de réponses au moment de l'installation, mais celui-ci n'évolue pas dans le temps. Pour répondre à cette problématique qui concerne essentiellement la mise à jour de machines déjà installées, il est possible de paramétrer debconf via son fichier de configuration <filename>/etc/debconf.conf</filename> pour lui demander d'utiliser des sources de données externes (comme LDAP ou un fichier distant accessible par NFS ou Samba). Les sources peuvent être multiples et complémentaires. La base de données locale reste employée en lecture-écriture mais les autres bases de données sont généralement accessibles en lecture seule uniquement. La page de manuel <citerefentry><refentrytitle>debconf.conf</refentrytitle><manvolnum>5</manvolnum></citerefentry> détaille toutes les possibilités offertes (pour cela il faut installer le paquet <emphasis role=\"pkg\">debconf-doc</emphasis>)."

msgid "<primary><command>debconf</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "Using a Preseed File"
msgstr "Employer un fichier de préconfiguration"

msgid "There are several places where the installer can get a preseeding file:"
msgstr "L'installateur peut récupérer un fichier de préconfiguration à différents emplacements :"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary><filename>preseed.cfg</filename></primary>"
msgstr "<primary>preseed</primary>"

msgid "in the initrd used to start the machine; in this case, preseeding happens at the very beginning of the installation, and all questions can be avoided. The file just needs to be called <filename>preseed.cfg</filename> and stored in the initrd root."
msgstr "dans l'initrd employé pour démarrer la machine — dans ce cas, la préconfiguration a lieu au tout début de l'installation et toutes les questions peuvent être évitées par ce biais. Il suffit de nommer le fichier <filename>preseed.cfg</filename> et de le placer à la racine de l'initrd."

msgid "on the boot media (CD or USB key); preseeding then happens as soon as the media is mounted, which means right after the questions about language and keyboard layout. The <literal>preseed/file</literal> boot parameter can be used to indicate the location of the preseeding file (for instance, <filename>/cdrom/preseed.cfg</filename> when the installation is done off a CD-ROM, or <filename>/hd-media/preseed.cfg</filename> in the USB-key case)."
msgstr "sur le support de démarrage (CD-Rom ou clé USB) — dans ce cas, la préconfiguration a lieu dès que le support en question est monté, soit juste après les questions concernant la langue et le clavier. Le paramètre de démarrage <literal>preseed/file</literal> permet d'indiquer l'emplacement du fichier de préconfiguration (ex : <filename>/cdrom/preseed.cfg</filename> si l'on emploie un CD-Rom ou <filename>/hd-media/preseed.cfg</filename> pour une clé USB)."

#, fuzzy
#| msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."
msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal> (HTTPS, FTPS, SFTP, etc. are not supported)."
msgstr "depuis le réseau — la préconfiguration n'a alors lieu qu'après la configuration (automatique) du réseau et le paramètre de démarrage à employer est de la forme <literal>preseed/url=http://<replaceable>serveur</replaceable>/preseed.cfg</literal>."

msgid "At a glance, including the preseeding file in the initrd looks like the most interesting solution; however, it is rarely used in practice, because generating an installer initrd is rather complex. The other two solutions are much more common, especially since boot parameters provide another way to preseed the answers to the first questions of the installation process. The usual way to save the bother of typing these boot parameters by hand at each installation is to save them into the configuration for <command>isolinux</command> (in the CD-ROM case) or <command>syslinux</command> (USB key)."
msgstr "Inclure le fichier de préconfiguration dans l'initrd semble au premier abord la solution la plus intéressante, mais on ne l'emploiera que très rarement, en raison de la complexité de génération d'un initrd adapté à l'installateur. Les deux autres solutions seront donc privilégiées, d'autant plus qu'il existe un autre moyen de préconfigurer les premières questions de l'installation via les paramètres de démarrage. Pour éviter de les saisir manuellement, il faudra simplement modifier la configuration de <command>isolinux</command> (démarrage sur CD-Rom) ou <command>syslinux</command> (démarrage sur clé USB)."

msgid "Creating a Preseed File"
msgstr "Créer un fichier de préconfiguration"

msgid "A preseed file is a plain text file, where each line contains the answer to one Debconf question. A line is split across four fields separated by whitespace (spaces or tabs), as in, for instance, <literal>d-i mirror/suite string stable</literal>:"
msgstr "Un fichier de préconfiguration est un simple fichier texte où chaque ligne contient une réponse à une question Debconf. Les questions se décomposent en 4 champs séparés par des blancs (espaces ou tabulations) comme dans l'exemple <literal>d-i mirror/suite string stable</literal> :"

msgid "the first field is the “owner” of the question; “d-i” is used for questions relevant to the installer, but it can also be a package name for questions coming from Debian packages;"
msgstr "Le premier champ est le propriétaire de la question ; on y met <literal>d-i</literal> pour les questions concernant l'installateur, ou le nom du paquet pour les questions Debconf employées par les paquets Debian;"

#, fuzzy
#| msgid "the second field is an identifier for the question;"
msgid "the second field is an identifier for the question (the template name);"
msgstr "Le deuxième champ est l'identifiant de la question;"

msgid "third, the type of question;"
msgstr "Le troisième champ est le type de la question;"

msgid "the fourth and last field contains the value for the answer. Note that it must be separated from the third field with a single space; if there are more than one, the following space characters are considered part of the value."
msgstr "Et enfin, le quatrième champ contient la valeur de la réponse. Signalons qu'un espace unique sépare le type de la valeur ; s'il y en a plus qu'un, les espaces suivants feront partie de la valeur."

msgid "The simplest way to write a preseed file is to install a system by hand. Then <command>debconf-get-selections --installer</command> will provide the answers concerning the installer. Answers about other packages can be obtained with <command>debconf-get-selections</command>. However, a cleaner solution is to write the preseed file by hand, starting from an example and the reference documentation: with such an approach, only questions where the default answer needs to be overridden can be preseeded; using the <literal>priority=critical</literal> boot parameter will instruct Debconf to only ask critical questions, and use the default answer for others."
msgstr "Le moyen le plus simple de rédiger un fichier de préconfiguration est d'installer manuellement un système. On récupère ensuite toutes les réponses concernant debian-installer avec <command>debconf-get-selections --installer</command> ; pour les réponses concernant les paquets, on utilise <command>debconf-get-selections</command>. Toutefois, il est plus propre de rédiger un tel fichier manuellement à partir d'un exemple et de la documentation de référence : on ne préconfigurera une réponse que lorsque la réponse par défaut ne convient pas et pour le reste, on s'appuiera sur le paramètre de démarrage <literal>priority=critical</literal> qui restreint l'affichage aux seules questions critiques."

msgid "Pre-setting a value in a preseed file automatically instructs the Debian installer to not ask that question. This happens, because loading the preseed file does not just set the given value(s), but also marks each of the affected dialogs as “seen“ by the user. Thus it is possible to pre-set a question's value and still present the dialog to the user by resetting the “seen“ flag. Beware that order in this case matters and that the value has to be preseeded before setting the dialog to “unseen“ as shown in the following example:"
msgstr ""

msgid ""
"d-i netcfg/hostname string worker\n"
"d-i netcfg/hostname seen false"
msgstr ""

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary><command>debconf-get-selections</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "<emphasis>DOCUMENTATION</emphasis> Installation guide appendix"
msgstr "<emphasis>DOCUMENTATION</emphasis> Annexe du manuel de l'installateur"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>preseed</primary><secondary>all templates</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"
msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. There are also collections of all debconf templates extracted from each component and suite of Debian: <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/apb\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/example-preseed.txt\" /> <ulink type=\"block\" url=\"https://preseed.debian.net/\" />"
msgstr "L'utilisation d'un fichier de préconfiguration est très bien documentée dans une annexe du manuel de l'installateur que l'on trouve en ligne. Il fournit également un exemple détaillé et commenté d'un fichier de préconfiguration que l'on pourra reprendre et adapter à sa guise. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"

msgid "Preseeding an installation is often not as straightforward as one would wish. It sometimes requires to understand how packages process the given values in their scripts. Don't hesitate to ask on the <email>debian-cd@lists.debian.org</email> mailing list or in the <literal>#debian-cd</literal> IRC channel if you require help. Also be aware that some complex setups still cannot be achieved by preseeding."
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>mailing lists</primary><secondary><email>debian-cd@lists.debian.org</email></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Creating a Customized Boot Media"
msgstr "Créer un support de démarrage adapté"

msgid "Knowing where to store the preseed file is all very well, but the location isn't everything: one must, one way or another, alter the installation boot media to change the boot parameters and add the preseed file."
msgstr "Ce n'est pas tout de savoir où il faut mettre le fichier de préconfiguration, encore faut-il savoir comment le faire. En effet, il faut d'une manière ou d'une autre modifier le support de démarrage de l'installation pour y changer les paramètres de démarrage et pour y ajouter le fichier."

msgid "Booting From the Network"
msgstr "Démarrage depuis le réseau"

#, fuzzy
#| msgid "<primary>LXC</primary>"
msgid "<primary>PXE</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"
msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/ch04s05\" />"
msgstr "Lorsqu'on démarre un ordinateur depuis le réseau, c'est le serveur chargé d'envoyer les éléments de démarrage qui en définit les paramètres. C'est donc la configuration PXE sur le serveur de démarrage qu'il faut aller modifier et en particulier le fichier <filename>/tftpboot/pxelinux.cfg/default</filename>. La mise en place du démarrage par le réseau est un prérequis ; elle est détaillée dans le manuel d'installation. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"

msgid "Preparing a Bootable USB Key"
msgstr "Préparer une clé USB amorçable"

#, fuzzy
#| msgid "<primary><emphasis>VirtualBox</emphasis></primary>"
msgid "<primary><filename>syslinux.cfg</filename></primary>"
msgstr "<primary><emphasis>VirtualBox</emphasis></primary>"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>syslinux</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>isolinux</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>Icinga</primary>"
msgid "<primary><filename>grub.cfg</filename></primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>:"
msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>, copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>."
msgstr "Après avoir préparé une clé amorçable comme documenté dans la <xref linkend=\"sect.install-usb\" />, il ne reste plus que quelques opérations à effectuer (on suppose le contenu de la clé accessible via <filename>/media/usbdisk/</filename>) :"

msgid "If you have been using a hybrid ISO image to create the bootable USB stick, then you have to edit <filename>/media/usbdisk/boot/grub/grub.cfg</filename> (for the EFI boot screen):"
msgstr ""

#, fuzzy
#| msgid "syslinux.cfg file and preseeding parameters"
msgid "boot/grub/grub.cfg file and preseeding parameters"
msgstr "Fichier syslinux.cfg et paramètres pour la préconfiguration"

#, fuzzy
#| msgid ""
#| "default vmlinuz\n"
#| "append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgid ""
"menuentry --hotkey=i 'Install' {\n"
"    set background_color=black\n"
"    linux    /install.amd/vmlinuz preseed/file=/cdrom/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 --- quiet \n"
"    initrd   /install.amd/initrd.gz\n"
"}"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=fr_FR console-keymaps-at/keymap=fr-latin9 languagechooser/language-name=French countrychooser/shortlist=FR vga=normal initrd=initrd.gz  --"

msgid "And you have to edit <filename>/media/usbdisk/isolinux/isolinux.cfg</filename> (for BIOS boot) or one of the files it utilizes - e.g. <filename>/media/usbdisk/isolinux/txt.cfg</filename> - to add required boot parameters:"
msgstr ""

#, fuzzy
#| msgid "syslinux.cfg file and preseeding parameters"
msgid "isolinux/txt.cfg file and preseeding parameters"
msgstr "Fichier syslinux.cfg et paramètres pour la préconfiguration"

#, fuzzy
#| msgid ""
#| "default vmlinuz\n"
#| "append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgid ""
"label install\n"
"        menu label ^Install\n"
"        kernel [...]\n"
"        append preseed/file=/cdrom/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=/install.amd/initrd.gz --- quiet"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=fr_FR console-keymaps-at/keymap=fr-latin9 languagechooser/language-name=French countrychooser/shortlist=FR vga=normal initrd=initrd.gz  --"

#, fuzzy
#| msgid "edit <filename>/media/usbdisk/syslinux.cfg</filename> and add required boot parameters (see example below)."
msgid "If you have been using the <filename>hd-media</filename> installer image for a custom USB stick, edit <filename>/media/usbdisk/syslinux.cfg</filename> and add the required boot parameters as shown in the example below:"
msgstr "modifier <filename>/media/usbdisk/syslinux.cfg</filename> pour y ajouter les paramètres souhaités (voir un exemple ci-dessous)."

msgid "syslinux.cfg file and preseeding parameters"
msgstr "Fichier syslinux.cfg et paramètres pour la préconfiguration"

msgid ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=fr_FR console-keymaps-at/keymap=fr-latin9 languagechooser/language-name=French countrychooser/shortlist=FR vga=normal initrd=initrd.gz  --"

msgid "Creating a CD-ROM Image"
msgstr "Créer une image de CD-Rom"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">debian-cd</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>genisoimage</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>mkisofs</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>xorriso</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "A USB key is a read-write media, so it was easy for us to add a file there and change a few parameters. In the CD-ROM case, the operation is more complex, since we need to regenerate a full ISO image. This task is handled by <emphasis role=\"pkg\">debian-cd</emphasis>, but this tool is rather awkward to use: it needs a local mirror, and it requires an understanding of all the options provided by <filename>/usr/share/debian-cd/CONF.sh</filename>; even then, <command>make</command> must be invoked several times. <filename>/usr/share/debian-cd/README</filename> is therefore a very recommended read."
msgstr "Une clé USB étant un support accessible en lecture/écriture, il est facile d'y ajouter un fichier et de modifier quelques paramètres. Ce n'est plus le cas avec un CD-Rom : nous devons régénérer une image ISO d'installation de Debian. C'est précisément le rôle de <emphasis role=\"pkg\">debian-cd</emphasis>. Malheureusement, cet outil est assez contraignant à l'usage. Il faut en effet disposer d'un miroir Debian local, prendre le temps de comprendre toutes les options offertes par <filename>/usr/share/debian-cd/CONF.sh</filename>, puis enchaîner des invocations de <command>make</command>. La lecture de <filename>/usr/share/debian-cd/README</filename> s'avérera nécessaire."

#, fuzzy
#| msgid "Having said that, debian-cd always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific file is <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgid "Having said that, <emphasis role=\"pkg\">debian-cd</emphasis> always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific files are <filename>$TDIR/$CODENAME/CD1/isolinux/isolinux.cfg</filename> and <filename>$TDIR/$CODENAME/CD1/boot/grub/grub.cfg</filename> as shown above). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgstr "Cela dit, debian-cd procède toujours de la même manière : il crée un répertoire « image » qui contient exactement ce que le CD-Rom devra contenir, puis emploie un programme (<command>genisoimage</command>, <command>mkisofs</command> ou <command>xorriso</command>) pour transformer ce répertoire en fichier ISO. Le répertoire image est finalisé juste après l'étape <command>make image-trees</command> de debian-cd. À ce moment, au lieu de procéder directement à la génération du fichier ISO, on peut déposer le fichier de préconfiguration dans ce fameux répertoire (qui se trouve être <filename>$TDIR/$CODENAME/CD1/</filename>, <filename>$TDIR</filename> et <filename>$CODENAME</filename> étant des paramètres fournis par le fichier de configuration <filename>CONF.sh</filename>). Le chargeur d'amorçage du CD-Rom est <command>isolinux</command> ; la configuration préparée par debian-cd doit également être modifiée à ce moment, afin d'ajouter les paramètres de démarrage souhaités (le fichier précis à éditer est <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Finalement, il n'y a plus qu'à générer l'image ISO avec <command>make image CD=1</command> (ou <command>make images</command> si l'on génère un jeu de CD-Rom)."

msgid "Simple-CDD: The All-In-One Solution"
msgstr "Simple-CDD : la solution tout en un"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">simple-cdd</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Simply using a preseed file is not enough to fulfill all the requirements that may appear for large deployments. Even though it is possible to execute a few scripts at the end of the normal installation process, the selection of the set of packages to install is still not quite flexible (basically, only “tasks” can be selected); more important, this only allows installing official Debian packages, and precludes locally-generated ones."
msgstr "L'emploi d'un fichier de préconfiguration ne répond pas à tous les besoins liés à un déploiement de parc informatique. Même s'il est possible d'exécuter quelques scripts à la fin de l'installation, la souplesse de sélection des paquets à installer reste limitée (on sélectionne essentiellement les tâches) et surtout cela ne permet pas d'installer des paquets locaux ne provenant pas de Debian."

#, fuzzy
#| msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what Simple-CDD (in the <emphasis role=\"pkg\">simple-cdd</emphasis> package) does."
msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what <emphasis role=\"pkg\">simple-cdd</emphasis> does."
msgstr "Pourtant debian-cd sait intégrer des paquets externes et debian-installer peut être étendu en insérant des étapes au cours de l'installation. En combinant ces deux facultés, il est donc possible de créer un installateur répondant à nos besoins, qui pourrait même configurer certains services après avoir procédé au décompactage des paquets désirés. Ce qui vient d'être décrit, ce n'est pas qu'une vue de l'esprit, c'est exactement le service que Simple-CDD (dans le paquet <emphasis role=\"pkg\">simple-cdd</emphasis>) propose ."

#, fuzzy
#| msgid "The purpose of Simple-CDD is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgid "The purpose of this tool is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgstr "Simple-CDD se veut un outil permettant à tout un chacun de créer facilement une distribution dérivée de Debian en sélectionnant un sous-ensemble de paquets, en les préconfigurant avec debconf, en y intégrant quelques logiciels spécifiques et en exécutant des scripts personnalisés à la fin de l'installation. On retrouve bien là la philosophie du système d'exploitation universel que chacun peut adapter pour ses besoins."

msgid "Creating Profiles"
msgstr "Définir des profils"

msgid "Simple-CDD defines “profiles” that match the FAI “classes” concept, and a machine can have several profiles (determined at installation time). A profile is defined by a set of <filename>profiles/<replaceable>profile</replaceable>.*</filename> files:"
msgstr "À l'instar des « classes » de FAI, Simple-CDD permet de créer des « profils » et, au moment de l'installation, on décide de quels profils une machine va hériter. Un profil se définit par un ensemble de fichiers <filename>profiles/<replaceable>profil</replaceable>.*</filename> :"

msgid "the <filename>.description</filename> file contains a one-line description for the profile;"
msgstr "Le fichier <filename>.description</filename> contient une ligne de description du profil;"

msgid "the <filename>.packages</filename> file lists packages that will automatically be installed if the profile is selected;"
msgstr "Le fichier <filename>.packages</filename> liste les paquets qui seront automatiquement installés si le profil est sélectionné;"

msgid "the <filename>.downloads</filename> file lists packages that will be stored onto the installation media, but not necessarily installed;"
msgstr "Le fichier <filename>.downloads</filename> liste des paquets qui seront intégrés sur l'image d'installation mais qui ne seront pas nécessairement installés;"

msgid "the <filename>.preseed</filename> file contains preseeding information for Debconf questions (for the installer and/or for packages);"
msgstr "Le fichier <filename>.preseed</filename> contient une préconfiguration de questions debconf (aussi bien pour l'installateur que pour les paquets);"

msgid "the <filename>.postinst</filename> file contains a script that will be run at the end of the installation process;"
msgstr "Le fichier <filename>.postinst</filename> contient un script qui est exécuté sur le système installé juste avant la fin de l'installation;"

#, fuzzy
#| msgid "lastly, the <filename>.conf</filename> file allows changing some Simple-CDD parameters based on the profiles to be included in an image."
msgid "lastly, the <filename>.conf</filename> file allows changing some parameters based on the profiles to be included in an image."
msgstr "Et enfin le fichier <filename>.conf</filename> permet de modifier les paramètres de Simple-CDD en fonction des profils inclus dans une image."

msgid "The <literal>default</literal> profile has a particular role, since it is always selected; it contains the bare minimum required for Simple-CDD to work. The only thing that is usually customized in this profile is the <literal>simple-cdd/profiles</literal> preseed parameter: this allows avoiding the question, introduced by Simple-CDD, about what profiles to install."
msgstr "Le profil <literal>default</literal> est particulier puisqu'il est systématiquement employé et contient le strict minimum pour que Simple-CDD puisse fonctionner. La seule chose qu'il soit intéressant de personnaliser dans ce profil est le paramètre de préconfiguration <literal>simple-cdd/profiles</literal> : on peut ainsi éviter une question introduite par Simple-CDD et qui demande la liste des profils qui doivent être installés."

msgid "Note also that the commands will need to be invoked from the parent directory of the <filename>profiles</filename> directory."
msgstr "Signalons également qu'il faudra invoquer la commande depuis le répertoire parent de ce répertoire <filename>profiles</filename>."

msgid "Configuring and Using <command>build-simple-cdd</command>"
msgstr "Configuration et fonctionnement de <command>build-simple-cdd</command>"

msgid "<primary><command>build-simple-cdd</command></primary>"
msgstr "<primary><command>build-simple-cdd</command></primary>"

msgid "<emphasis>QUICK LOOK</emphasis> Detailed configuration file"
msgstr "<emphasis>DÉCOUVERTE</emphasis> Fichier de configuration détaillé"

#, fuzzy
#| msgid "An example of a Simple-CDD configuration file, with all possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). This can be used as a starting point when creating a custom configuration file."
msgid "An example of a Simple-CDD configuration file, with most possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed</filename>). This can be used as a starting point when creating a custom configuration file. Unfortunately not everything is documented there, so some variables are only listed and explained in <filename>/usr/lib/python3/dist-packages/simple_cdd/variables.py</filename>."
msgstr "Un exemple de fichier de configuration pour Simple-CDD contenant tous les paramètres existants se trouve dans le paquet, il s'agit de <filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>. On peut s'en inspirer pour rédiger son propre fichier de configuration."

msgid "It is further important to familiarize yourself with the variables understood by <filename>/usr/share/debian-cd/CONF.sh</filename>."
msgstr ""

msgid "Simple-CDD requires many parameters to operate fully. They will most often be gathered in a configuration file, which <command>build-simple-cdd</command> can be pointed at with the <literal>--conf</literal> option, but they can also be specified via dedicated parameters given to <command>build-simple-cdd</command>. Here is an overview of how this command behaves, and how its parameters are used:"
msgstr "Afin de pouvoir faire son œuvre, il faut fournir à Simple-CDD toute une série d'informations. Le plus pratique est de les regrouper dans un fichier de configuration que l'on transmettra à <command>build-simple-cdd</command> par son option <literal>--conf</literal>. Mais elles peuvent parfois être spécifiées par le biais de paramètres dédiés de <command>build-simple-cdd</command>. Passons en revue le fonctionnement de cette commande et l'influence des différents paramètres :"

msgid "the <literal>profiles</literal> parameter lists the profiles that will be included on the generated CD-ROM image;"
msgstr "Le paramètre <literal>profiles</literal> liste les profils à inclure sur l'image de CD-Rom générée;"

msgid "based on the list of required packages, Simple-CDD downloads the appropriate files from the server mentioned in <literal>server</literal>, and gathers them into a partial mirror (which will later be given to debian-cd);"
msgstr "À partir de la liste des paquets requis, Simple-CDD recrée un miroir Debian partiel (qu'il passera en paramètre à debian-cd plus tard) en téléchargeant les fichiers nécessaires depuis le serveur mentionné dans <literal>server</literal>;"

msgid "the custom packages mentioned in <literal>local_packages</literal> are also integrated into this local mirror;"
msgstr "Il intègre dans ce miroir local les paquets Debian personnalisés listés dans <literal>local_packages</literal>;"

msgid "debian-cd is then executed (within a default location that can be configured with the <literal>debian_cd_dir</literal> variable), with the list of packages to integrate;"
msgstr "Il exécute debian-cd (dont l'emplacement par défaut peut être configuré grâce à la variable <literal>debian_cd_dir</literal>) en lui passant la liste des paquets à intégrer;"

msgid "once debian-cd has prepared its directory, Simple-CDD applies some changes to this directory:"
msgstr "Il interfère sur le répertoire préparé par debian-cd de plusieurs manières :"

msgid "files containing the profiles are added in a <filename>simple-cdd</filename> subdirectory (that will end up on the CD-ROM);"
msgstr "Il dépose les fichiers concernant les profils dans un répertoire <filename>simple-cdd</filename> sur le CD-Rom;"

msgid "other files listed in the <literal>all_extras</literal> parameter are also added;"
msgstr "Il ajoute les fichiers listés par le paramètre <literal>all_extras</literal>;"

msgid "the boot parameters are adjusted so as to enable the preseeding. Questions concerning language and country can be avoided if the required information is stored in the <literal>language</literal> and <literal>country</literal> variables."
msgstr "Il rajoute des paramètres de démarrage pour activer la préconfiguration et pour éviter les premières questions concernant la langue et le pays. Il récupère ces informations depuis les paramètres <literal>language</literal> et <literal>country</literal>."

msgid "debian-cd then generates the final ISO image."
msgstr "Il demande à debian-cd de générer l'image ISO finale."

msgid "Generating an ISO Image"
msgstr "Générer une image ISO"

#, fuzzy
#| msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-8.0-amd64-CD-1.iso</filename>."
msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-11-amd64-CD-1.iso</filename>."
msgstr "Une fois le fichier de configuration rédigé et les profils correctement définis, il ne reste donc plus qu'à invoquer <command>build-simple-cdd --conf simple-cdd.conf</command>. Après quelques minutes, on obtient alors l'image souhaitée : <filename>images/debian-8.0-amd64-CD-1.iso</filename>."

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>monitoring</primary>"
msgstr "<primary>Munin</primary>"

msgid "<primary>Munin</primary>"
msgstr "<primary>Munin</primary>"

msgid "<primary>Nagios</primary>"
msgstr "<primary>Nagios</primary>"

msgid "Monitoring is a generic term, and the various involved activities have several goals: on the one hand, following usage of the resources provided by a machine allows anticipating saturation and the subsequent required upgrades; on the other hand, alerting the administrator as soon as a service is unavailable or not working properly means that the problems that do happen can be fixed sooner."
msgstr "La supervision couvre plusieurs aspects et répond à plusieurs problématiques. D'une part, il s'agit de suivre dans le temps l'évolution de l'usage des ressources offertes par une machine donnée, afin d'anticiper la saturation et les besoins de mises à jour. D'autre part, il s'agit d'être alerté en cas d'indisponibilité ou de dysfonctionnement d'un service afin d'y remédier dans les plus brefs délais."

msgid "<emphasis>Munin</emphasis> covers the first area, by displaying graphical charts for historical values of a number of parameters (used RAM, occupied disk space, processor load, network traffic, Apache/MySQL load, and so on). <emphasis>Nagios</emphasis> covers the second area, by regularly checking that the services are working and available, and sending alerts through the appropriate channels (e-mails, text messages, and so on). Both have a modular design, which makes it easy to create new plug-ins to monitor specific parameters or services."
msgstr "<emphasis>Munin</emphasis> répond très bien à la première problématique en proposant sous forme graphique des historiques de nombreux paramètres (usage mémoire vive, usage disque, charge processeur, trafic réseau, charge de Apache/MySQL, etc.). <emphasis>Nagios</emphasis> répond à la seconde en vérifiant très régulièrement que les services sont fonctionnels et disponibles et en remontant les alertes par les canaux appropriés (souvent par le courrier électronique, parfois avec des SMS, etc.). Les deux logiciels sont conçus de manière modulaire : il est relativement aisé de créer de nouveaux greffons <foreignphrase>(plug-ins)</foreignphrase> pour surveiller des services ou paramètres spécifiques."

msgid "<emphasis>ALTERNATIVE</emphasis> Zabbix, an integrated monitoring tool"
msgstr "<emphasis>ALTERNATIVE</emphasis> Zabbix, un système de supervision intégré"

msgid "<primary>Zabbix</primary>"
msgstr "<primary>Zabbix</primary>"

#, fuzzy
#| msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"http://www.zabbix.com/\" />"
msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"https://zabbix.com/\" />"
msgstr "Bien que Munin et Nagios soient très répandus, ils ne sont pas les seuls logiciels permettant la supervision et ils ne traitent qu'une partie de la problématique (graphage ou alerte). On citera notamment Zabbix. Il intègre les deux activités en un seul logiciel et est pour partie configuré via une interface web. Il s'est grandement amélioré dans les dernières années et peut être considéré comme une alternative viable. Sur le serveur de supervision, il faut installer <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (ou <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), la plupart du temps avec <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> pour disposer d'une interface web. Sur les hôtes à superviser, il faut installer <emphasis role=\"pkg\">zabbix-agent</emphasis> afin qu'ils puissent remonter les informations pertinentes au serveur. <ulink type=\"block\" url=\"http://www.zabbix.org/\" />"

msgid "<emphasis>ALTERNATIVE</emphasis> Icinga, a Nagios fork"
msgstr "<emphasis>ALTERNATIVE</emphasis> Icinga, un fork de Nagios"

msgid "<primary>Icinga</primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"
msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"https://icinga.com/\" />"
msgstr "Suite à des divergences d'opinion concernant le développement de Nagios (qui est contrôlé par une entreprise), un certain nombre de développeurs ont créé Icinga en repartant de Nagios. Le logiciel reste compatible — pour le moment — avec les configurations et greffons Nagios, mais ajoute des fonctionnalités supplémentaires. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"

msgid "Setting Up Munin"
msgstr "Mise en œuvre de Munin"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary>grapher</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The purpose of Munin is to monitor many machines; therefore, it quite naturally uses a client/server architecture. The central host — the grapher — collects data from all the monitored hosts, and generates historical graphs."
msgstr "Ce logiciel permet de superviser de nombreuses machines ; il emploie donc fort logiquement une architecture client/serveur. Une machine centrale — le grapheur — va collecter les données exportées par tous les hôtes à superviser, pour en faire des graphiques historiques."

msgid "Configuring Hosts To Monitor"
msgstr "Configuration des hôtes à superviser"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>server</primary><secondary>munin-node</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The first step is to install the <emphasis role=\"pkg\">munin-node</emphasis> package. The daemon installed by this package listens on port 4949 and sends back the data collected by all the active plugins. Each plugin is a simple program returning a description of the collected data as well as the latest measured value. Plugins are stored in <filename>/usr/share/munin/plugins/</filename>, but only those with a symbolic link in <filename>/etc/munin/plugins/</filename> are really used."
msgstr "La première étape consiste à installer le paquet <emphasis role=\"pkg\">munin-node</emphasis>. Ce dernier contient un démon qui écoute sur le port 4949 et qui renvoie toutes les valeurs collectées par l'ensemble des greffons actifs. Chaque greffon est un simple programme qui peut renvoyer une description des informations qu'il collecte ainsi que la dernière valeur constatée. Ils sont placés dans <filename>/usr/share/munin/plugins/</filename> mais seuls ceux qui sont liés dans <filename>/etc/munin/plugins/</filename> sont réellement employés."

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/munin/</filename></secondary><see>Munin</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/plugins/</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary>plugins</secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this auto-configuration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the Plugin Gallery can be interesting even though not all plugins have comprehensive documentation. <ulink type=\"block\" url=\"https://gallery.munin-monitoring.org\" />"
msgstr ""

#, fuzzy
#| msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the <ulink url=\"http://gallery.munin-monitoring.org\">Plugin Gallery</ulink> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgid "However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgstr "L'installation initiale du paquet préconfigure une liste de greffons actifs en fonction des logiciels disponibles et de la configuration actuelle de la machine. Ce paramétrage automatique dépend d'une fonctionnalité intégrée à chaque greffon et il n'est pas toujours judicieux d'en rester là. Il est intéressant de naviguer sur la <ulink url=\"http://gallery.munin-monitoring.org\">galerie des greffons</ulink>, même si tous les greffons ne disposent pas d'une documentation complète. Cela dit, tous les greffons sont des scripts, souvent relativement simples et contenant quelques commentaires explicatifs. Il ne faut donc pas hésiter à faire le tour de <filename>/etc/munin/plugins/</filename> pour supprimer les greffons inutiles. De même, on peut activer un greffon intéressant repéré dans <filename>/usr/share/munin/plugins/</filename> avec une commande <command>ln -sf /usr/share/munin/plugins/<replaceable>greffon</replaceable> /etc/munin/plugins/</command>. Attention, les greffons dont le nom se termine par un tiret souligné (_) sont particuliers, ils ont besoin d'un paramètre. Celui-ci doit être intégré dans le nom du lien symbolique créé (par exemple le greffon <filename>if_</filename> sera installé avec un lien symbolique <filename>if_eth0</filename> pour surveiller le trafic sur l'interface réseau <literal>eth0</literal>)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/munin-node.conf</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>munin-node.service</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>service munin-node restart</command>."
msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>systemctl restart munin-node</command>."
msgstr "Une fois tous les greffons correctement mis en place, il faut paramétrer le démon pour indiquer qui a le droit de récupérer ces valeurs. Cela s'effectue dans le fichier <filename>/etc/munin/munin-node.conf</filename> avec une directive <literal>allow</literal>. Par défaut, on trouve <literal>allow ^127\\.0\\.0\\.1$</literal> qui n'autorise l'accès qu'à l'hôte local. Il convient d'ajouter une ligne similaire contenant l'adresse IP de la machine qui va assumer le rôle de grapheur puis de relancer le démon avec <command>service munin-node restart</command>."

msgid "<emphasis>GOING FURTHER</emphasis> Creating local plugins"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Créer ses propres greffons"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><command>munin-run</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"
msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"https://guide.munin-monitoring.org/en/latest/plugin/writing.html\" />"
msgstr "Munin dispose d'une documentation conséquente sur le fonctionnement théorique de ces greffons et sur la manière d'en développer de nouveaux. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"

msgid "A plugin is best tested when run in the same conditions as it would be when triggered by munin-node; this can be simulated by running <command>munin-run <replaceable>plugin</replaceable></command> as root. A potential second parameter given to this command (such as <literal>config</literal>) is passed to the plugin as a parameter."
msgstr "Pour tester le greffon, il convient de le lancer dans les mêmes conditions que munin-node le ferait en exécutant <command>munin-run <replaceable>greffon</replaceable></command> en tant qu'utilisateur root. Le deuxième paramètre éventuel (comme <literal>config</literal>) est réemployé comme paramètre lors de l'exécution du greffon."

msgid "When a plugin is invoked with the <literal>config</literal> parameter, it must describe itself by returning a set of fields:"
msgstr "Lorsque le greffon est appelé avec le paramètre <literal>config</literal>, il doit renvoyer un ensemble de champs le décrivant, par exemple :"

#, fuzzy
#| msgid ""
#| "<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
#| "</userinput><computeroutput>graph_title Load average\n"
#| "graph_args --base 1000 -l 0\n"
#| "graph_vlabel load\n"
#| "graph_scale no\n"
#| "graph_category system\n"
#| "load.label load\n"
#| "graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
#| "load.info 5 minute load average\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"

#, fuzzy
#| msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"https://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgstr "Les différents champs qu'il est possible de renvoyer sont décrits sur une page web décrivant le « protocole de configuration ». <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"

msgid "When invoked without a parameter, the plugin simply returns the last measured values; for instance, executing <command>sudo munin-run load</command> could return <literal>load.value 0.12</literal>."
msgstr "Lorsque le greffon est appelé sans paramètre, il renvoie simplement les dernières valeurs associées ; ainsi l'exécution de <command>sudo munin-run load</command> retourne par exemple <literal>load.value 0.12</literal>."

msgid "Finally, when a plugin is invoked with the <literal>autoconf</literal> parameter, it should return “yes” (and a 0 exit status) or “no” (with a 1 exit status) according to whether the plugin should be enabled on this host."
msgstr "Enfin, lorsque le greffon est appelé avec <literal>autoconf</literal> comme paramètre, il renvoie « yes » (avec un code retour à 0) ou « no » (avec un code de retour à 1) pour signifier si oui ou non le greffon devrait être activé sur cette machine."

msgid "Configuring the Grapher"
msgstr "Configuration du grapheur"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><command>munin-cron</command></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/munin.conf</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The “grapher” is simply the computer that aggregates the data and generates the corresponding graphs. The required software is in the <emphasis role=\"pkg\">munin</emphasis> package. The standard configuration runs <command>munin-cron</command> (once every 5 minutes), which gathers data from all the hosts listed in <filename>/etc/munin/munin.conf</filename> (only the local host is listed by default), saves the historical data in RRD files (<emphasis>Round Robin Database</emphasis>, a file format designed to store data varying in time) stored under <filename>/var/lib/munin/</filename> and generates an HTML page with the graphs in <filename>/var/cache/munin/www/</filename>."
msgstr "Par grapheur, on entend simplement la machine qui va collecter les données et générer les graphiques correspondants. Le paquet correspondant à installer est <emphasis role=\"pkg\">munin</emphasis>. La configuration initiale du paquet lance <command>munin-cron</command> toutes les 5 minutes. Ce dernier collecte les données depuis toutes les machines listées dans <filename>/etc/munin/munin.conf</filename> (uniquement l'hôte local par défaut), stocke les historiques sous forme de fichiers RRD (<foreignphrase>Round Robin Database</foreignphrase> est un format de fichier adapté au stockage de données variant dans le temps) dans <filename>/var/lib/munin/</filename> et régénère une page HTML avec des graphiques dans <filename>/var/cache/munin/www/</filename>."

msgid "All monitored machines must therefore be listed in the <filename>/etc/munin/munin.conf</filename> configuration file. Each machine is listed as a full section with a name matching the machine and at least an <literal>address</literal> entry giving the corresponding IP address."
msgstr "Il faut donc éditer <filename>/etc/munin/munin.conf</filename> pour y ajouter toutes les machines à surveiller. Chaque machine se présente sous la forme d'une section complète portant son nom et contenant une entrée <literal>address</literal> qui indique l'adresse IP de la machine à superviser."

msgid ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"
msgstr ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"

msgid "Sections can be more complex, and describe extra graphs that could be created by combining data coming from several machines. The samples provided in the configuration file are good starting points for customization."
msgstr "Les sections peuvent être plus élaborées et décrire des graphiques supplémentaires à créer à partir de la combinaison de données provenant de plusieurs machines. On peut s'inspirer des exemples fournis dans le fichier de configuration."

msgid "The last step is to publish the generated pages; this involves configuring a web server so that the contents of <filename>/var/cache/munin/www/</filename> are made available on a website. Access to this website will often be restricted, using either an authentication mechanism or IP-based access control. See <xref linkend=\"sect.http-web-server\" /> for the relevant details."
msgstr "Enfin, la dernière étape consiste à publier les pages générées. Il faut configurer votre serveur web pour que l'on puisse accéder au contenu de <filename>/var/cache/munin/www/</filename> par l'intermédiaire d'un site web. On choisira généralement de restreindre l'accès soit à l'aide d'un système d'authentification, soit en fournissant une liste d'adresses IP autorisées à consulter ces informations. La <xref linkend=\"sect.http-web-server\" /> fournit les explications nécessaires."

msgid "Setting Up Nagios"
msgstr "Mise en œuvre de Nagios"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">nagios4</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">monitoring-plugins</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

msgid "Unlike Munin, Nagios does not necessarily require installing anything on the monitored hosts; most of the time, Nagios is used to check the availability of network services. For instance, Nagios can connect to a web server and check that a given web page can be obtained within a given time."
msgstr "Contrairement à Munin, Nagios ne nécessite pas forcément d'installer quoi que ce soit sur les machines à superviser. En effet, il est fréquemment employé simplement pour vérifier la disponibilité de certains services réseau. Par exemple, Nagios peut se connecter à un serveur web et vérifier qu'il peut récupérer une page web donnée dans un certain délai."

msgid "Installing"
msgstr "Installation"

msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios4</emphasis> and <emphasis role=\"pkg\">monitoring-plugins</emphasis> packages. Installing the packages configures the web interface and the Apache server. The <literal>authz_groupfile</literal> and <literal>auth_digest</literal> Apache modules must be enabled, for that execute:"
msgstr ""

msgid ""
"<computeroutput># </computeroutput><userinput>a2enmod authz_groupfile\n"
"</userinput><computeroutput>Considering dependency authz_core for authz_groupfile:\n"
"Module authz_core already enabled\n"
"Module authz_core already enabled\n"
"Enabling module authz_groupfile.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>a2enmod auth_digest\n"
"</userinput><computeroutput>Considering dependency authn_core for auth_digest:\n"
"Module authn_core already enabled\n"
"Enabling module auth_digest.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>systemctl restart apache2\n"
"</userinput>"
msgstr ""

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/nagios4/</filename></secondary><see>Nagios</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/hdigest.users</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "Adding other users is a simple matter of inserting them in the <filename>/etc/nagios4/hdigest.users</filename> file."
msgstr ""

#, fuzzy
#| msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios3/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios4/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgstr "En se connectant sur <literal>http://<replaceable>serveur</replaceable>/nagios3/</literal>, on découvre l'interface web et l'on peut constater que Nagios surveille déjà certains paramètres de la machine sur laquelle il fonctionne. Cependant, en essayant d'utiliser certaines fonctionnalités interactives comme l'ajout de commentaires concernant un hôte, on constate qu'elles ne fonctionnent pas. Par défaut, Nagios est effectivement configuré de manière très restrictive (pour plus de sécurité) et ces fonctionnalités sont désactivées."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/nagios.cfg</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "As documented in <filename>/usr/share/doc/nagios3/README.Debian</filename>, enabling some features involves editing <filename>/etc/nagios3/nagios.cfg</filename> and setting its <literal>check_external_commands</literal> parameter to “1”. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgid "Enabling some features involves editing <filename>/etc/nagios4/nagios.cfg</filename>. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgstr "En consultant <filename>/usr/share/doc/nagios3/README.Debian</filename> on comprend qu'il faut éditer <filename>/etc/nagios3/nagios.cfg</filename> et positionner le paramètre <literal>check_external_commands</literal> à « 1 ». Puis il faut changer les permissions d'écriture sur un répertoire employé par Nagios avec ces quelques commandes :"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
#| "</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
#| "</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
#| "<computeroutput>[...]</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>systemctl stop nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios4/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl start nagios4\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
"</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
"<computeroutput>[...]</computeroutput>"

msgid "Configuring"
msgstr "Configuration"

#, fuzzy
#| msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios3/nagios.cfg</filename>."
msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios4/nagios.cfg</filename>."
msgstr "L'interface web de Nagios est relativement plaisante, mais elle ne permet pas de le configurer. Il n'est pas possible d'ajouter des hôtes et des services à surveiller. Toute la configuration de ce logiciel s'effectue par un ensemble de fichiers référencés par le fichier de configuration central <filename>/etc/nagios3/nagios.cfg</filename>."

msgid "These files should not be dived into without some understanding of the Nagios concepts. The configuration lists objects of the following types:"
msgstr "Avant de plonger dans ces fichiers, il faut se familiariser avec les concepts de Nagios. La configuration liste un ensemble d'objets de différents types :"

msgid "a <emphasis>host</emphasis> is a machine to be monitored;"
msgstr "Un hôte <foreignphrase>(host)</foreignphrase> est une machine du réseau que l'on souhaite surveiller;"

msgid "a <emphasis>hostgroup</emphasis> is a set of hosts that should be grouped together for display, or to factor some common configuration elements;"
msgstr "Un <foreignphrase>hostgroup</foreignphrase> est un ensemble d'hôtes que l'on souhaite regrouper pour un affichage plus clair ou pour factoriser des éléments de configuration;"

msgid "a <emphasis>service</emphasis> is a testable element related to a host or a host group. It will most often be a check for a network service, but it can also involve checking that some parameters are within an acceptable range (for instance, free disk space or processor load);"
msgstr "Un <foreignphrase>service</foreignphrase> est un élément à tester qui concerne un hôte ou un groupe d'hôtes. En général, il s'agit effectivement de vérifier le fonctionnement de « services » réseau, mais il peut s'agir de vérifier que des paramètres soient dans un intervalle acceptable (comme l'espace disque ou la charge CPU);"

msgid "a <emphasis>servicegroup</emphasis> is a set of services that should be grouped together for display;"
msgstr "Un <foreignphrase>servicegroup</foreignphrase> est un ensemble de services que l'on souhaite regrouper dans l'affichage;"

msgid "a <emphasis>contact</emphasis> is a person who can receive alerts;"
msgstr "Un <foreignphrase>contact</foreignphrase> est une personne qui peut recevoir des alertes;"

msgid "a <emphasis>contactgroup</emphasis> is a set of such contacts;"
msgstr "Un <foreignphrase>contactgroup</foreignphrase> est un ensemble de contacts à avertir;"

msgid "a <emphasis>timeperiod</emphasis> is a range of time during which some services have to be checked;"
msgstr "Une <foreignphrase>timeperiod</foreignphrase> est une plage horaire pendant laquelle certains services doivent être vérifiés;"

msgid "a <emphasis>command</emphasis> is the command line invoked to check a given service."
msgstr "Une commande <foreignphrase>(command)</foreignphrase> est une ligne de commande à exécuter pour tester un service donné."

msgid "According to its type, each object has a number of properties that can be customized. A full list would be too long to include, but the most important properties are the relations between the objects."
msgstr "Chaque objet a un certain nombre de propriétés (selon son type) qu'il est possible de personnaliser. Une liste exhaustive serait trop longue, mais les relations entre ces objets sont les propriétés les plus importantes."

msgid "A <emphasis>service</emphasis> uses a <emphasis>command</emphasis> to check the state of a feature on a <emphasis>host</emphasis> (or a <emphasis>hostgroup</emphasis>) within a <emphasis>timeperiod</emphasis>. In case of a problem, Nagios sends an alert to all members of the <emphasis>contactgroup</emphasis> linked to the service. Each member is sent the alert according to the channel described in the matching <emphasis>contact</emphasis> object."
msgstr "Un <emphasis>service</emphasis> emploie une <emphasis>commande</emphasis> pour vérifier l'état d'une fonctionnalité sur un <emphasis>hôte</emphasis> ou un <emphasis>groupe d'hôtes</emphasis> dans une <emphasis>plage horaire</emphasis> donnée. En cas de problèmes, Nagios envoie une alerte à tous les membres du <emphasis>contactgroup</emphasis> associé au service défaillant. Chaque membre est alerté selon les modalités précisées dans son objet <emphasis>contact</emphasis> correspondant."

#, fuzzy
#| msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios3/conf.d/</filename> are a good source of information on how they work."
msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios4/conf.d/</filename> are a good source of information on how they work."
msgstr "Une fonctionnalité d'héritage entre les objets permet de partager facilement un ensemble de propriétés entre un grand nombre d'objets, tout en évitant une duplication de l'information. Par ailleurs, la configuration initiale comporte un certain nombre d'objets standards et, dans la plupart des cas, il suffit de définir de nouveaux hôtes, services et contacts en héritant des objets génériques prédéfinis. La lecture des fichiers de <filename>/etc/nagios3/conf.d/</filename> permet de se familiariser avec ceux-ci."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/conf.d/</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

msgid "The Falcot Corp administrators use the following configuration:"
msgstr "Voici la configuration employée par les administrateurs de Falcot :"

#, fuzzy
#| msgid "<filename>/etc/nagios3/conf.d/falcot.cfg</filename> file"
msgid "<filename>/etc/nagios4/conf.d/falcot.cfg</filename> file"
msgstr "Fichier <filename>/etc/nagios3/conf.d/falcot.cfg</filename>"

#, fuzzy
#| msgid ""
#| "define contact{\n"
#| "    name                            generic-contact\n"
#| "    service_notification_period     24x7\n"
#| "    host_notification_period        24x7\n"
#| "    service_notification_options    w,u,c,r\n"
#| "    host_notification_options       d,u,r\n"
#| "    service_notification_commands   notify-service-by-email\n"
#| "    host_notification_commands      notify-host-by-email\n"
#| "    register                        0 ; Template only\n"
#| "}\n"
#| "define contact{\n"
#| "    use             generic-contact\n"
#| "    contact_name    rhertzog\n"
#| "    alias           Raphael Hertzog\n"
#| "    email           hertzog@debian.org\n"
#| "}\n"
#| "define contact{\n"
#| "    use             generic-contact\n"
#| "    contact_name    rmas\n"
#| "    alias           Roland Mas\n"
#| "    email           lolando@debian.org\n"
#| "}\n"
#| "\n"
#| "define contactgroup{\n"
#| "    contactgroup_name     falcot-admins\n"
#| "    alias                 Falcot Administrators\n"
#| "    members               rhertzog,rmas\n"
#| "}\n"
#| "\n"
#| "define host{\n"
#| "    use                   generic-host ; Name of host template to use\n"
#| "    host_name             www-host\n"
#| "    alias                 www.falcot.com\n"
#| "    address               192.168.0.5\n"
#| "    contact_groups        falcot-admins\n"
#| "    hostgroups            debian-servers,ssh-servers\n"
#| "}\n"
#| "define host{\n"
#| "    use                   generic-host ; Name of host template to use\n"
#| "    host_name             ftp-host\n"
#| "    alias                 ftp.falcot.com\n"
#| "    address               192.168.0.6\n"
#| "    contact_groups        falcot-admins\n"
#| "    hostgroups            debian-servers,ssh-servers\n"
#| "}\n"
#| "\n"
#| "# 'check_ftp' command with custom parameters\n"
#| "define command{\n"
#| "    command_name          check_ftp2\n"
#| "    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
#| "}\n"
#| "\n"
#| "# Generic Falcot service\n"
#| "define service{\n"
#| "    name                  falcot-service\n"
#| "    use                   generic-service\n"
#| "    contact_groups        falcot-admins\n"
#| "    register              0\n"
#| "}\n"
#| "\n"
#| "# Services to check on www-host\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   HTTP\n"
#| "    check_command         check_http\n"
#| "}\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   HTTPS\n"
#| "    check_command         check_https\n"
#| "}\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   SMTP\n"
#| "    check_command         check_smtp\n"
#| "}\n"
#| "\n"
#| "# Services to check on ftp-host\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             ftp-host\n"
#| "    service_description   FTP\n"
#| "    check_command         check_ftp2\n"
#| "}"
msgid ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.12\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"
msgstr ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# commande 'check_ftp' avec paramètres personnalisés\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Service générique à Falcot\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"# Services à vérifier sur www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"# Services à vérifier sur ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"

#, fuzzy
#| msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>."
msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios4/conf.d/services_nagios2.cfg</filename>."
msgstr "Ce fichier de configuration définit deux hôtes à surveiller. Le premier concerne le serveur web de Falcot ; on y surveille le fonctionnement du serveur web sur le port HTTP (80) et sur le port HTTP sécurisé (443). On vérifie également qu'un serveur SMTP est accessible sur son port 25. Le second concerne le serveur FTP et l'on vérifie qu'on obtient une réponse en moins de 20 secondes. Au-delà de ce délai, une mise en garde <foreignphrase>(warning)</foreignphrase> est générée et, au-delà de 30 secondes, une alerte critique. En se rendant sur l'interface web, on peut se rendre compte que le service SSH est également surveillé : cette surveillance est due à l'appartenance des hôtes au groupe <literal>ssh-servers</literal>. Le service standard correspondant est défini dans <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>."

msgid "Note the use of inheritance: an object is made to inherit from another object with the “use <replaceable>parent-name</replaceable>”. The parent object must be identifiable, which requires giving it a “name <replaceable>identifier</replaceable>” property. If the parent object is not meant to be a real object, but only to serve as a parent, giving it a “register 0” property tells Nagios not to consider it, and therefore to ignore the lack of some parameters that would otherwise be required."
msgstr "On peut noter l'usage de l'héritage : pour hériter d'un autre objet, on emploie la propriété <command>use <replaceable>nom-parent</replaceable></command>. Pour identifier un objet dont on veut hériter, il faut lui attribuer une propriété <command>name <replaceable>identifiant</replaceable></command>. Si l'objet parent n'est pas un objet réel, mais est uniquement destiné à servir de rôle de parent, on lui ajoute la propriété <command>register 0</command> qui indique à Nagios de ne pas le considérer et donc d'ignorer l'absence de certains paramètres normalement requis."

msgid "<emphasis>DOCUMENTATION</emphasis> List of object properties"
msgstr "<emphasis>DOCUMENTATION</emphasis> Liste des propriétés des objets"

#, fuzzy
#| msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation provided by the <emphasis role=\"pkg\">nagios3-doc</emphasis> package. This documentation is directly accessible from the web interface, with the “Documentation” link in the top left corner. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation hosted on <ulink url=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/\" />. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgstr "Pour avoir une meilleure idée des nombreuses possibilités de paramétrage de Nagios, il faut consulter la documentation fournie par le paquet <emphasis role=\"pkg\">nagios3-doc</emphasis>. Elle est directement accessible depuis l'interface web via le lien <guimenuitem>Documentation</guimenuitem> en haut à gauche. On y trouve notamment une liste exhaustive des différents types d'objets avec toutes les propriétés que l'on peut leur affecter. Il est également expliqué comment créer de nouveaux greffons."

msgid "<emphasis>GOING FURTHER</emphasis> Remote tests with NRPE"
msgstr "<emphasis>POUR ALLER PLUS LOIN</emphasis> Tests distants avec NRPE"

msgid "<primary>Nagios Remote Plugin Executor</primary><see>NRPE</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>NRPE</primary>"
msgstr "<primary>RAID</primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary>Nagios</primary><secondary><emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary>Nagios</primary><secondary><emphasis role=\"pkg\">nagios-nrpe-server</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtual-manager</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios/nrpe.cfg</filename></secondary>"
msgstr "<primary>installation</primary><secondary>automatisée</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>check_nrpe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <emphasis>check_nrpe</emphasis> command."
msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <command>check_nrpe</command> command."
msgstr "De nombreux greffons de Nagios permettent de vérifier l'état de certains paramètres locaux d'une machine. Si l'on souhaite effectuer ces vérifications sur de nombreuses machines tout en centralisant les résultats sur une seule installation, il faut employer le greffon NRPE <foreignphrase>(Nagios Remote Plugin Executor)</foreignphrase>. On installe <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> sur le serveur Nagios et <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> sur les machines sur lesquelles on veut exécuter certains tests locaux. Ce dernier se configure par le biais du fichier <filename>/etc/nagios/nrpe.cfg</filename>. On y indique les tests que l'on peut démarrer à distance ainsi que les adresses IP des machines qui sont autorisées à les déclencher. Du côté de Nagios, il suffit d'ajouter les services correspondants en faisant appel à la nouvelle commande <emphasis>check_nrpe</emphasis>."

#~ msgid "<primary><emphasis>VMWare</emphasis></primary>"
#~ msgstr "<primary><emphasis>VMWare</emphasis></primary>"

#~ msgid "<primary><emphasis>Bochs</emphasis></primary>"
#~ msgstr "<primary><emphasis>Bochs</emphasis></primary>"

#~ msgid "<primary><emphasis>QEMU</emphasis></primary>"
#~ msgstr "<primary><emphasis>QEMU</emphasis></primary>"

#~ msgid "<primary><emphasis>KVM</emphasis></primary>"
#~ msgstr "<primary><emphasis>KVM</emphasis></primary>"

#~ msgid "<primary><emphasis>LXC</emphasis></primary>"
#~ msgstr "<primary><emphasis>LXC</emphasis></primary>"

#~ msgid "<emphasis>DOCUMENTATION</emphasis> <command>xl</command> options"
#~ msgstr "<emphasis>DOCUMENTATION</emphasis> Options de la commande <command>xl</command>"

#~ msgid ""
#~ "#auto eth0\n"
#~ "#iface eth0 inet dhcp\n"
#~ "\n"
#~ "auto br0\n"
#~ "iface br0 inet dhcp\n"
#~ "  bridge-ports eth0"
#~ msgstr ""
#~ "#auto eth0\n"
#~ "#iface eth0 inet dhcp\n"
#~ "\n"
#~ "auto br0\n"
#~ "iface br0 inet dhcp\n"
#~ "  bridge-ports eth0"

#~ msgid "The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<filename>/var/lib/lxc/testlxc/config</filename>) and add a few <literal>lxc.network.*</literal> entries:"
#~ msgstr "Le système de fichiers nouvellement créé contient désormais un système Debian minimal. Le conteneur associé n'a aucun périphérique réseau (mis à part la boucle locale). Puisque cela n'est pas vraiment souhaitable, nous éditerons le fichier de configuration du conteneur (<filename>/var/lib/lxc/testlxc/config</filename>) et ajouterons ces quelques directives <literal>lxc.network.*</literal> :"

#~ msgid "copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>"
#~ msgstr "copier le fichier de préconfiguration dans <filename>/media/usbdisk/preseed.cfg</filename> ;"

#~ msgid "<primary>simple-cdd</primary>"
#~ msgstr "<primary>simple-cdd</primary>"

#~ msgid ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"
#~ msgstr ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"

#~ msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> and <emphasis role=\"pkg\">nagios3-doc</emphasis> packages. Installing the packages configures the web interface and creates a first <literal>nagiosadmin</literal> user (for which it asks for a password). Adding other users is a simple matter of inserting them in the <filename>/etc/nagios3/htpasswd.users</filename> file with Apache's <command>htpasswd</command> command. If no Debconf question was displayed during installation, <command>dpkg-reconfigure nagios3-cgi</command> can be used to define the <literal>nagiosadmin</literal> password."
#~ msgstr "La première étape est donc d'installer les paquets <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> et <emphasis role=\"pkg\">nagios3-doc</emphasis>. Une fois cela effectué, l'interface web de Nagios est d'ores et déjà configurée et un premier utilisateur <literal>nagiosadmin</literal> (dont le mot de passe vient d'être saisi) peut y accéder. Il est possible d'ajouter d'autres utilisateurs en les insérant dans le fichier <filename>/etc/nagios3/htpasswd.users</filename> à l'aide de la commande <command>htpasswd</command> de Apache. Si aucune question debconf n'est apparue au cours de l'installation, il est possible d'exécuter <command>dpkg-reconfigure nagios3-cgi</command> pour définir le mot de passe de l'utilisateur <literal>nagiosadmin</literal>."

#, fuzzy
#~| msgid "the <emphasis role=\"distribution\">Wheezy</emphasis> standard kernel does not allow limiting the amount of memory available to a container; the feature exists, and is built in the kernel, but it is disabled by default because it has a (slight) cost on overall system performance; however, enabling it is a simple matter of setting the <command>cgroup_enable=memory</command> kernel command-line option at boot time;"
#~ msgid "the <emphasis role=\"distribution\">Jessie</emphasis> standard kernel does not allow limiting the amount of memory available to a container; the feature exists, and is built in the kernel, but it is disabled by default because it has a (slight) cost on overall system performance; however, enabling it is a simple matter of setting the <command>cgroup_enable=memory</command> kernel command-line option at boot time;"
#~ msgstr "Bien que le noyau de <emphasis role=\"distribution\">Wheezy</emphasis> dispose de la fonctionnalité permettant de limiter la mémoire accessible à un conteneur, cette dernière n'est pas activée par défaut car elle réduit légèrement les performances globales du système. Toutefois, elle peut facilement être activée on définissant l'option <command>cgroup_enable=memory</command> sur la ligne de commande du noyau."

#~ msgid "The procedure requires setting up a <filename>yum.conf</filename> file containing the necessary parameters, including the path to the source RPM repositories, the path to the plugin configuration, and the destination folder. For this example, we will assume that the environment will be stored in <filename>/var/tmp/yum-bootstrap</filename>. The file <filename>/var/tmp/yum-bootstrap/yum.conf</filename> file should look like this:"
#~ msgstr "La procédure nécessite de mettre en place un fichier <filename>yum.conf</filename> avec les paramètres requis, notamment le chemin vers les dépôts RPM, le chemin de la configuration des greffons et le répertoire cible. Pour cet exemple, nous supposerons que l'environnement sera stocké dans <filename>/var/tmp/yum-bootstrap/</filename>. Le fichier <filename>/var/tmp/yum-bootstrap/yum.conf</filename> devrait ressembler à ceci :"

#~ msgid ""
#~ "[main]\n"
#~ "reposdir=/var/tmp/yum-bootstrap/repos.d\n"
#~ "pluginconfpath=/var/tmp/yum-bootstrap/pluginconf.d\n"
#~ "cachedir=/var/cache/yum\n"
#~ "installroot=/path/to/destination/domU/install\n"
#~ "exclude=$exclude\n"
#~ "keepcache=1\n"
#~ "#debuglevel=4  \n"
#~ "#errorlevel=4\n"
#~ "pkgpolicy=newest\n"
#~ "distroverpkg=centos-release\n"
#~ "tolerant=1\n"
#~ "exactarch=1\n"
#~ "obsoletes=1\n"
#~ "gpgcheck=1\n"
#~ "plugins=1\n"
#~ "metadata_expire=1800"
#~ msgstr ""
#~ "[main]\n"
#~ "reposdir=/var/tmp/yum-bootstrap/repos.d\n"
#~ "pluginconfpath=/var/tmp/yum-bootstrap/pluginconf.d\n"
#~ "cachedir=/var/cache/yum\n"
#~ "installroot=/path/to/destination/domU/install\n"
#~ "exclude=$exclude\n"
#~ "keepcache=1\n"
#~ "#debuglevel=4  \n"
#~ "#errorlevel=4\n"
#~ "pkgpolicy=newest\n"
#~ "distroverpkg=centos-release\n"
#~ "tolerant=1\n"
#~ "exactarch=1\n"
#~ "obsoletes=1\n"
#~ "gpgcheck=1\n"
#~ "plugins=1\n"
#~ "metadata_expire=1800"

#, fuzzy
#~| msgid "The <filename>/var/tmp/yum-bootstrap/repos.d</filename> directory should contain the descriptions of the RPM source repositories, just as in <filename>/etc/yum.repos.d</filename> in an already installed RPM-based system. Here is an example for a CentOS 6 installation:"
#~ msgid "The <filename>/var/tmp/yum-bootstrap/repos.d</filename> directory should contain the descriptions of the RPM source repositories in <filename>*.repo</filename> files, just as in <filename>/etc/yum.repos.d</filename> in an already installed RPM-based system. Here is an example for a CentOS 6 installation:"
#~ msgstr "Le répertoire <filename>/var/tmp/yum-bootstrap/repos.d</filename> devrait contenir la description des dépôts RPM source, similaire à ce que l'on trouverait dans <filename>/etc/yum.repos.d</filename> sur un système RPM déjà installé. Voici un exemple pour une installation de CentOS 6 :"

#~ msgid ""
#~ "[base]\n"
#~ "name=CentOS-6 - Base\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "   \n"
#~ "[updates]\n"
#~ "name=CentOS-6 - Updates\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[extras]\n"
#~ "name=CentOS-6 - Extras\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[centosplus]\n"
#~ "name=CentOS-6 - Plus\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6"
#~ msgstr ""
#~ "[base]\n"
#~ "name=CentOS-6 - Base\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "   \n"
#~ "[updates]\n"
#~ "name=CentOS-6 - Updates\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[extras]\n"
#~ "name=CentOS-6 - Extras\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"
#~ "\n"
#~ "[centosplus]\n"
#~ "name=CentOS-6 - Plus\n"
#~ "#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\n"
#~ "mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus\n"
#~ "gpgcheck=1\n"
#~ "gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\n"

#~ msgid "Finally, <filename>pluginconf.d/installonlyn.conf</filename> file should contain the following:"
#~ msgstr "Enfin, <filename>pluginconf.d/installonlyn.conf</filename> devrait contenir ceci :"

#~ msgid ""
#~ "[main]\n"
#~ "enabled=1\n"
#~ "tokeep=5"
#~ msgstr ""
#~ "[main]\n"
#~ "enabled=1\n"
#~ "tokeep=5\n"

#~ msgid "Once all this is setup, make sure the <command>rpm</command> databases are correctly initialized, with a command such as <command>rpm --rebuilddb</command>. An installation of CentOS 6 is then a matter of the following:"
#~ msgstr "Une fois tout ceci mis en place, assurez-vous que les bases de données RPM sont correctement initialisées en exécutant <command>rpm --rebuilddb</command>. Une installation de CentOS 6 s'effectue alors ainsi :"

#~ msgid "<userinput>yum -c /var/tmp/yum-bootstrap/yum.conf -y install coreutils basesystem centos-release yum-basearchonly initscripts</userinput>"
#~ msgstr "<userinput>yum -c /var/tmp/yum-bootstrap/yum.conf -y install coreutils basesystem centos-release yum-basearchonly initscripts</userinput>"

#~ msgid "Xen is currently only available for the i386 and amd64 architectures. Moreover, it uses processor instructions that haven't always been provided in all i386-class computers. Note that most of the Pentium-class (or better) processors made after 2001 will work, so this restriction won't apply to very many situations."
#~ msgstr "Xen n'est pour l'instant disponible que sur les architectures i386 et amd64. De plus, sur i386, il fait appel à des instructions du processeur qui n'ont pas toujours été présentes. Cela dit, tous les processeurs de classe Pentium ou supérieure produits après 2001 fonctionneront, donc cette restriction ne sera la plupart du temps pas gênante."

#~ msgid ""
#~ "# /etc/fstab: static file system information.\n"
#~ "[...]\n"
#~ "cgroup            /sys/fs/cgroup           cgroup    defaults        0       0"
#~ msgstr ""
#~ "\n"
#~ "# /etc/fstab: static file system information.\n"
#~ "[...]\n"
#~ "cgroup            /sys/fs/cgroup           cgroup    defaults        0       0\n"

#~ msgid "<filename>/sys/fs/cgroup</filename> will then be mounted automatically at boot time; if no immediate reboot is planned, the filesystem should be manually mounted with <command>mount /sys/fs/cgroup</command>."
#~ msgstr "<filename>/sys/fs/cgroup</filename> sera monté automatiquement au démarrage ; si l'on ne souhaite pas redémarrer tout de suite, on effectuera le montage manuellement avec <command>mount /sys/fs/cgroup</command>."

#~ msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots; its configuration file, <filename>/etc/default/lxc</filename>, is relatively straightforward; note that the container configuration files need to be stored in <filename>/etc/lxc/auto/</filename>; many users may prefer symbolic links, such as can be created with <command>ln -s /var/lib/lxc/testlxc/config /etc/lxc/auto/testlxc.config</command>."
#~ msgstr "Le paquet <emphasis role=\"pkg\">lxc</emphasis> contient un script de démarrage permettant de lancer automatiquement un ou plusieurs conteneurs au démarrage de l'hôte ; sa configuration est relativement simple et se fait dans <filename>/etc/default/lxc</filename>. Notez qu'il a besoin que les fichiers de configuration des conteneurs soient accessibles dans <filename>/etc/lxc/auto/</filename> ; on prendra donc soin de placer des liens symboliques : <command>ln -s /var/lib/lxc/testlxc/config /etc/lxc/auto/testlxc.config</command>."

#~ msgid "<emphasis>BEWARE</emphasis> Bugs in default <literal>debian</literal> template"
#~ msgstr "<emphasis>ATTENTION</emphasis> Problèmes dans le <foreignphrase>template</foreignphrase> <literal>debian</literal>"

#~ msgid "The <command>/usr/share/lxc/templates/lxc-debian</command> template creation script provided in the initial <emphasis role=\"distribution\">Wheezy</emphasis> package (aka <emphasis role=\"pkg\">lxc</emphasis> 0.8.0~rc1-8+deb7u1) suffers from numerous problems. The most important one is that it relies on the <command>live-debconfig</command> program which is not available in <emphasis role=\"distribution\">Wheezy</emphasis> but only in newer versions of Debian. <ulink type=\"block\" url=\"http://bugs.debian.org/680469\" /> <ulink type=\"block\" url=\"http://bugs.debian.org/686747\" />"
#~ msgstr "Le script de création de <foreignphrase>template</foreignphrase> (modèle) <command>/usr/share/lxc/templates/lxc-debian</command> fourni dans le paquet initial de <emphasis role=\"distribution\">Wheezy</emphasis> (c'est-à-dire <emphasis role=\"pkg\">lxc</emphasis> 0.8.0~rc1-8+deb7u1) souffre de nombreux problèmes. Le plus problématique est qu'il s'appuie sur l'outil <command>live-debconfig</command> qui n'est pas disponible dans <emphasis role=\"distribution\">Wheezy</emphasis> mais seulement dans des versions plus récentes de Debian. <ulink type=\"block\" url=\"http://bugs.debian.org/680469\" /> <ulink type=\"block\" url=\"http://bugs.debian.org/686747\" />"

#~ msgid "At the time of writing, there was no good solution and no usable work-around, except to use an alternate template creation script. Further updates of lxc might fix this though. This section assumes that <command>/usr/share/lxc/templates/lxc-debian</command> matches the upstream provided script: <ulink type=\"block\" url=\"https://github.com/lxc/lxc/raw/master/templates/lxc-debian.in\" />"
#~ msgstr "Au moment où ces lignes sont écrites, il n'y avait pas de solution ou de contournement satisfaisant, mis à part d'utiliser un script alternatif de création de <foreignphrase>template</foreignphrase>. Toutefois, de futures mises à jour pourraient corriger cela. En attendant, cette section suppose que <command>/usr/share/lxc/templates/lxc-debian</command> correspond au script fourni par les développeurs amont : <ulink type=\"block\" url=\"https://github.com/lxc/lxc/raw/master/templates/lxc-debian.in\" />"

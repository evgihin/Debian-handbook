#
# AUTHOR <EMAIL@ADDRESS>, YEAR.
#
msgid ""
msgstr "Project-Id-Version: 0\nPOT-Creation-Date: 2020-08-28 10:15+0200\nPO-Revision-Date: 2020-09-14 23:36+0000\nLast-Translator: Andika Triwidada <andika@gmail.com>\nLanguage-Team: Indonesian <https://hosted.weblate.org/projects/debian-handbook/12_advanced-administration/id/>\nLanguage: id-ID\nMIME-Version: 1.0\nContent-Type: application/x-publican; charset=UTF-8\nContent-Transfer-Encoding: 8bit\nPlural-Forms: nplurals=1; plural=0;\nX-Generator: Weblate 4.3-dev\n"

msgid "RAID"
msgstr "RAID"

msgid "LVM"
msgstr "LVM"

msgid "FAI"
msgstr "FAI"

msgid "Preseeding"
msgstr "Prabibit"

msgid "Monitoring"
msgstr "Pemantauan"

msgid "Virtualization"
msgstr "Virtualisasi"

msgid "Xen"
msgstr "Xen"

msgid "LXC"
msgstr "LXC"

msgid "Advanced Administration"
msgstr "Administrasi Tingkat Lanjut"

msgid "This chapter revisits some aspects we already described, with a different perspective: instead of installing one single computer, we will study mass-deployment systems; instead of creating RAID or LVM volumes at install time, we'll learn to do it by hand so we can later revise our initial choices. Finally, we will discuss monitoring tools and virtualization techniques. As a consequence, this chapter is more particularly targeting professional administrators, and focuses somewhat less on individuals responsible for their home network."
msgstr "Bab ini meninjau kembali beberapa aspek yang telah kami uraikan, dengan perspektif yang berbeda: alih-alih memasang pada sebuah komputer, kita akan mempelajari sistem deployment masal; alih-alih membuat volume RAID atau LVM pada saat instalasi, kita akan belajar melakukannya secara manual sehingga nanti kita dapat merevisi pilihan awal kita. Akhirnya, kita akan mendiskusikan perkakas pemantauan dan teknik virtualisasi. Sebagai konsekuensinya, bab ini secara lebih khusus menarget para administrator profesional, and sedikit kurang brfokus pada para individu yang bertanggungjawab atas jaringan rumahan mereka."

msgid "RAID and LVM"
msgstr "RAID dan LVM"

msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive reinstallation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgstr "<xref linkend=\"installation\" /> mempresentasikan teknologi ini dari sudut pandang pemasang, dan bagaimana itu mengintegrasikan mereka untuk membuat deployment mereka mudah dari awal. Setelah instalasi awal, seorang administrator mesti bisa menangani keperluan ruang penyimpanan yang berkembang tanpa mesti mengandalkan instalasi ulang yang mahal. Maka mereka mesti paham peralatan yang diperlukan untuk memanipulasi volume RAID dan LVM."

#, fuzzy
#| msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former secures the data against hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former ensures the security and availability of the data in case of hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgstr "RAID dan LVM adalah teknik untuk mengabstrakkan volume yang dikait dari pasangan fisik mereka (yaitu hard disk atau partisi); yang pertama mengamankan data dari kegagalan perangkat keras dengan memperkenalkan redundansi, yang belakangan membuat manajemen volume lebih luwes dan tak bergantung kepada ukuran sebenarnya dari disk yang mendasarinya. Dalam kedua kasus, sistem pada akhirnya mendapat perangkat blok baru, yang dapat dipakai untuk membuat sistem berkas atau ruang swap, tanpa perlu mereka dipetakan ke satu disk fisik. RAID dan LVM datang dari latar belakang yang cukup berbeda, tapi fungsionalitas mereka sebagian dapat bertumpang tindih, sehingga mereka sering disinggung bersama-sama."

msgid "<emphasis>PERSPECTIVE</emphasis> Btrfs combines LVM and RAID"
msgstr "<emphasis>PERSPEKTIF</emphasis> Btrfs menggabung LVM dan RAID"

#, fuzzy
#| msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a new filesystem, initially developed at Oracle, that purports to combine the featuresets of LVM and RAID and much more. It is mostly functional, and although it is still tagged “experimental” because its development is incomplete (some features aren't implemented yet), it has already seen some use in production environments. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"
msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a filesystem, initially developed at Oracle, that purports to combine the featuresets of LVM and RAID and much more. <ulink type=\"block\" url=\"https://btrfs.wiki.kernel.org/index.php/Main_Page\" />"
msgstr "Walaupun LVM dan RAID adalah dua subsistem kernel yang berbeda, yang hadir di antara perangkat blok disk dan sistem berkas mereka, <emphasis>btrfs</emphasis> adalah suatu sistem berkas baru, yang pada awalnya dikembangkan di Oracle, yang bertujuan menggabung set fitur dari LVM dan RAID serta lebih banyak lagi. Sebagian besar sudah berfungsi, dan walaupun masih di-tag \"eksperimental\" karena pengembangannya belum lengkap (beberapa fitur belum diimplementasi), itu telah terpakai dalam lingkungan produksi. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"

msgid "Among the noteworthy features are the ability to take a snapshot of a filesystem tree at any point in time. This snapshot copy doesn't initially use any disk space, the data only being duplicated when one of the copies is modified. The filesystem also handles transparent compression of files, and checksums ensure the integrity of all stored data."
msgstr "Diantara fitur yang menarik adalah kemampuan membuat snapshot dari suatu pohon sistem berkas pada sebarang waktu. Snapshot ini pada awalnya tak memakai sebarang ruang disk, data hanya diduplikasi ketika satu dari salinan-salinan dimodifikasi. Sistem berkas juga menangani kompresi transparan dari berkas, dan checksum memastikan integritas dari semua data yang disimpan."

msgid "In both the RAID and LVM cases, the kernel provides a block device file, similar to the ones corresponding to a hard disk drive or a partition. When an application, or another part of the kernel, requires access to a block of such a device, the appropriate subsystem routes the block to the relevant physical layer. Depending on the configuration, this block can be stored on one or several physical disks, and its physical location may not be directly correlated to the location of the block in the logical device."
msgstr "Pada kedua kasus RAID dan LVM, kernel menyediakan suatu berkas perangkat blok, mirip dengan yang berkaitan dengan suatu hard disk atau suatu partisi. Ketika suatu aplikasi, atau bagian lain dari kernel, meminta akses ke suatu blok dari perangkat seperti itu, subsistem yang sesuai mengarahkan blok ke lapisan fisik yang relevan. Bergantung kepada konfigurasi, blok ini dapat disimpan pada satu atau beberapa disk fisik, dan lokasi fisiknya mungkin tak berkorelasi langsung ke lokasi blok dalam perangkat lojik."

msgid "Software RAID"
msgstr "RAID Perangkat Lunak"

msgid "<primary>RAID</primary>"
msgstr "<primary>RAID</primary>"

#, fuzzy
#| msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss and ensure availability in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgstr "RAID adalah <emphasis>Redundant Array of Independent Disks</emphasis> (larik redundan dari disk-disk independen). Tujuan dari sistem ini adalah untuk mencegah kehilangan data dalam kasus kegagalan hard disk. Prinsip umumnya cukup sederhana: data disimpan pada beberapa disk fisik alih-alih hanya satu, dengan tingkat redundansi yang dapat dikonfigurasi. Bergantung kepada banyaknya redundansi ini, dan bahkan dalam kejadian kegagalan disk yang tak terduga, data dapat direkonstruksi tanpa adanya kehilangan dari disk sisanya."

msgid "<emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?"
msgstr "<emphasis>KULTUR</emphasis> <foreignphrase>Independen</foreignphrase> atau <foreignphrase>tidak mahal</foreignphrase>?"

#, fuzzy
#| msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavour of cheapness."
msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavor of cheapness."
msgstr "I dalam RAID pada awalnya merupakan singkatan dari <emphasis>inexpensive (tidak mahal)</emphasis>, karena RAID memungkinkan kenaikan drastis keselamatan data tanpa memerlukan investasi disk canggih yang mahal. Namun mungkin karena masalah citra, kini lebih umum dianggap singkatan dari <emphasis>independen</emphasis>, yang tak membawa kesan murahan yang tak menarik."

msgid "RAID can be implemented either by dedicated hardware (RAID modules integrated into SCSI or SATA controller cards) or by software abstraction (the kernel). Whether hardware or software, a RAID system with enough redundancy can transparently stay operational when a disk fails; the upper layers of the stack (applications) can even keep accessing the data in spite of the failure. Of course, this “degraded mode” can have an impact on performance, and redundancy is reduced, so a further disk failure can lead to data loss. In practice, therefore, one will strive to only stay in this degraded mode for as long as it takes to replace the failed disk. Once the new disk is in place, the RAID system can reconstruct the required data so as to return to a safe mode. The applications won't notice anything, apart from potentially reduced access speed, while the array is in degraded mode or during the reconstruction phase."
msgstr "RAID dapat diwujudkan baik oleh perangkat keras khusus (modul RAID yang terintegrasi ke dalam kartu pengendali SCSI atau SATA) atau oleh abstraksi perangkat lunak (kernel). Apakah perangkat keras atau perangkat lunak, sistem RAID dengan redundansi yang cukup bisa secara transparan tetap operasional ketika sebuah disk gagal; lapisan atas tumpukan (aplikasi) bahkan dapat tetap mengakses data terlepas dari kegagalan. Tentu saja, \"mode terdegradasi\" ini dapat memiliki dampak pada kinerja, dan redundansi berkurang, sehingga kegagalan disk lebih lanjut dapat mengakibatkan kehilangan data. Dalam prakteknya, oleh karena itu, kita akan berusaha untuk hanya berada dalam mode terdegradasi ini selama diperlukannya untuk menggantikan disk yang gagal. Sekali disk baru terpasang sistem RAID dapat merekonstruksi data yang dibutuhkan untuk kembali ke mode aman. Aplikasi tidak akan melihat apa-apa, selain kecepatan akses berpotensi berkurang, sementara larik ada dalam mode terdegradasi atau selama fase rekonstruksi."

msgid "When RAID is implemented by hardware, its configuration generally happens within the BIOS setup tool, and the kernel will consider a RAID array as a single disk, which will work as a standard physical disk, although the device name may be different (depending on the driver)."
msgstr "Ketika RAID diimplementasikan oleh perangkat keras, konfigurasinya umumnya terjadi dalam alat konfigurasi BIOS, dan kernel akan menganggap sebuah array RAID sebagai satu disk, yang akan bekerja sebagai disk fisik standar, meskipun nama perangkat mungkin berbeda (tergantung pada driver)."

msgid "We only focus on software RAID in this book."
msgstr "Kami hanya berfokus pada RAID perangkat lunak dalam buku ini."

msgid "Different RAID Levels"
msgstr "Tingkat-tingkat RAID"

msgid "RAID is actually not a single system, but a range of systems identified by their levels; the levels differ by their layout and the amount of redundancy they provide. The more redundant, the more failure-proof, since the system will be able to keep working with more failed disks. The counterpart is that the usable space shrinks for a given set of disks; seen the other way, more disks will be needed to store a given amount of data."
msgstr "RAID sebenarnya buka satu sistem, tapi berbagai sistem yang diidentifikasi oleh tingkat mereka; tingkat-tingkat itu dibedakan oleh tata letak dan banyaknya redundansi yang mereka sediakan. Semakin banyak redundan, semakin kebal kegagalan, karena sistem akan dapat terus bekerja dengan lebih banyak disk yang gagal. Kekurangannya adalah bahwa ruang yang dapat digunakan menyusut untuk satu set disk tertentu; dilihat dengan cara lain, akan diperlukan lebih banyak disk untuk menyimpan sejumlah data yang diberikan."

msgid "Linear RAID"
msgstr "RAID Linier"

#, fuzzy
#| msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That's about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That is about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgstr "Meskipun subsistem RAID kernel memungkinkan menciptakan \"linear RAID\", ini bukan RAID yang benar, karena konfigurasi ini tidak melibatkan redundansi apapun. Kernel hanya mengumpulkan beberapa disk end-to-end dan menyediakan volume agregat yang dihasilkan sebagai satu disk virtual (satu perangkat blok). Itu adalah satu-satunya fungsinya. Konfigurasi ini jarang digunakan sendirian (lihat nanti untuk pengecualian), terutama karena kurangnya redundansi berarti bahwa salah satu disk gagal membuat seluruh agregat, dan karena itu semua data, tidak tersedia."

msgid "RAID-0"
msgstr "RAID-0"

msgid "This level doesn't provide any redundancy either, but disks aren't simply stuck on end one after another: they are divided in <emphasis>stripes</emphasis>, and the blocks on the virtual device are stored on stripes on alternating physical disks. In a two-disk RAID-0 setup, for instance, even-numbered blocks of the virtual device will be stored on the first physical disk, while odd-numbered blocks will end up on the second physical disk."
msgstr "Tingkat ini tidak menyediakan redundansi apapun, tapi disk-disk tidak hanya sekadar dilekatkan satu di akhir yang lain: mereka dibagi dalam <emphasis>stripe</emphasis>, dan blok-blok di perangkat virtual disimpan dalam stripe di disk-disk fisik yang berbeda-beda. Dalam setup RAID-0 dua-disk, misalnya, blok bernomor genap dari perangkat virtual akan disimpan pada disk fisik pertama, sementara blok bernomor ganjil akan berakhir pada disk fisik kedua."

#, fuzzy
#| msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. However, RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. The disks are utilized entirely by the RAID device, so they should have the same size not to lose performance."
msgstr "Sistem ini tidak bertujuan meningkatkan keandalan, karena (seperti dalam kasus linier) ketersediaan semua data hancur begitu satu disk gagal, tetapi meningkatkan kinerja: selama akses berurutan ke sejumlah besar data yang berdekatan, kernel akan mampu membaca dari kedua disk (atau menulis ke mereka) secara paralel, yang akan meningkatkan laju transfer data. Namun, penggunaan RAID-0 menyusut, ceruk ini diisi oleh LVM (lihat nanti)."

msgid "RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgstr ""

msgid "RAID-1"
msgstr "RAID-1"

msgid "This level, also known as “RAID mirroring”, is both the simplest and the most widely used setup. In its standard form, it uses two physical disks of the same size, and provides a logical volume of the same size again. Data are stored identically on both disks, hence the “mirror” nickname. When one disk fails, the data is still available on the other. For really critical data, RAID-1 can of course be set up on more than two disks, with a direct impact on the ratio of hardware cost versus available payload space."
msgstr "Tingkat ini, juga dikenal sebagai \"RAID mirroring\", adalah yang paling sederhana dan setup yang paling banyak digunakan. Dalam bentuk standar, menggunakan dua disk fisik berukuran sama, dan memberikan volume logis berukuran yang sama lagi. Data disimpan identik pada disk kedua, maka dijuluki \"mirror (cermin)\". Ketika satu disk gagal, data ini masih tersedia di yang lain. Untuk data yang benar-benar penting, RAID-1 dapat tentu saja diatur pada disk yang lebih dari dua, dengan dampak langsung pada rasio biaya perangkat keras versus ruang muatan yang tersedia."

msgid "<emphasis>NOTE</emphasis> Disks and cluster sizes"
msgstr "<emphasis>CATATAN</emphasis> Ukuran klaster dan disk"

msgid "If two disks of different sizes are set up in a mirror, the bigger one will not be fully used, since it will contain the same data as the smallest one and nothing more. The useful available space provided by a RAID-1 volume therefore matches the size of the smallest disk in the array. This still holds for RAID volumes with a higher RAID level, even though redundancy is stored differently."
msgstr "Jika dua disk dengan ukuran yang berbeda diatur dalam cermin, yang lebih besar tidak akan sepenuhnya digunakan, karena itu akan berisi data yang sama seperti yang terkecil dan tidak lebih. Ruang tersedia yang berguna yang disediakan oleh volume RAID-1 karena itu cocok dengan ukuran disk terkecil dalam array. Ini masih berlaku untuk volume RAID dengan tingkat yang lebih tinggi, meskipun redundansi disimpan dengan cara berbeda."

msgid "It is therefore important, when setting up RAID arrays (except for RAID-0 and “linear RAID”), to only assemble disks of identical, or very close, sizes, to avoid wasting resources."
msgstr "Karena itu penting, ketika menyiapkan array RAID (kecuali RAID-0 dan \"linear RAID\"), untuk menyusun hanya disk-disk berukuran identik atau sangat dekat, untuk menghindari membuang-buang sumber daya."

msgid "<emphasis>NOTE</emphasis> Spare disks"
msgstr "<emphasis>CATATAN</emphasis> Disk cadangan"

msgid "RAID levels that include redundancy allow assigning more disks than required to an array. The extra disks are used as spares when one of the main disks fails. For instance, in a mirror of two disks plus one spare, if one of the first two disks fails, the kernel will automatically (and immediately) reconstruct the mirror using the spare disk, so that redundancy stays assured after the reconstruction time. This can be used as another kind of safeguard for critical data."
msgstr "Tingkat RAID yang mencakup redundansi memungkinkan menugaskan disk lebih banyak dari yang dibutuhkan untuk array. Tambahan disk digunakan sebagai suku cadang ketika salah satu disk utama gagal. Sebagai contoh, cermin dua disk ditambah cadangan satu, jika salah satu disk dari dua pertama gagal, kernel akan otomatis (dan segera) merekonstruksi cermin menggunakan disk cadangan, sehingga redundansi tetap terjamin setelah masa rekonstruksi. Ini dapat digunakan sebagai jenis lain dari perlindungan bagi data penting."

msgid "One would be forgiven for wondering how this is better than simply mirroring on three disks to start with. The advantage of the “spare disk” configuration is that the spare disk can be shared across several RAID volumes. For instance, one can have three mirrored volumes, with redundancy assured even in the event of one disk failure, with only seven disks (three pairs, plus one shared spare), instead of the nine disks that would be required by three triplets."
msgstr "Kita akan dimaafkan untuk bertanya-tanya bagaimana hal ini lebih baik daripada sekadar mencerminkan pada tiga disk di awal. Keuntungan dari konfigurasi \"disk cadangan\" adalah bahwa disk cadangan dapat dipakai bersama pada beberapa volume RAID. Sebagai contoh, kita dapat memiliki tiga volume tercermin, dengan redundansi yang dijamin bahkan jika salah satu kegagalan disk, hanya dengan tujuh disk (tiga pasang, ditambah satu cadangan bersama), bukan sembilan disk yang akan dibutuhkan oleh tiga buah kembar tiga."

msgid "This RAID level, although expensive (since only half of the physical storage space, at best, is useful), is widely used in practice. It is simple to understand, and it allows very simple backups: since both disks have identical contents, one of them can be temporarily extracted with no impact on the working system. Read performance is often increased since the kernel can read half of the data on each disk in parallel, while write performance isn't too severely degraded. In case of a RAID-1 array of N disks, the data stays available even with N-1 disk failures."
msgstr "Tingkat RAID ini, walaupun mahal (karena hanya separuh dari ruang penyimpanan fisik, dalam keadaan terbaik, berguna), secara luas digunakan dalam praktek. Hal ini mudah untuk dipahami, dan memungkinkan cadangan yang sangat sederhana: karena disk kedua memiliki isi yang identik, salah satu dari mereka dapat sementara diekstraksi dengan tidak berdampak pada sistem yang bekerja. Kinerja baca sering meningkat karena kernel bisa membaca setengah dari data pada setiap disk secara paralel, sementara kinerja tulis tidak terlalu turun parah. Dalam sebuah array RAID-1 N disk, data tetap tersedia bahkan dengan N-1 disk gagal."

msgid "RAID-4"
msgstr "RAID-4"

msgid "This RAID level, not widely used, uses N disks to store useful data, and an extra disk to store redundancy information. If that disk fails, the system can reconstruct its contents from the other N. If one of the N data disks fails, the remaining N-1 combined with the “parity” disk contain enough information to reconstruct the required data."
msgstr "Tingkat RAID ini, tidak banyak dipakai, menggunakan N disk untuk menyimpan data yang berguna, dan disk tambahan untuk menyimpan informasi redundansi. Jika disk itu gagal, sistem dapat merekonstruksi isinya dari N yang lain. Jika salah satu dari N disk data gagal, N-1 sisa yang dikombinasikan dengan disk \"paritas\" berisi cukup informasi untuk merekonstruksi data yang dibutuhkan."

msgid "RAID-4 isn't too expensive since it only involves a one-in-N increase in costs and has no noticeable impact on read performance, but writes are slowed down. Furthermore, since a write to any of the N disks also involves a write to the parity disk, the latter sees many more writes than the former, and its lifespan can shorten dramatically as a consequence. Data on a RAID-4 array is safe only up to one failed disk (of the N+1)."
msgstr "RAID-4 tidak terlalu mahal karena itu hanya melibatkan peningkatan biaya satu-dari-N dan tidak memiliki dampak yang terlihat pada kinerja baca, tapi penulisan melambat. Selanjutnya, karena menulis ke salah satu dari N disk juga melibatkan menulis ke disk paritas, yang terakhir melihat menulis lebih banyak daripada yang pertama, dan usia pakainya dapat memendek secara dramatis sebagai akibatnya. Data pada array RAID-4 aman hanya sampai dengan satu disk gagal (dari N+1)."

msgid "RAID-5"
msgstr "RAID-5"

msgid "RAID-5 addresses the asymmetry issue of RAID-4: parity blocks are spread over all of the N+1 disks, with no single disk having a particular role."
msgstr "RAID-5 menjawab masalah asimetri dari RAID-4: blok paritas disebar ke seluruh N+1 disk, tanpa ada satu disk yang memiliki peran tertentu."

msgid "Read and write performance are identical to RAID-4. Here again, the system stays functional with up to one failed disk (of the N+1), but no more."
msgstr "Kinerja baca dan tulis identik dengan RAID-4. Di sini, sistem tetap berfungsi bila satu disk (dari N+1) gagal, tapi tak boleh lebih."

msgid "RAID-6"
msgstr "RAID-6"

msgid "RAID-6 can be considered an extension of RAID-5, where each series of N blocks involves two redundancy blocks, and each such series of N+2 blocks is spread over N+2 disks."
msgstr "RAID-6 dapat dianggap perluasan dari RAID-5, dimana setiap seri N blok melibatkan dua blok redundansi, dan setiap seri N+2 blok disebar ke N+2 disk."

msgid "This RAID level is slightly more expensive than the previous two, but it brings some extra safety since up to two drives (of the N+2) can fail without compromising data availability. The counterpart is that write operations now involve writing one data block and two redundancy blocks, which makes them even slower."
msgstr "Tingkat RAID ini sedikit lebih mahal daripada dua sebelumnya, tapi itu membawa beberapa keamanan tambahan karena sampai dengan dua drive (dari N+2) bisa gagal tanpa mengorbankan ketersediaan data. Kekurangannya adalah bahwa sekarang operasi tulis melibatkan menulis satu blok data dan dua blok redundansi, yang membuat mereka lebih lambat lagi."

msgid "RAID-1+0"
msgstr "RAID-1+0"

#, fuzzy
#| msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there's no problem with that."
msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there is no problem with that."
msgstr "Ini secara presisi bukan tingkat RAID, tapi penumpukan dua pengelompokan RAID. Mulai dari 2×N disk, kita pertama kali menyiapkan pasangan-pasangan ke N volume RAID-1; N volume ini kemudian dikumpulkan menjadi satu, baik memakai \"linear RAID\" atau (semakin banyak) memakai LVM. Kasus terakhir ini di luar RAID murni, tapi tidak ada masalah dengan itu."

msgid "RAID-1+0 can survive multiple disk failures: up to N in the 2×N array described above, provided that at least one disk keeps working in each of the RAID-1 pairs."
msgstr "RAID-1+0 dapat bertahan dari beberapa disk gagal: sampai N dalam array 2×N yang dijelaskan di atas, asal bahwa setidaknya satu disk tetap bekerja di setiap pasangan RAID-1."

msgid "<emphasis>GOING FURTHER</emphasis> RAID-10"
msgstr "<emphasis>LEBIH JAUH</emphasis> RAID-10"

msgid "RAID-10 is generally considered a synonym of RAID-1+0, but a Linux specificity makes it actually a generalization. This setup allows a system where each block is stored on two different disks, even with an odd number of disks, the copies being spread out along a configurable model."
msgstr "RAID-10 umumnya dianggap sebagai sinonim dari RAID-1+0, namun kekhususan Linux membuat itu sebenarnya generalisasi. Konfigurasi ini memungkinkan sistem dimana setiap blok disimpan pada dua disk berbeda, bahkan dengan cacah disk ganjil, salinan disebar ke model yang dapat dikonfigurasi."

msgid "Performances will vary depending on the chosen repartition model and redundancy level, and of the workload of the logical volume."
msgstr "Kinerja akan bervariasi tergantung pada model repartisi dan tingkat redundansi yang dipilih, dan beban kerja dari volume logis."

msgid "Obviously, the RAID level will be chosen according to the constraints and requirements of each application. Note that a single computer can have several distinct RAID arrays with different configurations."
msgstr "Jelas, tingkat RAID akan dipilih sesuai dengan kendala dan persyaratan setiap aplikasi. Perhatikan bahwa satu komputer dapat memiliki beberapa array RAID yang berbeda dengan konfigurasi yang berbeda."

msgid "Setting up RAID"
msgstr "Menyiapkan RAID"

msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Setting up RAID volumes requires the <emphasis role=\"pkg\">mdadm</emphasis> package; it provides the <command>mdadm</command> command, which allows creating and manipulating RAID arrays, as well as scripts and tools integrating it to the rest of the system, including the monitoring system."
msgstr "Menyiapkan volume RAID memerlukan paket <emphasis role=\"pkg\">mdadm</emphasis>; ini menyediakan perintah <command>mdadm</command>, yang memungkinkan membuat dan memanipulasi array RAID, maupun skrip dan alat-alat yang mengintegrasikan ke seluruh sistem, termasuk sistem pemantauan."

msgid "Our example will be a server with a number of disks, some of which are already used, the rest being available to setup RAID. We initially have the following disks and partitions:"
msgstr "Contoh kita akan menjadi server dengan sejumlah disk, beberapa di antaranya sudah digunakan, sisanya tersedia untuk menyiapkan RAID. Kita awalnya memiliki disk dan partisi sebagai berikut:"

msgid "the <filename>sdb</filename> disk, 4 GB, is entirely available;"
msgstr "disk <filename>sdb</filename>, 4 GB, sepenuhnya tersedia;"

msgid "the <filename>sdc</filename> disk, 4 GB, is also entirely available;"
msgstr "disk <filename>sdc</filename>, 4 GB, ini juga sepenuhnya tersedia;"

msgid "on the <filename>sdd</filename> disk, only partition <filename>sdd2</filename> (about 4 GB) is available;"
msgstr "pada disk <filename>sdd</filename>, hanya partisi <filename>sdd2</filename> (sekitar 4 GB) tersedia;"

msgid "finally, a <filename>sde</filename> disk, still 4 GB, entirely available."
msgstr "akhirnya, disk <filename>sde</filename>, masih 4 GB, sepenuhnya tersedia."

msgid "<emphasis>NOTE</emphasis> Identifying existing RAID volumes"
msgstr "<emphasis>CATATAN</emphasis> Mengidentifikasi volume RAID yang ada"

msgid "The <filename>/proc/mdstat</filename> file lists existing volumes and their states. When creating a new RAID volume, care should be taken not to name it the same as an existing volume."
msgstr "Berkas <filename>/proc/mdstat</filename> memuat daftar volume yang ada dan keadaan mereka. Ketika membuat volume RAID baru, mesti hati-hati untuk tidak memberi nama yang sama dengan volume yang sudah ada."

msgid "We're going to use these physical elements to build two volumes, one RAID-0 and one mirror (RAID-1). Let's start with the RAID-0 volume:"
msgstr "Kita akan menggunakan unsur-unsur fisik ini untuk membangun dua volume, satu RAID-0 dan satu cermin (RAID-1). Mari kita mulai dengan volume RAID-0:"

msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"           Version : 1.2\n"
"     Creation Time : Tue Jun 25 08:47:49 2019\n"
"        Raid Level : raid0\n"
"        Array Size : 8378368 (7.99 GiB 8.58 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 08:47:49 2019\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"        Chunk Size : 512K\n"
"\n"
"Consistency Policy : none\n"
"\n"
"              Name : mirwiz:0  (local to host debian)\n"
"              UUID : 146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"            Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       32        0      active sync   /dev/sdb\n"
"       1       8       48        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 2094592 4k blocks and 524288 inodes\n"
"Filesystem UUID: 413c3dff-ab5e-44e7-ad34-cf1a029cfe98\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.9G   36M  7.4G   1% /srv/raid-0\n"
"</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n<computeroutput>mdadm: Defaulting to version 1.2 metadata\nmdadm: array /dev/md0 started.\n# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n<computeroutput>/dev/md0:\n           Version : 1.2\n     Creation Time : Tue Jun 25 08:47:49 2019\n        Raid Level : raid0\n        Array Size : 8378368 (7.99 GiB 8.58 GB)\n      Raid Devices : 2\n     Total Devices : 2\n       Persistence : Superblock is persistent\n\n       Update Time : Tue Jun 25 08:47:49 2019\n             State : clean \n    Active Devices : 2\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 0\n\n        Chunk Size : 512K\n\nConsistency Policy : none\n\n              Name : mirwiz:0  (local to host debian)\n              UUID : 146e104f:66ccc06d:71c262d7:9af1fbc7\n            Events : 0\n\n    Number   Major   Minor   RaidDevice State\n       0       8       32        0      active sync   /dev/sdb\n       1       8       48        1      active sync   /dev/sdc\n# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\nDiscarding device blocks: done                            \nCreating filesystem with 2094592 4k blocks and 524288 inodes\nFilesystem UUID: 413c3dff-ab5e-44e7-ad34-cf1a029cfe98\nSuperblock backups stored on blocks: \n\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (16384 blocks): done\nWriting superblocks and filesystem accounting information: done \n\n# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n/dev/md0        7.9G   36M  7.4G   1% /srv/raid-0\n</computeroutput>"

#, fuzzy
#| msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It's also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It is also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgstr "Perintah <command>mdadm --create</command> memerlukan beberapa parameter: nama volume yang akan dibuat (<filename>/dev/md*</filename>, dengan MD singkatan dari <foreignphrase>Multiple Devices</foreignphrase>), tingkat RAID, cacah disk (yang wajib meskipun sebagian besar bermakna hanya dengan RAID-1 dan di atasnya), dan drive fisik yang akan digunakan. Setelah perangkat dibuat, kita dapat menggunakannya seperti kita akan menggunakan sebuah partisi normal, membuat sebuah sistem berkas di atasnya, mengait sistem berkas itu, dan sebagainya. Perhatikan bahwa penciptaan kita atas suatu volume RAID-0 pada <filename>md0</filename> hanya kebetulan, dan penomoran array tidak perlu berkorelasi dengan pilihan banyaknya redundansi. Hal ini juga memungkinkan untuk membuat array RAID bernama, dengan memberikan parameter <command>mdadm</command> seperti misalnya <filename>/dev/md/linear</filename> bukan <filename>/dev/md0</filename>."

msgid "Creation of a RAID-1 follows a similar fashion, the differences only being noticeable after the creation:"
msgstr "Penciptaan RAID-1 mengikuti cara yang sama, perbedaannya hanya menjadi terlihat setelah penciptaan:"

msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Tue Jun 25 10:21:22 2019\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 10:22:03 2019\n"
"             State : clean, resyncing \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"     Resync Status : 93% complete\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 16\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       64        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n<computeroutput>mdadm: Note: this array has metadata at the start and\n    may not be suitable as a boot device.  If you plan to\n    store '/boot' on this device please ensure that\n    your boot-loader understands md/v1.x metadata, or use\n    --metadata=0.90\nmdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\nContinue creating array? </computeroutput><userinput>y</userinput>\n<computeroutput>mdadm: Defaulting to version 1.2 metadata\nmdadm: array /dev/md1 started.\n# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n<computeroutput>/dev/md1:\n           Version : 1.2\n     Creation Time : Tue Jun 25 10:21:22 2019\n        Raid Level : raid1\n        Array Size : 4189184 (4.00 GiB 4.29 GB)\n     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n      Raid Devices : 2\n     Total Devices : 2\n       Persistence : Superblock is persistent\n\n       Update Time : Tue Jun 25 10:22:03 2019\n             State : clean, resyncing \n    Active Devices : 2\n   Working Devices : 2\n    Failed Devices : 0\n     Spare Devices : 0\n\nConsistency Policy : resync\n\n     Resync Status : 93% complete\n\n              Name : mirwiz:1  (local to host debian)\n              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n            Events : 16\n\n    Number   Major   Minor   RaidDevice State\n       0       8       64        0      active sync   /dev/sdd2\n       1       8       80        1      active sync   /dev/sde\n# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n<computeroutput>/dev/md1:\n[...]\n          State : clean\n[...]\n</computeroutput>"

msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgstr "<emphasis>TIPS</emphasis> RAID, disk, dan partisi"

msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions, and do not require full disks."
msgstr "Seperti digambarkan oleh contoh kita, peranti RAID dapat dibangun dari partisi disk, dan tidak memerlukan disk penuh."

msgid "A few remarks are in order. First, <command>mdadm</command> notices that the physical elements have different sizes; since this implies that some space will be lost on the bigger element, a confirmation is required."
msgstr "Beberapa komentar perlu disinggung. Pertama, <command>mdadm</command> tahu bahwa elemen-elemen fisik memiliki ukuran yang berbeda; karena hal ini menyiratkan bahwa sebagian ruang akan hilang pada elemen yang lebih besar, konfirmasi diperlukan."

msgid "More importantly, note the state of the mirror. The normal state of a RAID mirror is that both disks have exactly the same contents. However, nothing guarantees this is the case when the volume is first created. The RAID subsystem will therefore provide that guarantee itself, and there will be a synchronization phase as soon as the RAID device is created. After some time (the exact amount will depend on the actual size of the disks…), the RAID array switches to the “active” or “clean” state. Note that during this reconstruction phase, the mirror is in a degraded mode, and redundancy isn't assured. A disk failing during that risk window could lead to losing all the data. Large amounts of critical data, however, are rarely stored on a freshly created RAID array before its initial synchronization. Note that even in degraded mode, the <filename>/dev/md1</filename> is usable, and a filesystem can be created on it, as well as some data copied on it."
msgstr "Lebih penting lagi, perhatikan keadaan cermin. Keadaan normal dari suatu cermin RAID adalah bahwa kedua disk memiliki isi yang tepat sama. Namun, tidak ada yang menjamin ini ketika volume pertama kali dibuat. Subsistem RAID karena itu akan memberikan jaminan itu sendiri, dan akan ada tahap sinkronisasi segera setelah perangkat RAID dibuat. Setelah beberapa waktu (lama persisnya akan tergantung pada ukuran sebenarnya dari disk...), array RAID berpindah ke keadaan \"aktif\" atau \"bersih\". Perhatikan bahwa selama fase rekonstruksi ini, cermin ada dalam mode terdegradasi, dan redundansi tidak dijamin. Sebuah disk yang gagal selama jendela risiko itu bisa mengakibatkan kehilangan semua data. Namun, sejumlah besar data penting, jarang disimpan pada sebuah array RAID yang baru dibuat sebelum sinkronisasi awalnya. Catat bahwa bahkan dalam mode terdegradasi, <filename>/dev/md1</filename> dapat digunakan, dan sebuah sistem berkas dapat dibuat di atasnya, maupun data dapat disalin ke sana."

msgid "<emphasis>TIP</emphasis> Starting a mirror in degraded mode"
msgstr "<emphasis>TIPS</emphasis> Memulai mirror dalam mode terdegradasi"

msgid "Sometimes two disks are not immediately available when one wants to start a RAID-1 mirror, for instance because one of the disks one plans to include is already used to store the data one wants to move to the array. In such circumstances, it is possible to deliberately create a degraded RAID-1 array by passing <filename>missing</filename> instead of a device file as one of the arguments to <command>mdadm</command>. Once the data have been copied to the “mirror”, the old disk can be added to the array. A synchronization will then take place, giving us the redundancy that was wanted in the first place."
msgstr "Kadang-kadang dua disk tidak tersedia seketika saat seseorang ingin memulai suatu cermin RAID-1, misalnya karena salah satu disk yang rencananya akan disertakan sudah digunakan untuk menyimpan data yang ingin dipindah ke array. Dalam keadaan seperti itu, dimungkinkan untuk sengaja menciptakan array RAID-1 yang terdegradasi dengan memberikan <filename>missing</filename>, bukan berkas perangkat sebagai salah satu argumen untuk <command>mdadm</command>. Setelah data telah disalin ke \"cermin\", disk lama dapat ditambahkan ke array. Sinkronisasi kemudian akan terjadi, memberikan kita redundansi yang diinginkan di awal."

msgid "<emphasis>TIP</emphasis> Setting up a mirror without synchronization"
msgstr "<emphasis>TIPS</emphasis> Menyiapkan cermin tanpa sinkronisasi"

msgid "RAID-1 volumes are often created to be used as a new disk, often considered blank. The actual initial contents of the disk is therefore not very relevant, since one only needs to know that the data written after the creation of the volume, in particular the filesystem, can be accessed later."
msgstr "Volume RAID-1 sering dibuat untuk digunakan sebagai disk baru, sering dianggap kosong. Isi awal sebenarnya dari disk ini karena itu tidak sangat relevan, karena hanya perlu diketahui bahwa data setelah penciptaan volume, khususnya sistem berkas, dapat diakses setelahnya."

msgid "One might therefore wonder about the point of synchronizing both disks at creation time. Why care whether the contents are identical on zones of the volume that we know will only be read after we have written to them?"
msgstr "Karena itu orang mungkin bertanya-tanya tentang titik sinkronisasi kedua disk pada waktu penciptaan. Mengapa peduli apakah isi identik pada zona volume yang kita ketahui hanya dapat dibaca setelah kita telah menulis?"

msgid "Fortunately, this synchronization phase can be avoided by passing the <literal>--assume-clean</literal> option to <command>mdadm</command>. However, this option can lead to surprises in cases where the initial data will be read (for instance if a filesystem is already present on the physical disks), which is why it isn't enabled by default."
msgstr "Untungnya, tahap sinkronisasi ini dapat dihindari dengan memberikan opsi <literal>--assume-clean</literal> untuk <command>mdadm</command>. Namun, pilihan ini dapat menyebabkan kejutan dalam kasus-kasus di mana data awal akan dibaca (misalnya jika sistem berkas tersebut sudah hadir pada disk fisik), itulah sebabnya itu tidak diaktifkan secara default."

msgid "Now let's see what happens when one of the elements of the RAID-1 array fails. <command>mdadm</command>, in particular its <literal>--fail</literal> option, allows simulating such a disk failure:"
msgstr "Sekarang mari kita lihat apa yang terjadi ketika salah satu elemen array RAID-1 gagal. <command>mdadm</command>, khususnya opsi <literal>--fail</literal>, memungkinkan simulasi suatu kegagalan disk:"

msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"       Update Time : Tue Jun 25 11:03:44 2019\n"
"             State : clean, degraded \n"
"    Active Devices : 1\n"
"   Working Devices : 1\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 20\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       -       0        0        0      removed\n"
"       1       8       80        1      active sync   /dev/sdd2\n"
"\n"
"       0       8       64        -      faulty   /dev/sde</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n<computeroutput>/dev/md1:\n[...]\n       Update Time : Tue Jun 25 11:03:44 2019\n             State : clean, degraded \n    Active Devices : 1\n   Working Devices : 1\n    Failed Devices : 1\n     Spare Devices : 0\n\nConsistency Policy : resync\n\n              Name : mirwiz:1  (local to host debian)\n              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n            Events : 20\n\n    Number   Major   Minor   RaidDevice State\n       -       0        0        0      removed\n       1       8       80        1      active sync   /dev/sdd2\n\n       0       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "The contents of the volume are still accessible (and, if it is mounted, the applications don't notice a thing), but the data safety isn't assured anymore: should the <filename>sdd</filename> disk fail in turn, the data would be lost. We want to avoid that risk, so we'll replace the failed disk with a new one, <filename>sdf</filename>:"
msgstr "Isi dari volume masih dapat diakses (dan, jika dipasang, aplikasi tidak menyadari apapun), tapi keselamatan data tidak dijamin lagi: seandainya <filename>sdd</filename> disk gagal bergantian, data akan hilang. Kami ingin menghindari risiko, jadi kami akan mengganti disk yang gagal dengan yang baru, <filename>sdf</filename>:"

msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Tue Jun 25 11:09:42 2019\n"
"             State : clean, degraded, recovering \n"
"    Active Devices : 1\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 1\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 27% complete\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      spare rebuilding   /dev/sdf\n"
"       1       8       80        1      active sync   /dev/sdd2\n"
"\n"
"       0       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"       Update Time : Tue Jun 25 11:10:47 2019\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : mirwiz:1  (local to host debian)\n"
"              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n"
"            Events : 39\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sdf\n"
"\n"
"       0       8       64        -      faulty   /dev/sde</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n<computeroutput>mdadm: added /dev/sdf\n# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n<computeroutput>/dev/md1:\n[...]\n      Raid Devices : 2\n     Total Devices : 3\n       Persistence : Superblock is persistent\n\n       Update Time : Tue Jun 25 11:09:42 2019\n             State : clean, degraded, recovering \n    Active Devices : 1\n   Working Devices : 2\n    Failed Devices : 1\n     Spare Devices : 1\n\nConsistency Policy : resync\n\n    Rebuild Status : 27% complete\n\n              Name : mirwiz:1  (local to host debian)\n              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n            Events : 26\n\n    Number   Major   Minor   RaidDevice State\n       2       8       96        0      spare rebuilding   /dev/sdf\n       1       8       80        1      active sync   /dev/sdd2\n\n       0       8       64        -      faulty   /dev/sde\n# </computeroutput><userinput>[...]</userinput>\n<computeroutput>[...]\n# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n<computeroutput>/dev/md1:\n[...]\n       Update Time : Tue Jun 25 11:10:47 2019\n             State : clean \n    Active Devices : 2\n   Working Devices : 2\n    Failed Devices : 1\n     Spare Devices : 0\n\nConsistency Policy : resync\n\n              Name : mirwiz:1  (local to host debian)\n              UUID : 7d123734:9677b7d6:72194f7d:9050771c\n            Events : 39\n\n    Number   Major   Minor   RaidDevice State\n       2       8       96        0      active sync   /dev/sdd2\n       1       8       80        1      active sync   /dev/sdf\n\n       0       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "Here again, the kernel automatically triggers a reconstruction phase during which the volume, although still accessible, is in a degraded mode. Once the reconstruction is over, the RAID array is back to a normal state. One can then tell the system that the <filename>sde</filename> disk is about to be removed from the array, so as to end up with a classical RAID mirror on two disks:"
msgstr "Di sini lagi, kernel secara otomatis memicu tahap rekonstruksi yang ketika berlangsung, meskipun volume masih dapat diakses, berada dalam mode terdegradasi. Setelah rekonstruksi berakhir, array RAID kembali ke keadaan normal. Kita kemudian dapat memberitahu ke sistem bahwa disk <filename>sde</filename> akan dihapus dari array, sehingga berakhir dengan RAID mirror klasik pada dua disk:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      active sync   /dev/sdf</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       2       8       96        0      active sync   /dev/sdd2\n"
"       1       8       80        1      active sync   /dev/sdf</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf</computeroutput>"

msgid "From then on, the drive can be physically removed when the server is next switched off, or even hot-removed when the hardware configuration allows hot-swap. Such configurations include some SCSI controllers, most SATA disks, and external drives operating on USB or Firewire."
msgstr "Selanjutnya drive dapat secara fisik dicabut saat server berikutnya dimatikan, atau bahkan dicabut saat menyala ketika konfigurasi hardware mengizinkan hot-swap. Konfigurasi tersebut termasuk beberapa pengendali SCSI, kebanyakan disk SATA, dan drive eksternal yang beroperasi pada USB atau Firewire."

msgid "Backing up the Configuration"
msgstr "Mem-back up Konfigurasi"

msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgstr "Kebanyakan meta-data tentang volume RAID disimpan secara langsung pada disk yang menusun array ini, sehingga kernel dapat mendeteksi array dan komponen mereka dan merakit mereka secara otomatis saat sistem mulai berjalan. Namun, membuat cadangan konfigurasi ini disarankan, karena deteksi ini tidak kebal kesalahan, dan hanya diharapkan bahwa itu akan gagal tepat dalam keadaan yang sensitif. Dalam contoh kita, jika kegagalan disk <filename>sde</filename> telah nyata (bukan simulasi) dan sistem sudah direstart tanpa menghapus disk <filename>sde</filename> ini, disk ini bisa mulai bekerja lagi karena telah dijajaki selama reboot. Kernel kemudian akan memiliki tiga elemen fisik, masing-masing mengklaim mengandung setengah dari volume RAID yang sama. Sumber kebingungan lain dapat datang ketika volume RAID dari dua server dikonsolidasi hanya ke satu server. Jika array ini sedang berjalan biasanya sebelum disk dipindahkan, kernel akan mampu mendeteksi dan merakit kembali pasangan dengan benar; tetapi jika disk yang dipindah telah diagregasi ke dalam <filename>md1</filename> pada server lama, dan server baru telah memiliki <filename>md1</filename>, salah satu cermin akan diubah nama."

msgid "Backing up the configuration is therefore important, if only for reference. The standard way to do it is by editing the <filename>/etc/mdadm/mdadm.conf</filename> file, an example of which is listed here:"
msgstr "Karena itu cadangan konfigurasi penting, walaupun hanya untuk referensi. Cara standar untuk melakukannya adalah dengan menyunting berkas <filename>/etc/mdadm/mdadm.conf</filename>, contohnya tercantum di sini:"

msgid "<command>mdadm</command> configuration file"
msgstr "berkas konfigurasi <command>mdadm</command>"

msgid ""
"# mdadm.conf\n"
"#\n"
"# !NB! Run update-initramfs -u after updating this file.\n"
"# !NB! This will ensure that initramfs has an uptodate copy.\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# auto-create devices with Debian standard permissions\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c\n"
"\n"
"# This configuration was auto-generated on Tue, 25 Jun 2019 07:54:35 -0400 by mkconf"
msgstr "# mdadm.conf\n#\n# !NB! Run update-initramfs -u after updating this file.\n# !NB! This will ensure that initramfs has an uptodate copy.\n#\n# Please refer to mdadm.conf(5) for information about this file.\n#\n\n# by default (built-in), scan all partitions (/proc/partitions) and all\n# containers for MD superblocks. alternatively, specify devices to scan, using\n# wildcards if desired.\nDEVICE /dev/sd*\n\n# auto-create devices with Debian standard permissions\nCREATE owner=root group=disk mode=0660 auto=yes\n\n# automatically tag new arrays as belonging to the local system\nHOMEHOST &lt;system&gt;\n\n# instruct the monitoring daemon where to send mail alerts\nMAILADDR root\n\n# definitions of existing MD arrays\nARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\nARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c\n\n# This configuration was auto-generated on Tue, 25 Jun 2019 07:54:35 -0400 by mkconf"

msgid "One of the most useful details is the <literal>DEVICE</literal> option, which lists the devices where the system will automatically look for components of RAID volumes at start-up time. In our example, we replaced the default value, <literal>partitions containers</literal>, with an explicit list of device files, since we chose to use entire disks and not only partitions, for some volumes."
msgstr "Salah satu rincian paling berguna adalah opsi <literal>DEVICE</literal>, yang berisi daftar perangkat tempat sistem akan secara otomatis mencari komponen volume RAID saat start-up. Dalam contoh, kita menggantikan nilai default, <literal>partitions containers</literal>, dengan daftar eksplisit berkas perangkat, karena kita memilih untuk menggunakan seluruh disk dan tidak hanya partisi, untuk beberapa volume."

msgid "The last two lines in our example are those allowing the kernel to safely pick which volume number to assign to which array. The metadata stored on the disks themselves are enough to re-assemble the volumes, but not to determine the volume number (and the matching <filename>/dev/md*</filename> device name)."
msgstr "Dua baris terakhir dalam contoh kita adalah yang memungkinkan kernel untuk secara aman memilih nomor volume yang ditetapkan ke array mana. Metadata yang tersimpan pada disk itu sendiri cukup untuk membangun kembali volume, tetapi tidak untuk menentukan nomor volume (dan nama perangkat <filename>/dev/md*</filename> yang cocok)."

msgid "Fortunately, these lines can be generated automatically:"
msgstr "Untungnya, baris-baris ini dapat dihasilkan secara otomatis:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
#| "<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=146e104f:66ccc06d:71c262d7:9af1fbc7\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=7d123734:9677b7d6:72194f7d:9050771c</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"

msgid "The contents of these last two lines doesn't depend on the list of disks included in the volume. It is therefore not necessary to regenerate these lines when replacing a failed disk with a new one. On the other hand, care must be taken to update the file when creating or deleting a RAID array."
msgstr "Isi dari dua baris terakhir ini tidak tergantung pada daftar disk yang disertakan dalam volume. Maka tidak diperlukan untuk meregenerasi baris-baris ini ketika menggantikan disk gagal dengan yang baru. Di sisi lain, perawatan harus diambil untuk memperbarui berkas ketika membuat atau menghapus sebuah array RAID."

msgid "<primary>LVM</primary>"
msgstr "<primary>LVM</primary>"

msgid "<primary>Logical Volume Manager</primary>"
msgstr "<primary>Logical Volume Manager</primary>"

msgid "LVM, the <emphasis>Logical Volume Manager</emphasis>, is another approach to abstracting logical volumes from their physical supports, which focuses on increasing flexibility rather than increasing reliability. LVM allows changing a logical volume transparently as far as the applications are concerned; for instance, it is possible to add new disks, migrate the data to them, and remove the old disks, without unmounting the volume."
msgstr "LVM, <emphasis>Logical Volume Manager</emphasis>, adalah pendekatan lain untuk mengabstrakkan volume logis dari dukungan fisik mereka, yang berfokus pada peningkatan fleksibilitas daripada meningkatkan kehandalan. LVM dapat mengubah volume logis secara transparan bagi aplikasi; sebagai contoh, sangat mungkin untuk menambahkan disk baru, memigrasi data ke mereka, dan menghapus disk lama, tanpa melepas kait volume."

msgid "LVM Concepts"
msgstr "Konsep LVM"

msgid "This flexibility is attained by a level of abstraction involving three concepts."
msgstr "Fleksibilitas ini dicapai dengan tingkat abstraksi yang melibatkan tiga konsep."

msgid "First, the PV (<emphasis>Physical Volume</emphasis>) is the entity closest to the hardware: it can be partitions on a disk, or a full disk, or even any other block device (including, for instance, a RAID array). Note that when a physical element is set up to be a PV for LVM, it should only be accessed via LVM, otherwise the system will get confused."
msgstr "Pertama, PV (<emphasis>Physical Volume</emphasis>) adalah entitas terdekat dengan perangkat keras: itu bisa berupa partisi pada disk atau seluruh disk, atau bahkan perangkat blok lain (termasuk, sebagai contoh, sebuah array RAID). Perhatikan bahwa ketika sebuah elemen fisik diatur hingga menjadi PV untuk LVM, itu mesti hanya diakses melalui LVM, jika tidak sistem akan bingung."

#, fuzzy
#| msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there's no risk of using them directly."
msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there is no risk of using them directly."
msgstr "Sejumlah PV dapat dikumpulkan dalam VG (<emphasis>Volume Group</emphasis>), yang dapat dibandingkan dengan disk virtual dan extensible. VG abstrak, dan tidak muncul dalam perangkat berkas di hirarki <filename>/dev</filename>, sehingga tidak ada risiko menggunakan mereka secara langsung."

msgid "The third kind of object is the LV (<emphasis>Logical Volume</emphasis>), which is a chunk of a VG; if we keep the VG-as-disk analogy, the LV compares to a partition. The LV appears as a block device with an entry in <filename>/dev</filename>, and it can be used as any other physical partition can be (most commonly, to host a filesystem or swap space)."
msgstr "Jenis ke tiga objek adalah LV (<emphasis>Logical Volume</emphasis>), yang berupa potongan dari suatu VG; jika kita memakai analogi VG-sebagai-disk, LV setara dengan partisi. LV muncul sebagai perangkat blok dengan entri di <filename>/dev</filename>, dan dapat digunakan seperti setiap partisi fisik lainnya dapat (paling sering, mewadahi sebuah sistem berkas atau ruang swap)."

msgid "The important thing is that the splitting of a VG into LVs is entirely independent of its physical components (the PVs). A VG with only a single physical component (a disk for instance) can be split into a dozen logical volumes; similarly, a VG can use several physical disks and appear as a single large logical volume. The only constraint, obviously, is that the total size allocated to LVs can't be bigger than the total capacity of the PVs in the volume group."
msgstr "Yang penting adalah bahwa pemisahan VG ke LV sepenuhnya independen dari komponen fisiknya (PV). VG dengan hanya satu komponen fisik (disk misalnya) dapat dipisah menjadi selusin volume logis; demikian pula, sebuah VG dapat menggunakan beberapa disk fisik dan muncul sebagai satu volume logis yang besar. Satu-satunya kendala, jelas, adalah bahwa ukuran total yang dialokasikan untuk LV tidak bisa lebih dari total kapasitas dari PV dalam kelompok volume."

msgid "It often makes sense, however, to have some kind of homogeneity among the physical components of a VG, and to split the VG into logical volumes that will have similar usage patterns. For instance, if the available hardware includes fast disks and slower disks, the fast ones could be clustered into one VG and the slower ones into another; chunks of the first one can then be assigned to applications requiring fast data access, while the second one will be kept for less demanding tasks."
msgstr "Namun sering masuk akal untuk memiliki semacam keseragaman antara komponen fisik VG, dan untuk membagi VG menjadi volume logis yang akan memiliki pola penggunaan serupa. Misalnya, jika perangkat keras yang tersedia termasuk disk cepat dan disk lambat, yang cepat dapat dikelompokkan ke satu VG dan yang lambat ke lain; potongan pertama dapat kemudian ditugaskan untuk aplikasi yang membutuhkan akses data yang cepat, sementara yang kedua akan disimpan untuk tugas-tugas yang kurang menuntut."

msgid "In any case, keep in mind that an LV isn't particularly attached to any one PV. It is possible to influence where the data from an LV are physically stored, but this possibility isn't required for day-to-day use. On the contrary: when the set of physical components of a VG evolves, the physical storage locations corresponding to a particular LV can be migrated across disks (while staying within the PVs assigned to the VG, of course)."
msgstr "Dalam kasus apapun, perlu diingat bahwa LV tidak perlu melekat ke PV manapun. Dimungkinkan untuk mempengaruhi mana data dari LV secara fisik disimpan, tapi kemungkinan ini tidak diperlukan untuk penggunaan sehari-hari. Sebaliknya: ketika set komponen fisik VG berkembang, lokasi penyimpanan fisik yang sesuai dengan LV tertentu dapat bermigrasi di seluruh disk (dan tentu saja tetap di dalam PVs yang ditugaskan untuk VG)."

msgid "Setting up LVM"
msgstr "Menyiapkan LVM"

msgid "Let us now follow, step by step, the process of setting up LVM for a typical use case: we want to simplify a complex storage situation. Such a situation usually happens after some long and convoluted history of accumulated temporary measures. For the purposes of illustration, we'll consider a server where the storage needs have changed over time, ending up in a maze of available partitions split over several partially used disks. In more concrete terms, the following partitions are available:"
msgstr "Mari kita sekarang ikuti, langkah demi langkah, proses pengaturan LVM untuk kasus penggunaan yang khas: kami ingin menyederhanakan situasi kompleks penyimpanan. Situasi seperti ini biasanya terjadi setelah beberapa sejarah yang panjang dan berbelit dari akumulasi langkah-langkah sementara. Untuk tujuan ilustrasi, kami akan mempertimbangkan server yang kebutuhan penyimpanannya telah berubah dari waktu ke waktu, berakhir dalam labirin dari partisi-partisi yang terpecah ke beberapa disk yang terpakai sebagian. Secara lebih konkret, partisi berikut tersedia:"

msgid "on the <filename>sdb</filename> disk, a <filename>sdb2</filename> partition, 4 GB;"
msgstr "pada disk <filename>sdb</filename>, sebuah partisi <filename>sdb2</filename>, 4 GB;"

msgid "on the <filename>sdc</filename> disk, a <filename>sdc3</filename> partition, 3 GB;"
msgstr "pada disk <filename>sdc</filename>, sebuah partisi <filename>sdc3</filename>, 3 GB;"

msgid "the <filename>sdd</filename> disk, 4 GB, is fully available;"
msgstr "disk <filename>sdd</filename>, 4 GB, sepenuhnya tersedia;"

msgid "on the <filename>sdf</filename> disk, a <filename>sdf1</filename> partition, 4 GB; and a <filename>sdf2</filename> partition, 5 GB."
msgstr "pada disk <filename>sdf</filename>, partisi <filename>sdf1</filename>, 4 GB; dan partisi <filename>sdf2</filename>, 5 GB."

msgid "In addition, let's assume that disks <filename>sdb</filename> and <filename>sdf</filename> are faster than the other two."
msgstr "Selain itu, mari kita asumsikan bahwa disk <filename>sdb</filename> dan <filename>sdf</filename> adalah lebih cepat daripada dua lainnya."

msgid "Our goal is to set up three logical volumes for three different applications: a file server requiring 5 GB of storage space, a database (1 GB) and some space for back-ups (12 GB). The first two need good performance, but back-ups are less critical in terms of access speed. All these constraints prevent the use of partitions on their own; using LVM can abstract the physical size of the devices, so the only limit is the total available space."
msgstr "Tujuan kami adalah untuk mengatur tiga volume logis untuk tiga aplikasi yang berbeda: server berkas memerlukan ruang penyimpanan 5 GB, sebuah basis data (1 GB) dan ruang untuk back-up (12 GB). Dua yang pertama perlu kinerja yang baik, tapi back-up kurang kritis dalam hal kecepatan akses. Semua kendala ini mencegah penggunaan partisi sendirian; menggunakan LVM dapat mengabstraksi ukuran fisik dari perangkat, sehingga satu-satunya batas adalah jumlah ruang yang tersedia."

msgid "The required tools are in the <emphasis role=\"pkg\">lvm2</emphasis> package and its dependencies. When they're installed, setting up LVM takes three steps, matching the three levels of concepts."
msgstr "Alat-alat yang diperlukan ada dalam paket <emphasis role=\"pkg\">lvm2</emphasis> dan dependensinya. Ketika mereka sedang diinstal, pengaturan LVM mengambil tiga langkah, cocok dengan konsep tiga tingkat."

msgid "First, we prepare the physical volumes using <command>pvcreate</command>:"
msgstr "Pertama, kami siapkan volume fisik menggunakan <command>pvcreate</command>:"

msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               z4Clgk-T5a4-C27o-1P0E-lIAF-OeUM-e7EMwq\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n"
"  Physical volume \"/dev/sdd\" successfully created.\n"
"  Physical volume \"/dev/sdf1\" successfully created.\n"
"  Physical volume \"/dev/sdf2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay -C</userinput><computeroutput>\n"
"  PV         VG Fmt  Attr PSize  PFree \n"
"  /dev/sdb2     lvm2 ---   4.00g  4.00g\n"
"  /dev/sdc3     lvm2 ---   3.00g  3.00g\n"
"  /dev/sdd      lvm2 ---   4.00g  4.00g\n"
"  /dev/sdf1     lvm2 ---   4.00g  4.00g\n"
"  /dev/sdf2     lvm2 ---  &lt;5.00g &lt;5.00g\n"
"</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n<computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n# </computeroutput><userinput>pvdisplay</userinput>\n<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n  --- NEW Physical volume ---\n  PV Name               /dev/sdb2\n  VG Name               \n  PV Size               4.00 GiB\n  Allocatable           NO\n  PE Size               0   \n  Total PE              0\n  Free PE               0\n  Allocated PE          0\n  PV UUID               z4Clgk-T5a4-C27o-1P0E-lIAF-OeUM-e7EMwq\n\n# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n<computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n  Physical volume \"/dev/sdd\" successfully created.\n  Physical volume \"/dev/sdf1\" successfully created.\n  Physical volume \"/dev/sdf2\" successfully created.\n# </computeroutput><userinput>pvdisplay -C</userinput><computeroutput>\n  PV         VG Fmt  Attr PSize  PFree \n  /dev/sdb2     lvm2 ---   4.00g  4.00g\n  /dev/sdc3     lvm2 ---   3.00g  3.00g\n  /dev/sdd      lvm2 ---   4.00g  4.00g\n  /dev/sdf1     lvm2 ---   4.00g  4.00g\n  /dev/sdf2     lvm2 ---  &lt;5.00g &lt;5.00g\n</computeroutput>"

msgid "So far, so good; note that a PV can be set up on a full disk as well as on individual partitions of it. As shown above, the <command>pvdisplay</command> command lists the existing PVs, with two possible output formats."
msgstr "Sejauh ini, masih baik; perhatikan bahwa PV dapat disiapkan pada seluruh disk maupun pada partisi individunya. Seperti yang ditunjukkan di atas, perintah <command>pvdisplay</command> menampilkan daftar PVs yang ada, dengan dua format keluaran mungkin."

msgid "Now let's assemble these physical elements into VGs using <command>vgcreate</command>. We'll gather only PVs from the fast disks into a <filename>vg_critical</filename> VG; the other VG, <filename>vg_normal</filename>, will also include slower elements."
msgstr "Sekarang mari kita merakit elemen-elemen fisik ini menjadi VG menggunakan <command>vgcreate</command>. Kita akan mengumpulkan hanya PV-PV dari disk cepat ke VG <filename>vg_critical</filename>; VG lain, <filename>vg_normal</filename>, juga akan memuat elemen-elemen yang lebih lambat."

msgid ""
"<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               7.99 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2046\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2046 / 7.99 GiB\n"
"  VG UUID               wAbBjx-d82B-q7St-0KFf-z40h-w5Mh-uAXkNZ\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize   VFree  \n"
"  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n"
"  vg_normal     3   0   0 wz--n- &lt;11.99g &lt;11.99g\n"
"</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n<computeroutput>  Volume group \"vg_critical\" successfully created\n# </computeroutput><userinput>vgdisplay</userinput>\n<computeroutput>  --- Volume group ---\n  VG Name               vg_critical\n  System ID             \n  Format                lvm2\n  Metadata Areas        2\n  Metadata Sequence No  1\n  VG Access             read/write\n  VG Status             resizable\n  MAX LV                0\n  Cur LV                0\n  Open LV               0\n  Max PV                0\n  Cur PV                2\n  Act PV                2\n  VG Size               7.99 GiB\n  PE Size               4.00 MiB\n  Total PE              2046\n  Alloc PE / Size       0 / 0   \n  Free  PE / Size       2046 / 7.99 GiB\n  VG UUID               wAbBjx-d82B-q7St-0KFf-z40h-w5Mh-uAXkNZ\n\n# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n<computeroutput>  Volume group \"vg_normal\" successfully created\n# </computeroutput><userinput>vgdisplay -C</userinput>\n<computeroutput>  VG          #PV #LV #SN Attr   VSize   VFree  \n  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n  vg_normal     3   0   0 wz--n- &lt;11.99g &lt;11.99g\n</computeroutput>"

msgid "Here again, commands are rather straightforward (and <command>vgdisplay</command> proposes two output formats). Note that it is quite possible to use two partitions of the same physical disk into two different VGs. Note also that we used a <filename>vg_</filename> prefix to name our VGs, but it is nothing more than a convention."
msgstr "Di sini lagi, perintahnya agak sederhana (dan <command>vgdisplay</command> mengusulkan dua format output). Perhatikan bahwa sangat mungkin untuk menggunakan dua partisi dari disk fisik yang sama ke dua VG yang berbeda. Perhatikan juga bahwa kita menggunakan awalan <filename>vg_</filename> untuk nama VG kita, tapi itu tidak lebih dari sebuah konvensi."

#, fuzzy
#| msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB, respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgstr "Kita sekarang memiliki dua \"disk virtual\", masing-masing berukuran sekitar 8 GB dan 12 GB. Mari kita sekarang mengukir mereka ke dalam \"partisi virtual\" (LV). Ini melibatkan perintah <command>lvcreate</command>, dan sintaks yang agak lebih kompleks:"

msgid ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created.\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                W6XT08-iBBx-Nrw2-f8F2-r2y4-Ltds-UrKogV\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time debian, 2019-11-30 22:45:46 -0500\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           254:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created.\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal</userinput>\n"
"<computeroutput>  Rounding up size to full physical extent 11.98 GiB\n"
"  Logical volume \"lv_backups\" created.\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 11.98g</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n<computeroutput>  Logical volume \"lv_files\" created.\n# </computeroutput><userinput>lvdisplay</userinput>\n<computeroutput>  --- Logical volume ---\n  LV Path                /dev/vg_critical/lv_files\n  LV Name                lv_files\n  VG Name                vg_critical\n  LV UUID                W6XT08-iBBx-Nrw2-f8F2-r2y4-Ltds-UrKogV\n  LV Write Access        read/write\n  LV Creation host, time debian, 2019-11-30 22:45:46 -0500\n  LV Status              available\n  # open                 0\n  LV Size                5.00 GiB\n  Current LE             1280\n  Segments               2\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     256\n  Block device           254:0\n\n# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n<computeroutput>  Logical volume \"lv_base\" created.\n# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal</userinput>\n<computeroutput>  Rounding up size to full physical extent 11.98 GiB\n  Logical volume \"lv_backups\" created.\n# </computeroutput><userinput>lvdisplay -C</userinput>\n<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n  lv_base    vg_critical -wi-a---  1.00g                                           \n  lv_files   vg_critical -wi-a---  5.00g                                           \n  lv_backups vg_normal   -wi-a--- 11.98g</computeroutput>"

msgid "Two parameters are required when creating logical volumes; they must be passed to the <command>lvcreate</command> as options. The name of the LV to be created is specified with the <literal>-n</literal> option, and its size is generally given using the <literal>-L</literal> option. We also need to tell the command what VG to operate on, of course, hence the last parameter on the command line."
msgstr "Dua parameter diperlukan ketika membuat volume logis; mereka harus diberikan ke <command>lvcreate</command> sebagai opsi. Nama LV yang akan dibuat ditetapkan dengan opsi <literal>-n</literal>, dan ukurannya biasanya diberikan menggunakan opsi <literal>-L</literal>. Tentu saja kita juga perlu memberitahu ke perintah, VG mana yang dikenai operasi, maka diberikanlah parameter terakhir pada baris perintah."

msgid "<emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> options"
msgstr "<emphasis>LEBIH JAUH</emphasis> opsi-opsi <command>lvcreate</command>"

msgid "The <command>lvcreate</command> command has several options to allow tweaking how the LV is created."
msgstr "Perintah <command>lvcreate</command> memiliki beberapa opsi untuk memungkinkan menala atas bagaimana cara LV dibuat."

msgid "Let's first describe the <literal>-l</literal> option, with which the LV's size can be given as a number of blocks (as opposed to the “human” units we used above). These blocks (called PEs, <emphasis>physical extents</emphasis>, in LVM terms) are contiguous units of storage space in PVs, and they can't be split across LVs. When one wants to define storage space for an LV with some precision, for instance to use the full available space, the <literal>-l</literal> option will probably be preferred over <literal>-L</literal>."
msgstr "Mari kita pertama menjelaskan opsi <literal>-l</literal>, dengannya ukuran LV dapat diberikan sebagai cacah blok (sebagai lawan dari unit \"manusia\" yang kita digunakan di atas). Blok-blok ini (disebut PE, <emphasis>physical extents</emphasis> dalam istilah LVM) adalah unit-unit ruang penyimpanan yang bersebelahan di PV, dan mereka tidak dapat dipecah di LV. Ketika seseorang ingin menentukan ruang penyimpanan untuk LV secara cukup presisi, misalnya menggunakan seluruh ruang yang tersedia, opsi <literal>-l</literal> mungkin akan lebih disukai daripada <literal>-L</literal>."

#, fuzzy
#| msgid "It's also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgid "It is also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgstr "Memungkinkan juga untuk menunjuk pada lokasi fisik LV, sehingga extent disimpan pada PV tertentu (tentu saja masih tetap ada di dalam yang ditugaskan untuk VG). Karena kita tahu bahwa <filename>sdb</filename> lebih cepat daripada <filename>sdf</filename>, kita mungkin ingin menyimpan <filename>lv_base</filename> di sana jika kita ingin memberikan keuntungan kepada server basis data dibandingkan dengan server berkas. Baris perintah menjadi: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Perhatikan bahwa perintah ini bisa gagal jika PV tidak memiliki cukup extent bebas. Dalam contoh kita, kita mungkin harus membuat <filename>lv_base</filename> sebelum <filename>lv_files</filename> untuk menghindari situasi ini - atau membebaskan sebagian ruang di <filename>sdb2</filename> dengan perintah <command>pvmove</command>."

msgid "Logical volumes, once created, end up as block device files in <filename>/dev/mapper/</filename>:"
msgstr "Volume logis, sekali dibuat, akan menjadi berkas perangkat blok dalam <filename>/dev/mapper/</filename>:"

msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"

#, fuzzy
#| msgid "<emphasis>NOTE</emphasis> Autodetecting LVM volumes"
msgid "<emphasis>NOTE</emphasis> Auto-detecting LVM volumes"
msgstr "<emphasis>CATATAN</emphasis> Mendeteksi otomatis volume LVM"

msgid "When the computer boots, the <filename>lvm2-activation</filename> systemd service unit executes <command>vgchange -aay</command> to “activate” the volume groups: it scans the available devices; those that have been initialized as physical volumes for LVM are registered into the LVM subsystem, those that belong to volume groups are assembled, and the relevant logical volumes are started and made available. There is therefore no need to edit configuration files when creating or modifying LVM volumes."
msgstr "Ketika komputer boot, unit layanan systemd <filename>lvm2-activation</filename> mengeksekusi <command>vgchange -aay</command> untuk \"mengaktifkan\" kelompok volume: memindai perangkat yang tersedia; yang telah diinisialisasi sebagai fisik untuk LVM didaftarkan ke subsistem LVM, yang berasal dari kelompok-kelompok volume dirakit, dan volume logis yang relevan dimulai dan dibuat tersedia. Karena itu tidak perlu menyunting berkas konfigurasi ketika membuat atau memodifikasi volume-volume LVM."

msgid "Note, however, that the layout of the LVM elements (physical and logical volumes, and volume groups) is backed up in <filename>/etc/lvm/backup</filename>, which can be useful in case of a problem (or just to sneak a peek under the hood)."
msgstr "Namun, perlu diketahui bahwa tata letak elemen LVM (volume fisik dan logis, dan kelompok-kelompok volume) direkam cadang dalam <filename>/etc/lvm/backup</filename>, yang dapat berguna dalam hal ada masalah (atau hanya untuk sekedar mengintip di balik layar)."

msgid "To make things easier, convenience symbolic links are also created in directories matching the VGs:"
msgstr "Untuk membuat semua lebih mudah, taut simbolik juga dibuat dalam direktori-direktori yang cocok dengan VG:"

msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"

msgid "The LVs can then be used exactly like standard partitions:"
msgstr "LV kemudian dapat digunakan persis seperti partisi standar:"

msgid ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 3140608 4k blocks and 786432 inodes\n"
"Filesystem UUID: b9e6ed2f-cb37-43e9-87d8-e77568446225\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   41M   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n<computeroutput>mke2fs 1.44.5 (15-Dec-2018)\nDiscarding device blocks: done                            \nCreating filesystem with 3140608 4k blocks and 786432 inodes\nFilesystem UUID: b9e6ed2f-cb37-43e9-87d8-e77568446225\nSuperblock backups stored on blocks: \n\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (16384 blocks): done\nWriting superblocks and filesystem accounting information: done \n\n# </computeroutput><userinput>mkdir /srv/backups</userinput>\n<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n/dev/mapper/vg_normal-lv_backups   12G   41M   12G   1% /srv/backups\n# </computeroutput><userinput>[...]</userinput>\n<computeroutput>[...]\n# </computeroutput><userinput>cat /etc/fstab</userinput>\n<computeroutput>[...]\n/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"

msgid "From the applications' point of view, the myriad small partitions have now been abstracted into one large 12 GB volume, with a friendlier name."
msgstr "Dari sudut pandang aplikasi, berbagai partisi kecil sekarang telah diabstrakkan ke dalam satu volume 12 GB besar, dengan nama yang lebih mudah."

msgid "LVM Over Time"
msgstr "LVM Dari Waktu Ke Waktu"

msgid "Even though the ability to aggregate partitions or physical disks is convenient, this is not the main advantage brought by LVM. The flexibility it brings is especially noticed as time passes, when needs evolve. In our example, let's assume that new large files must be stored, and that the LV dedicated to the file server is too small to contain them. Since we haven't used the whole space available in <filename>vg_critical</filename>, we can grow <filename>lv_files</filename>. For that purpose, we'll use the <command>lvresize</command> command, then <command>resize2fs</command> to adapt the filesystem accordingly:"
msgstr "Meskipun kemampuan untuk mengagregasi partisi atau disk fisik itu nyaman, ini bukanlah keuntungan utama yang dibawa oleh LVM. Fleksibilitas yang dibawanya terutama teramati seiring berjalannya waktu, ketika kebutuhan berevolusi. Dalam contoh kita, mari kita asumsikan bahwa berkas besar baru harus disimpan, dan bahwa LV yang didedikasikan untuk server berkas terlalu kecil untuk menampung mereka. Karena kita belum menggunakan seluruh ruang yang tersedia di <filename>vg_critical</filename>, kita bisa perbesar <filename>lv_files</filename>. Untuk tujuan tersebut, kita akan menggunakan perintah <command>lvresize</command>, lalu <command>resize2fs</command> untuk mengadaptasi sistem berkas:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 5.00g\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
#| "# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
#| "  Logical volume lv_files successfully resized\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 7.00g\n"
#| "# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
#| "<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
#| "Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
#| "old_desc_blocks = 1, new_desc_blocks = 1\n"
#| "The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
#| "\n"
#| "# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  4.9G  4.2G  485M  90% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 7.99g 1.99g\n"
"# </computeroutput><userinput>lvresize -L 6G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\n"
"  Logical volume vg_critical/lv_files successfully resized.\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 6.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.44.5 (15-Dec-2018)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1572864 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.9G  4.2G  1.5G  75% /srv/files</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
"# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
"  Logical volume lv_files successfully resized\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 7.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"

msgid "<emphasis>CAUTION</emphasis> Resizing filesystems"
msgstr "<emphasis>HATI-HATI</emphasis> Mengubah ukuran sistem berkas"

#, fuzzy
#| msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It's rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It is rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgstr "Tidak semua sistem berkas dapat diubah ukurannya secara daring; mengubah ukuran volume oleh karena itu mungkin pertama memerlukan melepas kait sistem berkas dan mengait ulang setelah itu. Tentu saja, jika seseorang ingin mengecilkan ruang yang dialokasikan untuk LV, sistem berkas harus diperkecil dulu; urutan dibalik ketika perubahan ukuran untuk arah lain: volume logis harus diperbesar sebelum sistem berkas di atasnya. Hal ini cukup sederhana, karena kapanpun ukuran sistem tidak boleh lebih besar dari perangkat blok tempat dia berada (apakah perangkat berupa partisi fisik atau volume logis)."

msgid "The ext3, ext4 and xfs filesystems can be grown online, without unmounting; shrinking requires an unmount. The reiserfs filesystem allows online resizing in both directions. The venerable ext2 allows neither, and always requires unmounting."
msgstr "Sistem berkas ext3, ext4, dan xfs dapat diperbesar secara daring, tanpa melepas kain; menyusutkan memerlukan melepas kait. Sistem berkas reiserfs memungkinkan perubahan ukuran secara daring di kedua arah. ext2 tidak memungkinkan keduanya, dan selalu membutuhkan melepas kait."

msgid "We could proceed in a similar fashion to extend the volume hosting the database, only we've reached the VG's available space limit:"
msgstr "Kita bisa melanjutkan dengan cara yang sama untuk memperbesar volume yang mewadai basis data, tapi kita telah mencapai batas ruang VG yang tersedia:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  976M  882M   28M  97% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree   \n"
"  vg_critical   2   2   0 wz--n- 7.99g 1016.00m</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
"  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"

msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb1</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgstr "Tidak masalah, karena LVM memungkinkan menambahkan volume fisik ke grup volume yang ada. Misalnya, mungkin kita telah memperhatikan bahwa partisi <filename>sdb1</filename>, yang sejauh ini digunakan di luar LVM, hanya berisi arsip yang dapat dipindahkan ke <filename>lv_backups</filename>. Kita sekarang dapat mendaur ulang itu dan mengintegrasikannya ke grup volume, dan dengan demikian memperoleh kembali ruang bebas. Ini adalah tujuan dari perintah <command>vgextend</command>. Tentu saja, partisi harus disiapkan sebagai sebuah volume fisik terlebih dahulu. Setelah VG telah diperbesar, kita dapat menggunakan perintah sejenis seperti yang sebelumnya untuk menumbuhkan volume logis kemudian sistem berkasnya:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
#| "# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created.\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
"  vg_critical   3   2   0 wz--n- &lt;9.99g &lt;1.99g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  882M  994M  48% /srv/base</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"

msgid "<emphasis>GOING FURTHER</emphasis> Advanced LVM"
msgstr "<emphasis>LEBIH JAUH</emphasis> LVM tingkat lanjut"

#, fuzzy
#| msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance, to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgstr "LVM juga melayani untuk penggunaan tingkat lanjut, dimana banyak rincian dapat dinyatakan secara manual. Misalnya, administrator dapat menala ukuran blok yang membuat volume fisik dan logis, serta letak fisik mereka. Juga mungkin untuk memindahkan blok di PV, misalnya untuk menala halus kinerja atau, dalam cara yang lebih biasa, untuk membebaskan PV ketika kita perlu mengekstrak disk fisik yang sesuai dari VG (baik untuk mempengaruhi bagi VG lain atau menghapusnya dari LVM sama sekali). Halaman manual yang menguraikan perintah umumnya jelas dan rinci. Titik awal yang baik adalah halaman manual <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry>."

msgid "RAID or LVM?"
msgstr "RAID atau LVM?"

msgid "RAID and LVM both bring indisputable advantages as soon as one leaves the simple case of a desktop computer with a single hard disk where the usage pattern doesn't change over time. However, RAID and LVM go in two different directions, with diverging goals, and it is legitimate to wonder which one should be adopted. The most appropriate answer will of course depend on current and foreseeable requirements."
msgstr "RAID dan LVM keduanya membawa keuntungan tak terbantahkan bila kita abaikan kasus sederhana komputer desktop dengan satu hard disk dengan pola penggunaan tidak berubah dari waktu ke waktu. Namun, RAID dan LVM mengambil arah yang berbeda, dengan tujuan divergen, dan sah-sah saja bertanya-tanya mana yang harus diambil. Jawaban paling tepat akan tentu saja tergantung pada kebutuhan saat ini dan masa mendatang."

msgid "There are a few simple cases where the question doesn't really arise. If the requirement is to safeguard data against hardware failures, then obviously RAID will be set up on a redundant array of disks, since LVM doesn't really address this problem. If, on the other hand, the need is for a flexible storage scheme where the volumes are made independent of the physical layout of the disks, RAID doesn't help much and LVM will be the natural choice."
msgstr "Ada beberapa kasus sederhana dimana pertanyaan tidak benar-benar muncul. Jika kebutuhan adalah untuk mengamankan data terhadap kegagalan perangkat keras, maka jelas RAID akan disiapkan pada array disk, karena LVM tidak benar-benar menjawab masalah ini. Si sisi lain, jika kebutuhan adalah untuk skema penyimpanan yang fleksibel dimana volume dibuat independen terhadap tata letak fisik dari disk, RAID tidak banyak membantu dan LVM akan menjadi pilihan yang tepat."

msgid "<emphasis>NOTE</emphasis> If performance matters…"
msgstr "<emphasis>CATATAN</emphasis> Jika kinerja penting…"

msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgstr "Jika kecepatan masukan/keluar adalah esensinya, terutama dalam hal waktu akses, menggunakan LVM dan/atau RAID di salah satu dari banyak kombinasi mungkin memiliki dampak pada kinerja, dan ini mungkin mempengaruhi keputusan untuk memilih yang mana. Namun, perbedaan-perbedaan dalam kinerja benar-benar kecil, dan hanya terukur dalam beberapa kasus penggunaan. Jika kinerja penting, keuntungan terbaik yang diperoleh adalah menggunakan media penyimpanan bukan rotasi (<indexterm><primary>SSD</primary></indexterm> <emphasis>solid-state drive</emphasis>); biaya per megabyte mereka lebih tinggi daripada hard disk drive standar, dan kapasitas mereka biasanya lebih kecil, tapi mereka memberikan kinerja yang sangat baik untuk akses acak. Jika pola penggunaan mencakup banyak operasi keluaran/masukan yang terpencar di seluruh sistem berkas, misalnya untuk basis data tempat query-query yang kompleks rutin dijalankan, maka keuntungan dari menjalankan mereka pada SSD jauh lebih besar daripada apa pun yang bisa diperoleh dengan memilih LVM atas RAID atau sebaliknya. Dalam situasi ini, pilihan harus ditentukan oleh pertimbangan selain murni kecepatan, karena aspek kinerja paling mudah ditangani dengan menggunakan SSD."

msgid "The third notable use case is when one just wants to aggregate two disks into one volume, either for performance reasons or to have a single filesystem that is larger than any of the available disks. This case can be addressed both by a RAID-0 (or even linear-RAID) and by an LVM volume. When in this situation, and barring extra constraints (for instance, keeping in line with the rest of the computers if they only use RAID), the configuration of choice will often be LVM. The initial set up is barely more complex, and that slight increase in complexity more than makes up for the extra flexibility that LVM brings if the requirements change or if new disks need to be added."
msgstr "Use case ketiga yang menarik adalah ketika seseorang hanya ingin mengagregat dua disk ke dalam satu volume, baik untuk alasan kinerja atau memiliki sistem berkas tunggal yang lebih besar daripada salah satu disk yang tersedia. Hal ini dapat dijawab oleh RAID 0 (atau bahkan linear-RAID) maupun dengan volume LVM. Dalam situasi ini, dan tanpa batasan tambahan kendala (misalnya, menjaga sejalan dengan sisa komputer jika mereka hanya menggunakan RAID), konfigurasi pilihan akan seringkali adalah LVM. Penyiapan awal hampir tidak lebih kompleks, dan bahwa sedikit peningkatan kompleksitas terbayar oleh fleksibilitas tambahan yang dibawah oleh LVM jika persyaratan berubah atau jika disk baru perlu ditambahkan."

msgid "Then of course, there is the really interesting use case, where the storage system needs to be made both resistant to hardware failure and flexible when it comes to volume allocation. Neither RAID nor LVM can address both requirements on their own; no matter, this is where we use both at the same time — or rather, one on top of the other. The scheme that has all but become a standard since RAID and LVM have reached maturity is to ensure data redundancy first by grouping disks in a small number of large RAID arrays, and to use these RAID arrays as LVM physical volumes; logical partitions will then be carved from these LVs for filesystems. The selling point of this setup is that when a disk fails, only a small number of RAID arrays will need to be reconstructed, thereby limiting the time spent by the administrator for recovery."
msgstr "Kemudian tentu saja, ada kasus penggunaan yang benar-benar menarik, dimana sistem penyimpanan perlu dibuat tahan terhadap kegagalan perangkat keras dan fleksibel tentang alokasi volume. RAID maupun LVM masing-masing dapat menjawab kedua persyaratan; ini adalah di mana kita menggunakan keduanya pada saat yang sama -- atau lebih tepatnya, satu di atas yang lain. Skema yang memiliki semua tapi belum menjadi standar karena RAID dan LVM telah mencapai kedewasaan untuk memastikan redundansi data pertama dengan pengelompokan disk dalam sejumlah kecil larik RAID besar, dan menggunakan larik RAID ini sebagai volume fisik LVM; partisi logis kemudian dapat ditoreh dari LV-LV ini untuk sistem berkas. Nilai jual konfigurasi ini adalah bahwa ketika sebuah disk gagal, hanya sejumlah kecil larik RAID yang perlu dibangun kembali, sehingga membatasi waktu yang dihabiskan oleh administrator untuk pemulihan."

msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited videoclips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgstr "Mari kita ambil contoh konkret: departemen hubungan masyarakat di Falcot Corp memerlukan sebuah workstation untuk penyuntingan video, tapi anggaran departemen tidak mengizinkan berinvestasi di perangkat keras kelas tinggi secara lengkap. Keputusan dibuat untuk mendukung perangkat keras yang khusus untuk sifat grafis pekerjaan (monitor dan kartu video), dan tetap dengan perangkat keras generik untuk penyimpanan. Namun, seperti sudah dikenal luas, video digital memiliki beberapa persyaratan khusus untuk penyimpanan: banyaknya data yang akan disimpan besar, dan kecepatan pembacaan dan penulisan data ini penting untuk keseluruhan kinerja sistem (lebih daripada waktu akses rata-rata, misalnya). Batasan-batasn ini perlu dipenuhi dengan perangkat keras generik, dalam kasus ini dua hard disk drive SATA 300 GB; data sistem juga harus dibuat tahan terhadap kegagalan perangkat keras, termasuk sebagian data pengguna. Klip video yang diedit memang harus aman, tetapi tidak perlu bergegas menyunting video yang tertunda, karena mereka masih berada pada kaset."

msgid "RAID-1 and LVM are combined to satisfy these constraints. The disks are attached to two different SATA controllers to optimize parallel access and reduce the risk of a simultaneous failure, and they therefore appear as <filename>sda</filename> and <filename>sdc</filename>. They are partitioned identically along the following scheme:"
msgstr "RAID-1 dan LVM digabungkan untuk memenuhi batasan-batasan ini. Disk dilekatkan pada dua pengendali SATA yang berbeda untuk mengoptimalkan akses paralel dan mengurangi risiko kegagalan simultan, dan karena itu mereka muncul sebagai <filename>sda</filename> dan <filename>sdc</filename>. Mereka dipartisi secara identik mengikut skema sebagai berikut:"

msgid ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"

msgid "The first partitions of both disks (about 1 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgstr "Partisi pertama dari kedua disk (sekitar 1 GB) dirakit menjadi volume RAID-1, <filename>md0</filename>. Cermin ini langsung digunakan untuk menyimpan sistem berkas root."

msgid "The <filename>sda2</filename> and <filename>sdc2</filename> partitions are used as swap partitions, providing a total 2 GB of swap space. With 1 GB of RAM, the workstation has a comfortable amount of available memory."
msgstr "Partisi <filename>sda2</filename> dan <filename>sdc2</filename> digunakan sebagai partisi swap, menyediakan total 2 GB ruang swap. Dengan RAM 1 GB, workstation memiliki sejumlah memori tersedia yang nyaman."

msgid "The <filename>sda5</filename> and <filename>sdc5</filename> partitions, as well as <filename>sda6</filename> and <filename>sdc6</filename>, are assembled into two new RAID-1 volumes of about 100 GB each, <filename>md1</filename> and <filename>md2</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 200 GB of safe space."
msgstr "<filename>sda5</filename> dan partisi <filename>sdc5</filename>, serta <filename>sda6</filename> dan <filename>sdc6</filename>, dirakit menjadi dua volume RAID-1 baru masing-masing sekitar 100 GB, <filename>md1</filename> dan <filename>md2 </filename>. Kedua cermin diinisialisasi sebagai volume fisik untuk LVM, dan ditugaskan ke grup volume <filename>vg_raid</filename>. Maka VG ini berisi ruang sekitar 200 GB yang aman."

msgid "The remaining partitions, <filename>sda7</filename> and <filename>sdc7</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 200 GB of space."
msgstr "Sisa partisi, <filename>sda7</filename> dan <filename>sdc7</filename>, langsung digunakan sebagai volume fisik, dan ditugaskan untuk VG lain yang disebut <filename>vg_bulk</filename>, yang karena itu menjadi ruang sekitar 200 GB."

msgid "Once the VGs are created, they can be partitioned in a very flexible way. One must keep in mind that LVs created in <filename>vg_raid</filename> will be preserved even if one of the disks fails, which will not be the case for LVs created in <filename>vg_bulk</filename>; on the other hand, the latter will be allocated in parallel on both disks, which allows higher read or write speeds for large files."
msgstr "Setelah VGs dibuat, mereka dapat dipartisi dengan cara yang sangat fleksibel. Harus tetap diingat bahwa LV yang dibuat dalam <filename>vg_raid</filename> akan dipertahankan bahkan jika salah satu disk gagal, yang tidak akan terjadi untuk LV dibuat di <filename>vg_bulk</filename>; di sisi lain, yang kedua akan dialokasikan secara paralel pada kedua disk, yang memungkinkan kecepatan baca atau tulis yang lebih tinggi untuk berkas-berkas besar."

msgid "We will therefore create the <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgstr "Karena itu kita akan membuat LV <filename>lv_var</filename>, dan <filename>lv_home</filename> pada <filename>vg_raid</filename>, untuk mewadahi sistem berkas yang cocok; LV besar lain, <filename>lv_movies</filename>, akan digunakan untuk mewadahi film-film versi definitif setelah penyuntingan. VG lain akan dibagi menjadi <filename>lv_rushes</filename>yang besar, untuk data langsung dari kamera video digital, dan <filename>lv_tmp</filename> untuk berkas-berkas sementara. Lokasi area kerja adalah pilihan yang kurang mudah untuk dibuat: sementara kinerja yang baik diperlukan untuk volume itu, apakah layak risiko kehilangan pekerjaan jika disk gagal selama sesi menyunting? Tergantung pada jawaban untuk pertanyaan itu, LV yang relevan akan dibuat pada satu VG atau yang lain."

msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications."
msgstr "Kita sekarang memiliki sebagian redundansi untuk data penting dan banyak fleksibilitas dalam bagaimana ruang yang tersedia dibagi atas berbagai aplikasi."

msgid "<emphasis>NOTE</emphasis> Why three RAID-1 volumes?"
msgstr "<emphasis>CATATAN</emphasis> Mengapa tiga volume RAID-1?"

msgid "We could have set up one RAID-1 volume only, to serve as a physical volume for <filename>vg_raid</filename>. Why create three of them, then?"
msgstr "Kita bisa saja menyiapkan hanya satu volume RAID-1, untuk melayani sebagai volume fisik bagi <filename>vg_raid</filename>. Lalu mengapa membuat tiga?"

msgid "The rationale for the first split (<filename>md0</filename> vs. the others) is about data safety: data written to both elements of a RAID-1 mirror are exactly the same, and it is therefore possible to bypass the RAID layer and mount one of the disks directly. In case of a kernel bug, for instance, or if the LVM metadata become corrupted, it is still possible to boot a minimal system to access critical data such as the layout of disks in the RAID and LVM volumes; the metadata can then be reconstructed and the files can be accessed again, so that the system can be brought back to its nominal state."
msgstr "Alasan untuk pemecahan pertama (<filename>md0</filename> vs yang lain) adalah tentang keamanan data: data yang ditulis ke kedua elemen dari cermin RAID 1 persis sama, dan karena itu mungkin untuk melewati lapisan RAID dan mengait salah satu disk secara langsung. Dalam kasus bug kernel, misalnya, atau jika metadata LVM menjadi rusak, itu masih mungkin untuk mem-boot sistem minimal untuk mengakses data penting seperti tata letak disk dalam RAID dan volume LVM; metadata dapat kemudian direkonstruksi dan berkas dapat diakses lagi, sehingga sistem dapat dibawa kembali ke keadaan nominal."

msgid "The rationale for the second split (<filename>md1</filename> vs. <filename>md2</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md2</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md2</filename> and integrate its components <filename>sda6</filename> and <filename>sdc6</filename> into the bulk VG (which grows by 200 GB instead of 100 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgstr "Alasan untuk pemecahan kedua (<filename>md1</filename> vs <filename>md2</filename>) kurang jelas, dan lebih berkaitan dengan mengakui bahwa masa depan itu tidak pasti. Ketika workstation pertama dirakit, persyaratan penyimpanan yang eksak tidak selalu diketahui dengan ketepatan yang sempurna; mereka juga dapat berkembang dari waktu ke waktu. Dalam kasus kita, kita tidak dapat mengetahui terlebih dahulu persyaratan ruang penyimpanan sebenarnya untuk video yang buru-buru dan klip video yang lengkap. Jika satu klip tertentu membutuhkan sejumlah yang sangat besar pekerjaan buru-buru, dan VG yang didedikasikan untuk data redundan masih terisi kurang dari setengah, kita dapat memakai kembali ruang yang tidak dibutuhkan. Kita dapat menghapus salah satu volume fisik, misalnya <filename>md2</filename>, dari <filename>vg_raid</filename> dan menambahkannya ke <filename>vg_bulk</filename> secara langsung (jika durasi operasi yang diharapkan cukup singkat sehingga kita dapat hidup dengan penurunan kinerja sementara), atau membatalkan setup RAID pada <filename>md2</filename> dan mengintegrasikan komponen <filename>sda6</filename> dan <filename>sdc6</filename> ke VG curah (yang tumbuh 200 GB, bukan 100 GB); volume logis <filename>lv_rushes</filename> dapat kemudian ditumbuhkan sesuai kebutuhan."

msgid "<primary>virtualization</primary>"
msgstr "<primary>virtualisasi</primary>"

msgid "Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security."
msgstr "Virtualisasi adalah salah satu kemajuan yang paling besar dalam beberapa tahun terakhir komputasi. Istilah ini mencakup berbagai abstraksi dan teknik simulasi komputer virtual dengan tingkat kebebasan yang variabel pada perangkat keras sebenarnya. Satu server fisik kemudian dapat mewadahi beberapa sistem yang bekerja pada waktu yang sama dan dalam isolasi. Ada banyak aplikasi, dan sering diturunkan dari isolasi ini: lingkungan uji dengan berbagai konfigurasi misalnya, atau pemisahan layanan kebeberapa mesin virtual untuk keamanan."

msgid "There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:"
msgstr "Ada beberapa solusi virtualisasi, masing-masing dengan pro dan kontra. Buku ini akan fokus pada Xen, LXC, dan KVM, tetapi implementasi penting lain adalah sebagai berikut:"

msgid "<primary><emphasis>VMWare</emphasis></primary>"
msgstr "<primary><emphasis>VMWare</emphasis></primary>"

msgid "<primary><emphasis>Bochs</emphasis></primary>"
msgstr "<primary><emphasis>Bochs</emphasis></primary>"

msgid "<primary><emphasis>QEMU</emphasis></primary>"
msgstr "<primary><emphasis>QEMU</emphasis></primary>"

msgid "<primary><emphasis>VirtualBox</emphasis></primary>"
msgstr "<primary><emphasis>VirtualBox</emphasis></primary>"

msgid "<primary><emphasis>KVM</emphasis></primary>"
msgstr "<primary><emphasis>KVM</emphasis></primary>"

msgid "<primary><emphasis>LXC</emphasis></primary>"
msgstr "<primary><emphasis>LXC</emphasis></primary>"

#, fuzzy
#| msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"
msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"https://www.qemu.org/\" />"
msgstr "QEMU adalah emulator perangkat lunak untuk sebuah komputer lengkap; kinerja jauh dari kecepatan yang bisa dicapai ketika berjalan secara native, tetapi ini memungkinkan menjalankan sistem operasi tanpa perubahan atau eksperimental pada perangkat keras yang diemulasi. Hal ini juga memungkinkan mengemulasi arsitektur perangkat keras yang berbeda: sebagai contoh, sistem <emphasis>amd64</emphasis> bisa mengemulasi komputer <emphasis>arm</emphasis>. QEMU adalah perangkat lunak bebas. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"

msgid "Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64)."
msgstr "Bochs adalah mesin virtual lain yang bebas, tapi itu hanya mengemulasi arsitektur x86 (i386 dan amd64)."

#, fuzzy
#| msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"
msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"https://www.vmware.com/\" />"
msgstr "VMWare adalah sebuah mesin virtual yang proprietari; salah satu yang tertua di luar sana, itu juga satu dari yang paling dikenal. Itu bekerja dengan prinsip yang serupa dengan QEMU. VMWare menyediakan fitur-fitur tingkat lanjut seperti membuat snapshot dari sebuah mesin virtual yang sedang berjalan. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"

msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type=\"block\" url=\"https://www.virtualbox.org/\" />"
msgstr "VirtualBox adalah sebuah mesin virtual yang sebagian besar perangkat lunak bebas (beberapa komponen ekstra tersedia di bawah lisensi proprietari). Sayang sekali itu ada di bagian \"contrib\" Debian karena menyertakan beberapa berkas terprakompilasi yang tidak dapat dibangun ulang tanpa suatu kompiler proprietari dan saat ini hanya ada di Debian Unstable karena kebijakan Oracle membuatnya tidak mungkin menjaganya tetap aman dalam suatu rilis stable Debian (lihat <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). Walaupun lebih muda daripada VMWare dan terbatas pada arsitektur i386 dan amd64, itu masih punya snapshot dan beberapa fitur menarik lainnya. <ulink type=\"block\" url=\"https://www.virtualbox.org/\" />"

#, fuzzy
#| msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgid "<emphasis>HARDWARE</emphasis> Virtualization support"
msgstr "<emphasis>LEBIH JAUH</emphasis> Virtualisasi masal"

msgid "Some computers might not have hardware virtualization support; when they do, it should be enabled in the BIOS."
msgstr ""

msgid "To know if you have virtualization support enabled, you can check if the relevant flag is enabled with <command>grep</command>. If the following command for your processor returns some text, you already have virtualization support enabled:"
msgstr ""

msgid "For Intel processors you can execute <command>grep vmx /proc/cpuinfo</command>"
msgstr ""

msgid "For AMD processors you can execute <command>grep svm /proc/cpuinfo</command>"
msgstr ""

msgid "Xen <indexterm><primary>Xen</primary></indexterm> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen."
msgstr "Xen <indexterm><primary>Xen</primary></indexterm> adalah sebuah solusi \"paravirtualization\". Ini memperkenalkan lapisan abstraksi tipis, dinamai \"hypervisor\", antara perangkat keras dan sistem di atas; ini bekerja sebagai wasit yang mengendalikan akses ke perangkat keras dari mesin-mesin virtual. Namun, itu hanya menangani beberapa instruksi, sisanya dieksekusi secara langsung oleh perangkat keras atas nama sistem. Keuntungan utama adalah kinerja tidak menurun, dan sistem berjalan mendekati kecepatan native; kekurangannya adalah kernal dari sistem operasi yang ingin dipakai pada suatu hypervisor Xen perlu diadaptasi untuk berjalan pada Xen."

#, fuzzy
#| msgid "Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgid "Let's spend some time on terms. The hypervisor is the lowest layer, which runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgstr "Mari kita bahas sejenak istilah-istilah. Hypervisor adalah lapisan terendah, yang berjalan langsung pada perangkat keras, bahkan di bawah kernel. Hypervisor ini dapat membagi sisa perangkat lunak di beberapa <emphasis>domain</emphasis>, yang dapat dilihat sebagai banyak mesin virtual. Salah satu domain (yang pertama dimulai) dikenal sebagai <emphasis>dom0</emphasis>, dan memiliki peran khusus, karena hanya domain ini dapat mengontrol hypervisor dan eksekusi domain lainnya. Domain lain tersebut dikenal sebagai <emphasis>domU</emphasis>. Dengan kata lain, dan dari sudut pandang pengguna, <emphasis>dom0</emphasis> adalah \"host\" sistem virtualisasi lain, sementara <emphasis>domU</emphasis> dapat dilihat sebagai \"tamu\"."

msgid "<emphasis>CULTURE</emphasis> Xen and the various versions of Linux"
msgstr "<emphasis>KULTUR</emphasis> Xen dan berbagai versi Linux"

msgid "Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <emphasis>paravirt_ops</emphasis> or <emphasis>pv_ops</emphasis> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially."
msgstr "Xen ini awalnya dikembangkan sebagai seperangkat patch yang berada di luar pohon resmi, dan tidak terintegrasi ke kernel Linux. Pada saat yang sama, beberapa sistem virtualisasi mendatang (termasuk KVM) memerlukan beberapa fungsi generik terkait virtualisasi untuk memfasilitasi integrasi mereka, dan kernel Linux memperoleh set fungsi (dikenal sebagai antarmuka <emphasis>paravirt_ops </emphasis> atau <emphasis>pv_ops</emphasis>). Karena patch Xen menduplikasi beberapa fungsionalitas antar muka ini, mereka tidak bisa diterima secara resmi."

#, fuzzy
#| msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgstr "XenSource, perusahaan di belakang Xen, karena itu harus mem-port Xen ke kerangka baru ini, sehingga patch Xen bisa digabungkan ke kernel Linux resmi. Itu berarti banyak penulisan ulang kode, dan meskipun Xensource segera memiliki versi yang bekerja berdasarkan antarmuka paravirt_ops, patch hanya digabungkan ke dalam kernel resmi secara progresif. Penggabungan selesai pada Linux 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"

#, fuzzy
#| msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgstr "Karena <emphasis role=\"distribution\">Jessie</emphasis> didasarkan pada kernel Linux versi 3.16, paket-paket standar <emphasis role=\"pkg\">linux-image-686-pae</emphasis> dan <emphasis role=\"pkg\">linux-image-amd64</emphasis> menyertakan kode yang diperlukan, dan patch spesifik distribusi yang diperlukan untuk <emphasis role=\"distribution\">Squeeze</emphasis> dan versi sebelumnya dari Debian tidak diperlukan lagi. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"

msgid "<emphasis>CULTURE</emphasis> Xen and non-Linux kernels"
msgstr "<emphasis>BUDAYA</emphasis> Xen dan kernel bukan Linux"

#, fuzzy
#| msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgstr "Xen memerlukan modifikasi ke semua sistem operasi yang ingin berjalan di atasnya; tidak semua kernel memiliki tingkat kedewasaan yang sama dalam hal ini. Banyak yang fungsional-penuh, baik sebagai dom0 maupun domU: Linux 3.0 dan setelahnya, NetBSD 4.0 dan setelahnya, dan OpenSolaris. Yang lain hanya bekerja sebagai domU. Anda dapat memeriksa status dari setiap sistem operasi di wiki Xen: <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"

msgid "However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows)."
msgstr "Namun, jika Xen dapat bergantung pada fungsi perangkat keras yang didedikasikan untuk virtualisasi (yang hanya hadir dalam prosesor yang lebih baru), bahkan sistem operasi tanpa modifikasi dapat berjalan sebagai domU (termasuk Windows)."

msgid "<emphasis>NOTE</emphasis> Architectures compatible with Xen"
msgstr "<emphasis>CATATAN</emphasis> Arsitektur yang kompatibel dengan Xen"

msgid "Xen is currently only available for the i386, amd64, arm64 and armhf architectures."
msgstr "Xen saat ini hanya tersedia untuk arsitektur i386, amd64, arm64, dan armhf."

msgid "Using Xen under Debian requires three components:"
msgstr "Menggunakan Xen di bawah Debian memerlukan tiga komponen:"

#, fuzzy
#| msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>."
msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.11-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.11-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.11-arm64</emphasis>."
msgstr "Hypervisor itu sendiri. Tergantung dari perangkat keras yang tersedia, paket yang tepat adalah <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis>, atau <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>."

#, fuzzy
#| msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <emphasis role=\"distribution\">Jessie</emphasis>."
msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 4.19 version present in <emphasis role=\"distribution\">Buster</emphasis>."
msgstr "Sebuah kernel yang berjalan pada hypervisor itu. Setiap kernel yang lebih baru daripada 3.0 bisa, termasuk versi 3.16 yang ada dalam <emphasis role=\"distribution\">Jessie</emphasis>."

msgid "The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <emphasis role=\"pkg\">libc6-xen</emphasis> package."
msgstr "Arsitektur i386 juga memerlukan pustaka standar dengan patch yang sesuai yang mengambil keuntungan dari Xen; ini ada dalam paket <emphasis role=\"pkg\">libc6-xen</emphasis>."

msgid "The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.11</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the GRUB bootloader menu, so as to start the chosen kernel in a Xen dom0. Note, however, that this entry is not usually set to be the first one in the list, but it will be selected by default."
msgstr "Hypervisor juga membawa <emphasis role=\"pkg\">xen-utils-4.11</emphasis>, yang berisi alat untuk mengontrol hypervisor dari dom0. Hal ini pada gilirannya membawa pustaka standar yang sesuai. Selama instalasi dari semua itu, skrip konfigurasi juga membuat entri baru dalam menu bootloader GRUB, untuk memulai kernel pilihan di dom0 Xen. Namun perlu dicatat bahwa entri ini tidak biasanya diatur untuk menjadi yang pertama dalam daftar, tapi akan dipilih secara default."

msgid "Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps."
msgstr "Setelah prapersyaratan ini diinstal, langkah berikutnya adalah untuk menguji perilaku dom0 sendiri; ini melibatkan reboot hypervisor dan kernel Xen. Sistem harus boot dalam cara standar, dengan beberapa tambahan pesan pada konsol selama langkah inisialisasi awal."

msgid "Now is the time to actually install useful systems on the domU systems, using the tools from <emphasis role=\"pkg\">xen-tools</emphasis>. This package provides the <command>xen-create-image</command> command, which largely automates the task. The only mandatory parameter is <literal>--hostname</literal>, giving a name to the domU; other options are important, but they can be stored in the <filename>/etc/xen-tools/xen-tools.conf</filename> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <command>xen-create-image</command> invocation. Important parameters of note include the following:"
msgstr "Sekarang adalah waktu untuk benar-benar menginstal sistem yang berguna pada sistem domU, menggunakan alat-alat dari <emphasis role=\"pkg\">xen-tools</emphasis>. Paket ini menyediakan perintah <command>xen-create-image</command>, yang mengotomatiskan sebagian besar tugas. Satu-satunya parameter wajib adalah <literal>--hostname</literal>, yang memberikan nama kepada domU; pilihan lainnya penting, tetapi mereka dapat disimpan dalam berkas konfigurasi <filename>/etc/xen-tools/xen-tools.conf</filename> dan ketidakhadiran mereka dari baris perintah tidak memicu kesalahan. Karena itu penting untuk memeriksa isi dari berkas ini sebelum membuat image, atau menggunakan parameter tambahan dalam pemanggilan <command>xen-create-image</command>. Parameter yang penting untuk diperhatikan adalah sebagai berikut:"

msgid "<literal>--memory</literal>, to specify the amount of RAM dedicated to the newly created system;"
msgstr "<literal>--memory</literal>, untuk menentukan banyaknya RAM yang didedikasikan bagi sistem yang baru dibuat;"

msgid "<literal>--size</literal> and <literal>--swap</literal>, to define the size of the “virtual disks” available to the domU;"
msgstr "<literal>--size</literal> dan <literal>--swap</literal>, untuk menentukan ukuran \"disk virtual\" yang tersedia bagi domU;"

#, fuzzy
#| msgid "<literal>--debootstrap</literal>, to cause the new system to be installed with <command>debootstrap</command>; in that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">jessie</emphasis>)."
msgid "<literal>--debootstrap-cmd</literal>, to specify the which debootstrap command is used. The default is <command>debootstrap</command> if debootstrap and cdebootstrap are installed. In that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">buster</emphasis>)."
msgstr "<literal>--debootstrap</literal>, menyebabkan sistem yang baru dipasangi <command>debootstrap</command>; dalam hal ini, opsi <literal>--dist</literal> akan juga seringkali digunakan (dengan nama distribusi seperti misalnya <emphasis role=\"distribution\">jessie</emphasis>)."

msgid "<emphasis>GOING FURTHER</emphasis> Installing a non-Debian system in a domU"
msgstr "<emphasis>LEBIH JAUH</emphasis> Memasang sistem bukan-Debian di domU"

msgid "In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <literal>--kernel</literal> option."
msgstr "Dalam hal non-Linux sistem, perlu hati-hati untuk mendefinisikan kernel yang mesti dipakai oleh domU, menggunakan opsi <literal>--kernel</literal>."

msgid "<literal>--dhcp</literal> states that the domU's network configuration should be obtained by DHCP while <literal>--ip</literal> allows defining a static IP address."
msgstr "<literal>--dhcp</literal> menyatakan bahwa konfigurasi jaringan domU harus diperoleh dengan DHCP sedangkan <literal>--ip</literal> memungkinkan menentukan alamat IP statis."

msgid "Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <literal>--dir</literal> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <literal>--lvm</literal> option, followed by the name of a volume group; <command>xen-create-image</command> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive."
msgstr "Terakhir, metode penyimpanan harus dipilih untuk image yang akan dibuat (yang akan dipandang sebagai hard disk drive dari domU). Metode paling sederhana, sesuai dengan pilihan <literal>--dir</literal>, adalah untuk menciptakan satu berkas pada dom0 untuk setiap perangkat yang harus disediakan oleh domU. Untuk sistem yang menggunakan LVM, alternatifnya adalah dengan menggunakan pilihan <literal>--lvm</literal>, diikuti oleh nama grup volume; <command>xen-create-image</command> kemudian akan menciptakan volume logis baru di dalam grup, dan volume logis ini akan dibuat tersedia bagi domU sebagai hard disk drive."

msgid "<emphasis>NOTE</emphasis> Storage in the domU"
msgstr "<emphasis>CATATAN</emphasis> Penyimpanan di domU"

msgid "Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <command>xen-create-image</command>, however, so editing the Xen image's configuration file is in order after its initial creation with <command>xen-create-image</command>."
msgstr "Seluruh hard disk dapat juga diekspor ke domU, maupun partisi, larik RAID, atau volume logis LVM yang sudah ada sebelumnya. Namun operasi ini tidak diotomatiskan oleh <command>xen-create-image</command>, jadi perlu menyunting berkas konfigurasi image Xen setelah penciptaan awal dengan <command>xen-create-image</command>."

msgid "Once these choices are made, we can create the image for our future Xen domU:"
msgstr "Setelah pilihan ini dibuat, kita dapat membuat image untuk domU Xen kita nanti:"

msgid ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=buster --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  buster\n"
"Mirror         :  http://deb.debian.org/debian\n"
"Partitions     :  swap            512M  (swap)\n"
"                  /               2G    (ext4)\n"
"Image type     :  sparse\n"
"Memory size    :  256M\n"
"Kernel path    :  /boot/vmlinuz-4.19.0-5-amd64\n"
"Initrd path    :  /boot/initrd.img-4.19.0-5-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  buster\n"
"MAC Address     :  00:16:3E:0C:74:2F\n"
"IP Address(es)  :  dynamic\n"
"SSH Fingerprint :  SHA256:PuAGX4/4S07Xzh1u0Cl2tL04EL5udf9ajvvbufBrfvU (DSA)\n"
"SSH Fingerprint :  SHA256:ajFTX54eakzolyzmZku/ihq/BK6KYsz5MewJ98BM5co (ECDSA)\n"
"SSH Fingerprint :  SHA256:/sFov86b+rD/bRSJoHKbiMqzGFiwgZulEwpzsiw6aSc (ED25519)\n"
"SSH Fingerprint :  SHA256:/NJg/CcoVj+OLE/cL3yyJINStnla7YkHKe3/xEdVGqc (RSA)\n"
"Root Password   :  EwmQMHtywY9zsRBpqQuxZTb\n"
"</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=buster --role=udev</userinput>\n<computeroutput>\n[...]\nGeneral Information\n--------------------\nHostname       :  testxen\nDistribution   :  buster\nMirror         :  http://deb.debian.org/debian\nPartitions     :  swap            512M  (swap)\n                  /               2G    (ext4)\nImage type     :  sparse\nMemory size    :  256M\nKernel path    :  /boot/vmlinuz-4.19.0-5-amd64\nInitrd path    :  /boot/initrd.img-4.19.0-5-amd64\n[...]\nLogfile produced at:\n         /var/log/xen-tools/testxen.log\n\nInstallation Summary\n---------------------\nHostname        :  testxen\nDistribution    :  buster\nMAC Address     :  00:16:3E:0C:74:2F\nIP Address(es)  :  dynamic\nSSH Fingerprint :  SHA256:PuAGX4/4S07Xzh1u0Cl2tL04EL5udf9ajvvbufBrfvU (DSA)\nSSH Fingerprint :  SHA256:ajFTX54eakzolyzmZku/ihq/BK6KYsz5MewJ98BM5co (ECDSA)\nSSH Fingerprint :  SHA256:/sFov86b+rD/bRSJoHKbiMqzGFiwgZulEwpzsiw6aSc (ED25519)\nSSH Fingerprint :  SHA256:/NJg/CcoVj+OLE/cL3yyJINStnla7YkHKe3/xEdVGqc (RSA)\nRoot Password   :  EwmQMHtywY9zsRBpqQuxZTb\n</computeroutput>"

msgid "We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters."
msgstr "Kita sekarang memiliki mesin virtual, tetapi saat ini tidak berjalan (dan karena itu hanya menggunakan ruang hard disk dom0). Tentu saja, kita dapat membuat lebih banyak image, mungkin dengan parameter yang berbeda."

#, fuzzy
#| msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgstr "Sebelum menyalakan mesin virtual ini, kita perlu menentukan bagaimana mereka akan diakses. Mereka tentu saja dapat dianggap mesin terisolasi, hanya diakses melalui konsol sistem mereka, tapi ini jarang cocok dengan pola penggunaan. Sebagian besar waktu, domU akan dianggap sebagai server jarak jauh, dan diakses hanya melalui jaringan. Namun, itu akan kurang nyaman untuk menambahkan kartu jaringan bagi setiap domU; itulah sebabnya Xen memungkinkan menciptakan antarmuka virtual, yang bisa dilihat dan digunakan dengan cara yang standar oleh setiap domain. Perhatikan bahwa kartu ini, meskipun mereka virtual, akan hanya menjadi berguna setelah terhubung ke jaringan, bahkan yang virtual. Xen memiliki beberapa model jaringan untuk itu:"

msgid "The simplest model is the <emphasis>bridge</emphasis> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch."
msgstr "Model yang paling sederhana adalah model <emphasis>bridge</emphasis>; semua kartu jaringan eth0 (baik dalam dom0 dan sistem domU) bersikap seolah-olah mereka secara langsung terhubung ke switch Ethernet."

msgid "Then comes the <emphasis>routing</emphasis> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network."
msgstr "Kemudian ada model <emphasis>routing</emphasis>, dimana dom0 berperilaku sebagai router yang berdiri di antara sistem domU dan jaringan eksternal (fisik)."

msgid "Finally, in the <emphasis>NAT</emphasis> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0."
msgstr "Akhirnya, dalam model <emphasis>NAT</emphasis>, dom0 lagi-lagi berada di antara sistem domU dan sisa jaringan, tetapi sistem domU tidak langsung dapat diakses dari luar, dan lalu lintas berjalan melalui perjemahan alamat jaringan pada dom0."

msgid "These three networking nodes involve a number of interfaces with unusual names, such as <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> and <filename>xenbr0</filename>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model."
msgstr "Ketiga simpul jaringan ini melibatkan sejumlah antarmuka dengan nama-nama yang tidak biasa, seperti <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename>, dan <filename>xenbr0</filename>. Hypervisor Xen mengatur mereka sesuai tata letak yang telah didefinisikan, di bawah kontrol perkakas pengguna. Karena NAT dan model routing hanya disesuaikan dengan kasus-kasus tertentu, kami hanya akan membahas model bridge."

#, fuzzy
#| msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.4</emphasis> package recommends it) to replace the existing eth0 entry:"
msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.11</emphasis> package recommends it) to replace the existing eth0 entry:"
msgstr "Konfigurasi standar dari paket Xen tidak mengubah konfigurasi sistem jaringan. Namun, daemon <command>xend</command> dikonfigurasi untuk mengintegrasikan jaringan antarmuka virtual ke jaringan bridge apapun yang sudah ada sebelumnya (dengan <filename>xenbr0</filename> didahulukan jika ada beberapa bridge). Kami karena itu harus menyiapkan sebuah bridge di <filename>/etc/network/interfaces</filename> (yang memerlukan instalasi paket <emphasis role=\"pkg\">bridge-utils</emphasis>, itulah sebabnya paket <emphasis role=\"pkg\">xen-utils-4.4</emphasis> merekomendasikan itu) untuk menggantikan entri eth0 yang sudah ada:"

msgid ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

#, fuzzy
#| msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them."
msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them. You might need to increase the default memory by editing the variable memory from configuration file (in this case, <filename>/etc/xen/testxen.cfg</filename>). Here we have set it to 1024 (megabytes)."
msgstr "Setelah reboot untuk memastikan bridge secara otomatis dibuat, kita sekarang dapat memulai domU dengan perkakas kendali Xen, khususnya perintah <command>xl</command>. Perintah ini memungkinkan manipulasi yang berbeda pada domain, termasuk menampilkan daftar mereka dan, memulai/menghentikan mereka."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   463     1     r-----      9.8\n"
#| "# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
#| "<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
#| "# </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   366     1     r-----     11.4\n"
#| "testxen                                      1   128     1     -b----      1.1</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  1894     2     r-----      63.5\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  1505     2     r-----     100.0\n"
"testxen                                     13  1024     0     --p---       0.0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   463     1     r-----      9.8\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   366     1     r-----     11.4\n"
"testxen                                      1   128     1     -b----      1.1</computeroutput>"

msgid "<emphasis>TOOL</emphasis> Choice of toolstacks to manage Xen VM"
msgstr "<emphasis>PERKAKAS</emphasis> Pilihan kumpulan perkakas untuk mengelola VM Xen"

msgid "<primary><command>xm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "<primary><command>xe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of libvirt and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgstr "Pada Debian 7 dan rilis yang lebih tua, <command>xm</command> adalah perkakas baris perintah acuan yang digunakan untuk mengelola mesin virtual Xen. Itu sekarang telah digantikan oleh <command>xl</command> yang umumnya kompatibel mundur. Tetapi bukan hanya mereka alat yang tersedia: <command>virsh</command> dari libvirt dan <command>xe</command> dari XenServer XAPI (produk komersial Xen) adalah alat-alat alternatif."

msgid "<emphasis>CAUTION</emphasis> Only one domU per image!"
msgstr "<emphasis>HATI-HATI</emphasis> Hanya satu domU per image!"

#, fuzzy
#| msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is, however, quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgstr "Meskipun tentu mungkin untuk memiliki beberapa sistem domU yang berjalan secara paralel, mereka semua perlu untuk menggunakan image mereka sendiri, karena setiap domU dibuat percaya ini berjalan pada perangkat keras sendiri (terlepas dari sepotong kecil kernel yang berbicara kepada hypervisor). Secara khusus, tidak mungkin bagi dua sistem domU berjalan bersamaan untuk berbagi ruang penyimpanan. Jika sistem domU tidak berjalan pada saat yang sama, sangat mungkin untuk menggunakan kembali satu partisi swap, atau partisi yang mewadahi sistem berkas <filename>/home</filename>."

msgid "Note that the <filename>testxen</filename> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly."
msgstr "Perhatikan bahwa domU <filename>testxen</filename> menggunakan memori nyata yang diambil dari RAM yang bila tidak demikian, tidak akan tersedia bagi dom0, bukan memori yang tersimulasi. Karena itu perlu hati-hati ketika membangun sebuah server yang dimaksudkan untuk mewadahi instansi Xen, untuk menyediakan RAM fisik yang sesuai."

msgid "Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <filename>hvc0</filename> console, with the <command>xl console</command> command:"
msgstr "Voilà! Mesin virtual kami memulai. Kita dapat mengaksesnya dengan satu dari dua mode. Cara yang biasa adalah untuk menyambung \"jarak jauh\" melalui jaringan, seperti kita akan menyambung ke mesin nyata; ini biasanya akan memerlukan mendirikan penyiapan server DHCP atau beberapa konfigurasi DNS. Cara lain, yang mungkin satu-satunya cara jika konfigurasi jaringan salah, adalah dengan menggunakan konsol <filename>hvc0</filename>, dengan perintah <command>xl console</command>:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
#| "<computeroutput>[...]\n"
#| "\n"
#| "Debian GNU/Linux 8 testxen hvc0\n"
#| "\n"
#| "testxen login: </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 10 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 8 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"

msgid "One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> key combination."
msgstr "Kita kemudian dapat membuka sesi, seperti yang orang akan lakukan jika duduk di papan ketik mesin virtual. Melepaskan dari konsol ini dicapai melalui kombinasi tombol <keycombo action=\"simul\"> <keycap>Control</keycap> <keycap>]</keycap></keycombo>."

msgid "<emphasis>TIP</emphasis> Getting the console straight away"
msgstr "<emphasis>TIPS</emphasis> Mendapatkan konsol langsung"

msgid "Sometimes one wishes to start a domU system and get to its console straight away; this is why the <command>xl create</command> command takes a <literal>-c</literal> switch. Starting a domU with this switch will display all the messages as the system boots."
msgstr "Kadang-kadang kita berharap untuk memulai sistem domU dan langsun mendapatkan konsol; inilah sebabnya mengapa perintah <command>xl create</command> menerima opsi <literal>-c</literal>. Memulai domU dengan switch ini akan menampilkan semua pesan ketika sistem boot."

msgid "<emphasis>TOOL</emphasis> OpenXenManager"
msgstr "<emphasis>ALAT</emphasis> OpenXenManager"

msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <command>xl</command> command."
msgstr "OpenXenManager (dalam paket <emphasis role=\"pkg\"> openxenmanager</emphasis>) adalah antarmuka grafis yang memungkinkan manajemen domain Xen jarak jauh melalui API Xen. Itu dapat mengontrol domain Xen dari jauh. Itu menyediakan sebagian besar fitur dari perintah <command>xl</command>."

msgid "Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <command>xl pause</command> and <command>xl unpause</command> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <command>xl save</command> and <command>xl restore</command> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system."
msgstr "Setelah domU hidup, itu dapat digunakan seperti server lain (karena itu adalah sistem GNU/Linux juga). Namun, status mesin virtual memungkinkan beberapa fitur tambahan. Sebagai contoh, domU dapat diistirahatkan sementara kemudian dilanjutkan kembali, dengan perintah <command>xl pause</command> dan <command>xl unpause</command>. Perhatikan bahwa meskipun domU yang diistirahatkan tidak menggunakan prosesor apapun, memori yang dialokasikan masih digunakan. Mungkin menarik untuk mempertimbangkan perintah <command>xl save</command> dan <command>xl restore</command>: menyimpan domU membebaskan sumber daya yang sebelumnya digunakan oleh domU ini, termasuk RAM. Ketika dipulihkan (atau dilanjutkan kembali), domU bahkan tidak melihat apapun selain berlalunya waktu. Jika domU sedang berjalan ketika dom0 dimatikan, skrip yang dikemas secara otomatis menyimpan domU, dan memulihkannya pada boot berikutnya. Ini tentu saja akan melibatkan ketidaknyamanan standar yang timbul ketika menhibernasi komputer laptop misalnya; khususnya, jika domU disuspensi terlalu lama, koneksi jaringan mungkin berakhir. Perhatikan juga bahwa Xen sejauh ini tidak kompatibel dengan sebagian besar manajemen daya ACPI, yang menghalangi mensuspensi sistem host (dom0)."

msgid "<emphasis>DOCUMENTATION</emphasis> <command>xl</command> options"
msgstr "<emphasis>DOKUMENTASI</emphasis> Opsi-opsi <command>xl</command>"

msgid "Most of the <command>xl</command> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> manual page."
msgstr "Sebagian besar sub perintah <command>xl</command> mengharapkan satu atau lebih argumen, sering kali nama domU. Argumen ini dijelaskan dengan baik dalam halaman manual <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry>."

msgid "Halting or rebooting a domU can be done either from within the domU (with the <command>shutdown</command> command) or from the dom0, with <command>xl shutdown</command> or <command>xl reboot</command>."
msgstr "Menghentikan atau reboot domU dapat dilakukan baik dari dalam domU (dengan perintah <command>shutdown</command>) atau dari dom0, dengan <command>xl shutdown</command> atau <command>xl reboot</command>."

msgid "<emphasis>GOING FURTHER</emphasis> Advanced Xen"
msgstr "<emphasis>LEBIH JAUH</emphasis> Xen tingkat lanjut"

#, fuzzy
#| msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"
msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"https://xenproject.org/help/documentation/\" />"
msgstr "Xen memiliki lebih banyak fitur daripada yang dapat kita gambarkan dalam beberapa paragraf ini. Secara khusus, sistem ini sangat dinamis, dan banyak parameter untuk satu domain (seperti banyaknya memori yang dialokasikan, hard drive yang terlihat, perilaku penjadwal tugas, dan sebagainya) dapat disesuaikan bahkan ketika domain tersebut sedang berjalan. DomU bahkan dapat dimigrasi lintas server tanpa dimatikan, dan tanpa kehilangan koneksi jaringannya! Untuk semua aspek canggih ini, sumber utama informasi adalah dokumentasi Xen resmi. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"

msgid "<primary>LXC</primary>"
msgstr "<primary>LXC</primary>"

msgid "Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <emphasis>control groups</emphasis>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system."
msgstr "Meskipun hal ini digunakan untuk membangun \"mesin virtual\", LXC bukanlah, secara tegas, suatu sistem virtualisasi, tetapi sebuah sistem untuk mengisolasi kelompok proses dari satu sama lain meskipun mereka semua berjalan pada host yang sama. Ini memanfaatkan evolusi baru-baru ini pada kernel Linux, yang secara kolektif dikenal sebagai <emphasis>grup kendali</emphasis>, dimana set proses yang disebut \"grup\" yang berbeda memiliki pandangan yang berbeda atas aspek-aspek tertentu dari sistem secara keseluruhan. Paling menonjol di antara aspek-aspek ini adalah pengidentifikasi proses, konfigurasi jaringan, dan titik kait. Sebuah kelompok proses terisolasi tidak akan memiliki akses ke proses lain dalam sistem, dan aksesnya ke sistem berkas dapat dibatasi ke subset spesifik. Itu dapat juga memiliki antarmuka jaringan dan tabel routing sendiri, dan mungkin dikonfigurasi untuk hanya melihat subset dari perangkat yang tersedia yang ada pada sistem."

#, fuzzy
#| msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there is no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgstr "Fitur ini dapat dikombinasikan untuk mengisolasi keluarga seluruh proses yang dimulai dari proses <command>init</command>, dan kumpulan yang dihasilkan terlihat sangat mirip dengan mesin virtual. Nama resmi untuk penyiapan seperti itu adalah \"container\" (maka moniker LXC: <emphasis>LinuX Containers</emphasis>), tapi perbedaan yang cukup penting dengan mesin virtual \"nyata\" seperti yang disediakan oleh Xen atau KVM adalah bahwa tidak ada kernel kedua; container menggunakan kernel yang sama dengan sistem host. Ini memiliki pro dan kontra: keuntungannya termasuk kinerja yang sangat baik karena total ketiadaan overhead, dan fakta bahwa kernel memiliki visi global dari semua proses yang berjalan pada sistem, sehingga penjadwalan dapat menjadi lebih efisien daripada jika dua kernel independen yang menjadwalkan set tugas yang berbeda. Paling utama di antara ketidaknyamanan adalah ketidakmungkinan untuk menjalankan sebuah kernel yang berbeda dalam container (apakah versi Linux yang berbeda atau sistem operasi yang berbeda sama sekali)."

msgid "<emphasis>NOTE</emphasis> LXC isolation limits"
msgstr "<emphasis>CATATAN</emphasis> Batas isolasi LXC"

msgid "LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:"
msgstr "Container LXC tidak memberikan tingkat isolasi yang dicapai oleh emulator atau virtualizers yang lebih berat. Khususnya:"

msgid "since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;"
msgstr "karena kernel dipakai bersama antara sistem host dan container, proses yang dibatasi ke container masih dapat mengakses pesan kernel, yang dapat menyebabkan kebocoran informasi jika pesan dipancarkan oleh kontainer;"

msgid "for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;"
msgstr "untuk alasan yang sama, jika sebuah container terganggu dan kerentanan kernel dieksploitasi, container lain mungkin akan terpengaruh juga;"

msgid "on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers."
msgstr "pada sistem berkas, kernel memeriksa izin menurut pengenal numerik untuk pengguna dan kelompok; pengidentifikasi ini dapat menetapkan pengguna dan kelompok yang berbeda tergantung pada container tersebut, yang harus diingat jika bagian-bagian yang dapat ditulisi dari sistem berkas dipakai bersama diantara wadah."

msgid "Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container."
msgstr "Karena kita berhadapan dengan isolasi dan virtualisasi yang tidak polos, menyiapkan container LXC lebih kompleks daripada hanya menjalankan debian-installer pada mesin virtual. Kami akan menjelaskan beberapa prasyarat, kemudian pergi ke konfigurasi jaringan; kami kemudian akan dapat benar-benar membuat sistem untuk dijalankan dalam container."

msgid "Preliminary Steps"
msgstr "Langkah Pendahuluan"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains the tools required to run LXC, and must therefore be installed."
msgstr "Paket <emphasis role=\"pkg\">lxc</emphasis> berisi alat-alat yang diperlukan untuk menjalankan LXC, dan karenanya harus dipasang."

msgid "LXC also requires the <emphasis>control groups</emphasis> configuration system, which is a virtual filesystem to be mounted on <filename>/sys/fs/cgroup</filename>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration."
msgstr "LXC juga memerlukan sistem konfigurasi <emphasis>control group</emphasis>, yang berupa sistem berkas virtual untuk dipasang pada <filename>/sys/fs/cgroup</filename>. Karena Debian 8 beralih ke systemd, yang juga bergantung pada control group, hal ini sekarang dilakukan secara otomatis saat boot tanpa konfigurasi lebih lanjut."

msgid "Network Configuration"
msgstr "Konfigurasi Jaringan"

#, fuzzy
#| msgid "The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgid "The goal of installing LXC is to set up virtual machines; while we could, of course, keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgstr "Tujuan dari memasang LXC adalah untuk menyiapkan mesin virtual; walaupun tentu saja kita bisa menjaga mereka terisolasi dari jaringan, dan hanya berkomunikasi dengan mereka melalui sistem berkas, penggunaan umumnya memberikan setidaknya akses jaringan minimum ke container. Dalam kasus yang tipikal, masing-masing akan mendapatkan antarmuka jaringan virtual, yang terhubung ke jaringan nyata melalui sebuah bridge. Antarmuka virtual ini dapat dipasang langsung ke antarmuka jaringan fisik host (dalam hal ini container langsung berada pada jaringan) atau ke antarmuka virtual lain yang didefinisikan pada host (dan host kemudian dapat menyaring atau mengarahkan lalu lintas). Dalam kedua kasus, paket <emphasis role=\"pkg\">bridge-utils</emphasis> akan diperlukan."

#, fuzzy
#| msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance, <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgstr "Kasus sederhana ini hanya sekadar menyunting <filename>/etc/network/interfaces</filename>, memindah konfigurasi untuk antarmuka fisik (misalnya <literal>eth0</literal>) ke antarmuka bridge (biasanya <literal>br0</literal>) dan mengkonfigurasi link antara mereka. Misalnya, jika berkas konfigurasi antarmuka jaringan pada awalnya berisi entri seperti berikut:"

msgid ""
"auto eth0\n"
"iface eth0 inet dhcp"
msgstr ""
"auto eth0\n"
"iface eth0 inet dhcp"

msgid "They should be disabled and replaced with the following:"
msgstr "Mereka harus dinonaktifkan dan diganti dengan yang berikut:"

msgid ""
"#auto eth0\n"
"#iface eth0 inet dhcp\n"
"\n"
"auto br0\n"
"iface br0 inet dhcp\n"
"  bridge-ports eth0"
msgstr ""
"#auto eth0\n"
"#iface eth0 inet dhcp\n"
"\n"
"auto br0\n"
"iface br0 inet dhcp\n"
"  bridge-ports eth0"

msgid "The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <literal>eth0</literal> as well as the interfaces defined for the containers."
msgstr "Efek dari konfigurasi ini akan mirip dengan apa yang dapat diperoleh jika container itu adalah mesin yang terhubung ke jaringan fisik yang sama seperti host. Konfigurasi \"jbridge\" mengelola transit dari frame-frame Ethernet antara semua antarmuka yang dijembatani, yang mencakup fisik <literal>eth0</literal> maupun antarmuka yang didefinisikan untuk container."

#, fuzzy
#| msgid "In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgid "In cases where this configuration cannot be used (for instance, if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgstr "Dalam kasus-kasus yang mana konfigurasi ini tidak dapat digunakan (misalnya jika tidak ada alamat IP publik dapat diberikan ke container), antarmuka virtual <emphasis>tap</emphasis> akan dibuat dan terhubung dengan bridge. Topologi jaringan kemudian menjadi suatu host dengan kartu jaringan kedua yang ditancapkan ke sebuah switch yang terpisah, dengan container juga dicolokkan ke switch itu. Host kemudian mesti bertindak sebagai sebuah gateway untuk container jika mereka dimaksudkan untuk berkomunikasi dengan dunia luar."

msgid "In addition to <emphasis role=\"pkg\">bridge-utils</emphasis>, this “rich” configuration requires the <emphasis role=\"pkg\">vde2</emphasis> package; the <filename>/etc/network/interfaces</filename> file then becomes:"
msgstr "Selain <emphasis role=\"pkg\">bridge-utils</emphasis>, konfigurasi \"kaya\" ini memerlukan paket <emphasis role=\"pkg\">vde2</emphasis>; berkas <filename>/etc/network/interfaces</filename> kemudian menjadi:"

msgid ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"
msgstr ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"

msgid "The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <literal>br0</literal> interface."
msgstr "Jaringan kemudian dapat diatur baik secara statis dalam container, atau secara dinamis dengan server DHCP yang berjalan pada host. Server DHCP tersebut perlu dikonfigurasi untuk menjawab pertanyaan pada antarmuka <literal>br0</literal>."

msgid "Setting Up the System"
msgstr "Menyiapkan Sistem"

msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgstr "Mari kita sekarang menyiapkan sistem berkas yang akan digunakan oleh container. Karena \"mesin virtual\" ini tidak akan berjalan secara langsung pada perangkat keras, beberapa tweak diperlukan bila dibandingkan dengan sistem berkas standar, terutama bila menyangkut kernel, perangkat, dan konsol. Untungnya, <emphasis role=\"pkg\">lxc</emphasis> menyertakan skrip yang kebanyakan mengotomatisasi konfigurasi ini. Sebagai contoh, perintah berikut (yang membutuhkan <emphasis role=\"pkg\">debootstrap</emphasis> dan <emphasis role=\"pkg\">rsync</emphasis> paket) akan menginstal sebuah container Debian:"

msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"root@mirwiz:~# </computeroutput>\n"
"        "
msgstr "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\nChecking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \nDownloading debian minimal ...\nI: Retrieving Release \nI: Retrieving Release.gpg \n[...]\nDownload complete.\nCopying rootfs to /var/lib/lxc/testlxc/rootfs...\n[...]\nroot@mirwiz:~# </computeroutput>\n        "

msgid "Note that the filesystem is initially created in <filename>/var/cache/lxc</filename>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required."
msgstr "Perhatikan bahwa sistem berkas awalnya dibuat di <filename>/var/cache/lxc</filename>, kemudian dipindah ke direktori tujuannya. Hal ini memungkinkan membuat container-container identik secara jauh lebih cepat, karena kemudian hanya perlu menyalin."

#, fuzzy
#| msgid "Note that the debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgid "Note that the Debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgstr "Perhatikan bahwa skrip penciptaan templat debian menerima pilihan <option>--arch</option> untuk menentukan arsitektur sistem yang akan diinstal dan pilihan <option>--release</option> jika Anda ingin menginstal sesuatu yang lain daripada rilis stabil Debian. Anda juga dapat menetapkan variabel lingkungan <literal>MIRROR</literal> untuk menunjuk ke mirror Debian lokal."

msgid "The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<filename>/var/lib/lxc/testlxc/config</filename>) and add a few <literal>lxc.network.*</literal> entries:"
msgstr "Sistem berkas yang baru dibuat sekarang berisi sistem Debian yang minimal, dan secara default container tidak memiliki antarmuka jaringan (selain loopback). Karena ini tidak benar-benar diinginkan, kita akan menyunting berkas konfigurasi container (<filename>/var/lib/lxc/testlxc/config</filename>) dan menambahkan beberapa entri <literal>lxc.network.*</literal>:"

#, fuzzy
#| msgid ""
#| "lxc.network.type = veth\n"
#| "lxc.network.flags = up\n"
#| "lxc.network.link = br0\n"
#| "lxc.network.hwaddr = 4a:49:43:49:79:20"
msgid ""
"lxc.net.0.type = veth\n"
"lxc.net.0.flags = up\n"
"lxc.net.0.link = br0\n"
"lxc.net.0.hwaddr = 4a:49:43:49:79:20"
msgstr ""
"lxc.network.type = veth\n"
"lxc.network.flags = up\n"
"lxc.network.link = br0\n"
"lxc.network.hwaddr = 4a:49:43:49:79:20"

msgid "These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <literal>br0</literal> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated."
msgstr "Entri ini berarti, masing-masing, bahwa suatu antarmuka virtual akan dibuat di dalam container; bahwa itu akan secara otomatis dihidupkan ketika container dimulai; itu akan secara otomatis terhubung ke bridge <literal>br0</literal> pada host; dan bahwa alamat MAC-nya akan seperti yang ditentukan. Bila entri terakhir ini hilang atau dinonaktifkan, alamat MAC acak akan dibuat."

msgid "Another useful entry in that file is the setting of the hostname:"
msgstr "Entri lain yang berguna dalam berkas itu adalah pengaturan nama host:"

#, fuzzy
#| msgid "lxc.utsname = testlxc"
msgid "lxc.uts.name = testlxc"
msgstr "lxc.utsname = testlxc"

msgid "Starting the Container"
msgstr "Memulai Container"

msgid "Now that our virtual machine image is ready, let's start the container with <command>lxc-start --daemon --name=testlxc</command>."
msgstr "Sekarang setelah image mesin virtual kita sudah siap, mari kita mulai container dengan <command>lxc-start --daemon --name=testlxc</command>."

msgid "In LXC releases following 2.0.8, root passwords are not set by default. We can set one running <command>lxc-attach -n testlxc <replaceable>passwd</replaceable>.</command> Now we can login:"
msgstr ""

msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 9 testlxc console\t\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 4.19.0-5-amd64 #1 SMP Debian 4.19.37-5 (2019-06-19) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  56736  6608 ?        Ss   09:28   0:00 /sbin/init\n"
"root        32  0.0  0.1  46096  4680 ?        Ss   09:28   0:00 /lib/systemd/systemd-journald\n"
"root        75  0.0  0.1  67068  3328 console  Ss   09:28   0:00 /bin/login --\n"
"root        82  0.0  0.1  19812  3664 console  S    09:30   0:00  \\_ -bash\n"
"root        88  0.0  0.1  38308  3176 console  R+   09:31   0:00      \\_ ps auxwf\n"
"root        76  0.0  0.1  69956  5636 ?        Ss   09:28   0:00 /usr/sbin/sshd -D\n"
"root@testlxc:~# </computeroutput>"
msgstr "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n</userinput><computeroutput>Debian GNU/Linux 9 testlxc console\t\n\ntestlxc login: </computeroutput><userinput>root</userinput><computeroutput>\nPassword: \nLinux testlxc 4.19.0-5-amd64 #1 SMP Debian 4.19.37-5 (2019-06-19) x86_64\n\nThe programs included with the Debian GNU/Linux system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\npermitted by applicable law.\nroot@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.0  0.2  56736  6608 ?        Ss   09:28   0:00 /sbin/init\nroot        32  0.0  0.1  46096  4680 ?        Ss   09:28   0:00 /lib/systemd/systemd-journald\nroot        75  0.0  0.1  67068  3328 console  Ss   09:28   0:00 /bin/login --\nroot        82  0.0  0.1  19812  3664 console  S    09:30   0:00  \\_ -bash\nroot        88  0.0  0.1  38308  3176 console  R+   09:31   0:00      \\_ ps auxwf\nroot        76  0.0  0.1  69956  5636 ?        Ss   09:28   0:00 /usr/sbin/sshd -D\nroot@testlxc:~# </computeroutput>"

msgid "We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<filename>/var/lib/lxc/testlxc/rootfs</filename>). We can exit the console with <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."
msgstr "Kita sekarang berada di dalam container; akses kita ke proses dibatasi ke hanya yang dimulai dari container itu sendiri, dan akses kita ke sistem berkas juga dibatasi ke subset yang terdedikasi dari sistem berkas penuh (<filename>/var/lib/lxc/testlxc/rootfs</filename>). Kita dapat keluar dari konsol dengan <keycombo action=\"simul\"><keycap>Kontrol</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."

msgid "Note that we ran the container as a background process, thanks to the <option>--daemon</option> option of <command>lxc-start</command>. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgstr "Perhatikan bahwa kita menjalankan container sebagai proses latar belakang, terima kasih kepada pilihan <option>--daemon</option> dari <command>lxc-start</command>. Kita dapat menginterupsi container dengan perintah seperti <command>lxc-stop --name=testlxc</command>."

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <command>lxc-autostart</command> which starts containers whose <literal>lxc.start.auto</literal> option is set to 1). Finer-grained control of the startup order is possible with <literal>lxc.start.order</literal> and <literal>lxc.group</literal>: by default, the initialization script first starts containers which are part of the <literal>onboot</literal> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <literal>lxc.start.order</literal> option."
msgstr "Paket <emphasis role=\"pkg\">lxc</emphasis> berisi skrip inisialisasi yang dapat secara otomatis memulai satu atau beberapa container ketika host mem-boot (bergantung pada <command>lxc-autostart</command> yang memulai container yang opsi <literal>lxc.start.auto</literal> diberi nilai 1). Kontrol urutan startup yang lebih baik dimungkinkan dengan <literal>lxc.start.order</literal> dan <literal>lxc.group</literal>: secara default, skrip inisialisasi pertama-tama memulai container yang merupakan bagian dari grup <literal>onboot</literal> dan kemudian container yang bukan bagian dari grup manapun. Dalam kedua kasus, urutan dalam grup ditentukan oleh opsi <literal>lxc.start.order</literal>."

msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgstr "<emphasis>LEBIH JAUH</emphasis> Virtualisasi masal"

msgid "Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <literal>tap</literal> and <literal>veth</literal> interfaces should be enough in many cases."
msgstr "Karena LXC adalah sebuah sistem isolasi yang sangat ringan, itu dapat diadaptasi untuk hosting besar-besaran dari server-server virtual. Konfigurasi jaringan mungkin akan sedikit lebih lanjut dari apa yang dijelaskan di atas, tetapi konfigurasi \"kaya\" menggunakan antarmuka <literal>tap</literal> dan <literal>veth</literal> mestinya cukup untuk banyak kasus."

msgid "It may also make sense to share part of the filesystem, such as the <filename>/usr</filename> and <filename>/lib</filename> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <literal>lxc.mount.entry</literal> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage."
msgstr "Mungkin juga masuk akal untuk berbagi bagian dari sistem berkas, seperti sub pohon <filename>/usr</filename> dan <filename>/lib</filename>, untuk menghindari duplikasi perangkat lunak yang mungkin perlu sama untuk beberapa container. Ini biasanya akan dicapai dengan entri <literal>lxc.mount.entry</literal> di berkas konfigurasi container. Efek samping yang menarik adalah bahwa proses kemudian akan menggunakan memori fisik yang lebih kecil, karena kernel mampu mendeteksi bahwa program dipakai bersama. Biaya marjinal dari satu container tambahan kemudian dikurangi menjadi ruang disk yang didedikasikan untuk data tertentu, dan beberapa proses tambahan yang harus dijadwalkan dan dikelola oleh kernel."

msgid "We haven't described all the available options, of course; more comprehensive information can be obtained from the <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> and <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual pages and the ones they reference."
msgstr "Kami tentu saja belum menguraikan semua pilihan yang tersedia; informasi lebih lengkap dapat diperoleh dari halaman manual <citerefentry><refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum></citerefentry> dan <citerefentry><refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum> </citerefentry> dan yang mereka acu."

msgid "Virtualization with KVM"
msgstr "Virtualisasi dengan KVM"

msgid "<primary>KVM</primary>"
msgstr "<primary>KVM</primary>"

msgid "KVM, which stands for <emphasis>Kernel-based Virtual Machine</emphasis>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <command>qemu-*</command> commands: it is still about KVM."
msgstr "KVM, kependekan dari <emphasis>Kernel-based Virtual Machine</emphasis>, adalah pertama dan terutama sebuah modul kernel yang menyediakan sebagian besar infrastruktur yang dapat digunakan oleh virtualizer, tetapi bukan virtualizer itu sendiri. Kontrol aktual untuk virtualisasi ditangani oleh sebuah aplikasi berbasis QEMU. Jangan khawatir jika bagian ini menyebutkan perintah <command>qemu-*</command>: itu masih tentang KVM."

msgid "Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <filename>/proc/cpuinfo</filename>."
msgstr "Tidak seperti sistem virtualisasi lain, KVM digabungkan ke kernel Linux sejak dari awal. Para pengembangnya memilih untuk mengambil keuntungan dari set instruksi prosesor yang didedikasikan untuk virtualisasi (Intel-VT dan AMD-V), yang menjaga KVM ringan, elegan, dan tidak boros sumber daya. Kekurangannya, tentu saja, adalah bahwa KVM tidak bekerja pada sebarang komputer tetapi hanya pada yang memiliki prosesor yang sesuai. Untuk komputer berbasis x86, Anda dapat memastikan bahwa Anda memiliki prosesor seperti itu dengan mencari \"vmx\" atau \"svm\" di bendera CPU yang tercantum dalam <filename>/proc/cpuinfo</filename>."

msgid "With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization."
msgstr "Dengan Red Hat secara aktif mendukung perkembangannya, KVM kurang lebih telah menjadi acuan untuk virtualisasi Linux."

msgid "<primary><command>virt-install</command></primary>"
msgstr "<primary><command>virt-install</command></primary>"

msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgstr "Tidak seperti alat-alat semacam VirtualBox, KVM itu sendiri tidak menyertakan antarmuka pengguna untuk membuat dan mengelola mesin virtual. Paket <emphasis role=\"pkg\">qemu-kvm</emphasis> hanya menyediakan program yang bisa memulai sebuah mesin virtual, serta skrip inisialisasi yang memuat modul-modul kernel yang sesuai."

msgid "<primary>libvirt</primary>"
msgstr "<primary>libvirt</primary>"

msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <command>virtual-manager</command> is a graphical interface that uses libvirt to create and manage virtual machines."
msgstr "Untungnya, Red Hat juga menyediakan satu set alat lain untuk mengatasi masalah itu, dengan mengembangkan perpustakaan <emphasis>libvirt</emphasis> dan alat-alat <emphasis>manajer mesin virtual</emphasis> terkait. Libvirt memungkinkan mengelola mesin virtual dalam cara yang seragam, secara independen dari sistem virtualisasi yang terlibat di balik layar (saat ini mendukung QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare, dan UML). <command>virtual-manager</command> adalah sebuah antarmuka grafis yang menggunakan libvirt untuk membuat dan mengelola mesin virtual."

msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "We first install the required packages, with <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-bin</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgid "We first install the required packages, with <command>apt-get install libvirt-clients libvirt-daemon-system qemu-kvm virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-daemon-system</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. <emphasis role=\"pkg\">libvirt-clients</emphasis> provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgstr "Kita pertama kali menginstal paket-paket yang diperlukan, dengan <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-bin</emphasis> menyediakan daemon <command>libvirtd</command>, yang memungkinkan manajemen mesin virtual (berpotensi remote) yang berjalan pada host, dan memulai VM yang diperlukan ketika host boot. Selain itu, paket ini menyediakan alat bantu baris perintah <command>virsh</command>, yang memungkinkan mengendalikan mesin-mesin yang dikelola oleh <command>libvirtd</command>."

msgid "The <emphasis role=\"pkg\">virtinst</emphasis> package provides <command>virt-install</command>, which allows creating virtual machines from the command line. Finally, <emphasis role=\"pkg\">virt-viewer</emphasis> allows accessing a VM's graphical console."
msgstr "Paket <emphasis role=\"pkg\">virtinst</emphasis> menyediakan <command>virt-install</command>, yang memungkinkan membuat mesin virtual dari baris perintah. Terakhir, <emphasis role=\"pkg\">virt-viewer</emphasis> memungkinkan mengakses sebuah konsol grafis VM."

msgid "Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <xref linkend=\"sect.lxc.network\" />)."
msgstr "Sama seperti Xen dan LXC, konfigurasi jaringan yang paling sering melibatkan bridge yang mengelompokkan antarmuka jaringan mesin virtual (lihat <xref linkend=\"sect.lxc.network\" />)."

msgid "Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network."
msgstr "Sebagai alternatif, dan dalam konfigurasi default yang disediakan oleh KVM, mesin virtual diberikan alamat pribadi (di kisaran 192.168.122.0/24), dan NAT diatur sehingga VM dapat mengakses jaringan luar."

msgid "The rest of this section assumes that the host has an <literal>eth0</literal> physical interface and a <literal>br0</literal> bridge, and that the former is connected to the latter."
msgstr "Sisa bagian ini mengasumsikan bahwa host memiliki antarmuka fisik <literal>eth0</literal> dan bridge <literal>br0</literal>, dan bahwa yang terdahulu terhubung ke yang terakhir."

msgid "Installation with <command>virt-install</command>"
msgstr "Instalasi dengan <command>virt-install</command>"

msgid "Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line."
msgstr "Membuat mesin virtual sangat mirip dengan menginstal sistem normal, kecuali bahwa karakteristik mesin virtual dijelaskan dalam baris perintah yang tampaknya tak berujung."

msgid "Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <xref linkend=\"sect.remote-desktops\" /> for details), which will allow us to control the installation process."
msgstr "Secara praktis, ini berarti kita akan menggunakan installer Debian, dengan mem-boot mesin virtual pada drive DVD-ROM virtual yang memetakan ke image DVD Debian yang tersimpan di sistem host. VM akan mengekspor konsol grafisnya lewat protokol VNC (lihat <xref linkend=\"sect.remote-desktops\" /> untuk rincian), yang akan memungkinkan kita untuk mengontrol proses instalasi."

msgid "We first need to tell libvirtd where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgstr "Pertama kita perlu memberitahu libvirtd di mana tempat menyimpan image disk, kecuali bila lokasi default (<filename>/var/lib/libvirt/images/</filename>) baik-baik saja."

msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"

msgid "<emphasis>TIP</emphasis> Add your user to the libvirt group"
msgstr "<emphasis>TIPS</emphasis> Menambahkan pengguna Anda ke grup libvirt"

#, fuzzy
#| msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yourself to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgstr "Semua contoh dalam bagian ini mengasumsikan bahwa Anda menjalankan perintah sebagai root. Secara efektif, jika Anda ingin mengontrol daemon libvirt lokal, Anda perlu untuk menjadi root atau menjadi anggota dari kelompok <literal>libvirt</literal> (yang tidak terjadi secara default). Jadi jika Anda ingin menghindari menggunakan hak root terlalu sering, Anda dapat menambahkan diri Anda sendiri ke grup <literal>libvirt</literal> dan menjalankan berbagai perintah di bawah identitas pengguna Anda."

msgid "Let us now start the installation process for the virtual machine, and have a closer look at <command>virt-install</command>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed."
msgstr "Mari kita sekarang mulai proses instalasi untuk mesin virtual, dan melihat lebih dekat pada pilihan-pilihan <command>virt-install</command> yang paling penting. Perintah ini mendaftarkan mesin virtual dan parameternya di libvirtd, kemudian memulainya sehingga instalasi dapat dilanjutkan."

msgid ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --memory 1024             <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-10.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n"
"               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debian10\n"
"</userinput><computeroutput>\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
"</computeroutput>"
msgstr "<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n               --virt-type kvm           <co id=\"virtinst.type\"></co>\n               --name testkvm            <co id=\"virtinst.name\"></co>\n               --memory 1024             <co id=\"virtinst.ram\"></co>\n               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n               --cdrom /srv/isos/debian-10.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n               --os-type linux           <co id=\"virtinst.os\"></co>\n               --os-variant debian10\n</userinput><computeroutput>\nStarting install...\nAllocating 'testkvm.qcow'             |  10 GB     00:00\n</computeroutput>"

msgid "The <literal>--connect</literal> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<literal>/system</literal>) from others (<literal>/session</literal>)."
msgstr "Opsi <literal>--connect</literal> menyatakan\"hypervisor\" yang akan dipakai. Bentuknya adalah URL yang memuat sistem virtualisasi (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, dan seterusnya) dan mesin yang harus menjadi host VM (ini dapat dibiarkan kosong dalam kasus hosting lokal). Selain itu, dan dalam kasus QEMU/KVM, setiap pengguna dapat mengelola mesin virtual yang bekerja dengan izin terbatas, dan path URL memungkinkan membedakan mesin \"sistem\" (<literal>/system</literal>) dari (<literal>/session</literal>) yang lain."

msgid "Since KVM is managed the same way as QEMU, the <literal>--virt-type kvm</literal> allows specifying the use of KVM even though the URL looks like QEMU."
msgstr "Karena KVM dikelola dengan cara yang sama seperti QEMU, <literal>--virt-type kvm</literal> mengizinkan menyatakan penggunaan KVM meskipun URL terlihat seperti QEMU."

msgid "The <literal>--name</literal> option defines a (unique) name for the virtual machine."
msgstr "Opsi <literal>--name</literal> mendefinisikan nama (unik) untuk mesin virtual."

#, fuzzy
#| msgid "The <literal>--ram</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgid "The <literal>--memory</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgstr "Opsi <literal>--ram</literal> memungkinkan menentukan banyaknya RAM (dalam MB) yang dialokasikan untuk mesin virtual."

#, fuzzy
#| msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>raw</literal>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space."
msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>qcow2</literal>) allows starting with a small file that only grows when the virtual machine starts actually using space."
msgstr "<literal>--disk</literal> menyatakan lokasi berkas image yang mewakili hard disk mesin virtual; berkas itu dibuat, kecuali sudah ada, dengan ukuran (dalam GB) yang ditentukan oleh parameter <literal>size</literal>. Parameter <literal>format</literal> memungkinkan memilih antara beberapa cara untuk menyimpan berkas image. Format default (<literal>raw</literal>) adalah satu berkas persis cocok dengan ukuran dan isi disk. Kita memilih format yang lebih maju di sini, yang khusus untuk QEMU dan memungkinkan mulai dengan berkas kecil yang hanya tumbuh ketika mesin virtual mulai benar-benar menggunakan ruang."

msgid "The <literal>--cdrom</literal> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <literal>/dev/cdrom</literal>)."
msgstr "Opsi <literal>--cdrom</literal> digunakan untuk menunjukkan di mana menemukan disk optik yang digunakan untuk instalasi. Path bisa berupa path lokal untuk berkas ISO, URL tempat berkas dapat diperoleh, atau perangkat berkas dari drive CD-ROM fisik (yaitu <literal>/dev/cdrom</literal>)."

msgid "The <literal>--network</literal> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24)."
msgstr "<literal>--network</literal> menyatakan bagaimana kartu jaringan virtual mengintegrasi dalam konfigurasi jaringan host. Perilaku default (yang secara eksplisit kita paksa dalam contoh kita) adalah mengintegrasikannya ke dalam jaringan bridge apapun yang sudah ada. Jika bridge seperti itu tidak ada, mesin virtual hanya akan mencapai jaringan fisik melalui NAT, sehingga mendapat alamat di subnet pribadi (192.168.122.0/24)."

#, fuzzy
#| msgid "<literal>--vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, the <literal>--vnclisten=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgid "<literal>--graphics vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, <literal>--graphics vnc,listen=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgstr "<literal>--vnc</literal> menyatakan bahwa konsol grafis harus dibuat tersedia menggunakan VNC. Perilaku default untuk server VNC terkait adalah hanya mendengarkan pada antarmuka lokal; jika klien VNC akan dijalankan pada host yang berbeda, membuat koneksi akan memerlukan pengaturan tunnel SSH (lihat <xref linkend=\"sect.ssh-port-forwarding\" />). Sebagai alternatif, <literal>--vnclisten=0.0.0.0</literal> dapat digunakan sehingga server VNC dapat diakses dari semua antarmuka; perhatikan bahwa jika Anda melakukannya, Anda benar-benar harus merancang firewall Anda sesuai dengan itu."

msgid "The <literal>--os-type</literal> and <literal>--os-variant</literal> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there."
msgstr "Pilihan <literal>--os-type</literal> dan <literal>--os-variant</literal> memungkinkan mengoptimalkan beberapa parameter mesin virtual, berdasarkan fitur yang dikenal dari sistem operasi yang disebutkan di sana."

msgid "At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <command>virt-viewer</command> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):"
msgstr "Pada titik ini, mesin virtual sedang berjalan, dan kita perlu terhubung ke konsol grafis untuk melanjutkan dengan proses instalasi. Jika operasi sebelumnya berjalan dari lingkungan desktop grafis, hubungan ini harus secara otomatis dimulai. Jika tidak, atau jika kita beroperasi jarak jauh, <command>virt-viewer</command> dapat dijalankan dari setiap lingkungan grafis untuk membuka konsol grafis (perhatikan bahwa kata sandi root dari host remote diminta dua kali karena operasi memerlukan 2 koneksi SSH):"

msgid ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"

msgid "When the installation process ends, the virtual machine is restarted, now ready for use."
msgstr "Ketika proses instalasi berakhir, mesin virtual dijalankan ulang, sekarang siap untuk digunakan."

msgid "Managing Machines with <command>virsh</command>"
msgstr "Mengelola Mesin dengan <command>virsh</command>"

msgid "<primary><command>virsh</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <command>libvirtd</command> for the list of the virtual machines it manages:"
msgstr "Sekarang setelah instalasi selesai, mari kita lihat bagaimana menangani mesin virtual yang tersedia. Hal pertama yang dicoba adalah untuk bertanya ke <command>libvirtd</command> daftar mesin virtual yang dikelolanya:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
#| " Id Name                 State\n"
#| "----------------------------------\n"
#| "  - testkvm              shut off\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  8 testkvm              shut off\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  - testkvm              shut off\n"
"</userinput>"

msgid "Let's start our test virtual machine:"
msgstr "Mari kita mulai jalankan mesin virtual uji kita:"

msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"

msgid "We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <command>vncviewer</command>):"
msgstr "Kita sekarang bisa mendapatkan petunjuk koneksi untuk konsol grafis (tampilan VNC yang dikembalikan dapat diberikan sebagai parameter ke <command>vncviewer</command>):"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
#| "</userinput><computeroutput>:0</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>127.0.0.1:0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>:0</computeroutput>"

msgid "Other available <command>virsh</command> subcommands include:"
msgstr "Sub perintah <command>virsh</command> lain yang tersedia meliputi:"

msgid "<literal>reboot</literal> to restart a virtual machine;"
msgstr "<literal>reboot</literal> untuk memulai jalankan lagi sebuah mesin virtual;"

msgid "<literal>shutdown</literal> to trigger a clean shutdown;"
msgstr "<literal>shutdown</literal> untuk memicu suatu shutdown yang bersih;"

msgid "<literal>destroy</literal>, to stop it brutally;"
msgstr "<literal>destroy</literal>, untuk menghentikannya secara brutal;"

msgid "<literal>suspend</literal> to pause it;"
msgstr "<literal>suspend</literal> untuk mengistirahatkannya;"

msgid "<literal>resume</literal> to unpause it;"
msgstr "<literal>resume</literal> untuk melanjutkan dari istirahat;"

msgid "<literal>autostart</literal> to enable (or disable, with the <literal>--disable</literal> option) starting the virtual machine automatically when the host starts;"
msgstr "<literal>autostart</literal> untuk mengaktifkan (atau menonaktifkan, dengan pilihan <literal>--disable</literal>) memulai mesin virtual secara otomatis ketika host mulai;"

msgid "<literal>undefine</literal> to remove all traces of the virtual machine from <command>libvirtd</command>."
msgstr "<literal>undefine</literal> untuk menghapus semua jejak mesin virtual dari <command>libvirtd</command>."

msgid "All these subcommands take a virtual machine identifier as a parameter."
msgstr "Semua sub perintah ini mengambil sebuah identifier mesin virtual sebagai parameter."

msgid "Installing an RPM based system in Debian with yum"
msgstr "Instalasi sistem berbasis RPM dalam Debian dengan yum"

msgid "If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility (available in the package of the same name)."
msgstr "Jika mesin virtual dimaksudkan untuk menjalankan Debian (atau salah satu turunannya), sistem dapat diinisialisasi dengan <command>debootstrap</command>, seperti dijelaskan di atas. Tetapi jika mesin virtual diinstal dengan sistem berbasis RPM (seperti Fedora, CentOS, atau Scientific Linux), penyiapan akan perlu dilakukan menggunakan utilitas <command>yum</command> (tersedia dalam paket dengan nama yang sama)."

msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum</command> to extract the remaining set of packages. But since we call <command>yum</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgstr "Prosedur tersebut perlu memakai <command>rpm</command> untuk mengekstrak set awal berkas, termasuk terutama berkas konfigurasi <command>yum</command>, dan kemudian memanggil <command>yum</command> untuk mengekstrak kumpulan paket sisanya. Tapi karena kita memanggil <command>yum</command> dari luar chroot, kita perlu membuat beberapa perubahan sementara. Dalam contoh di bawah ini, chroot target adalah <filename>/srv/centos</filename>."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
#| "</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
#| "</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
#| "rpm: However assuming you know what you are doing...\n"
#| "warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
#| "</userinput><computeroutput>[...]\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-6.1810.2.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-6.1810.2.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"

msgid "Automated Installation"
msgstr "Pemasangan Otomatis"

msgid "<primary>deployment</primary>"
msgstr "<primary>penggelaran</primary>"

msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgstr "<primary>instalasi</primary><secondary>instalasi terotomasi</secondary>"

msgid "The Falcot Corp administrators, like many administrators of large IT services, need tools to install (or reinstall) quickly, and automatically if possible, their new machines."
msgstr "Administrator Falcot Corp, seperti banyak administrator dari layanan TI yang besar, membutuhkan alat untuk menginstal (atau menginstal ulang) dengan cepat, dan secara otomatis jika mungkin, mesin-mesin baru mereka."

msgid "These requirements can be met by a wide range of solutions. On the one hand, generic tools such as SystemImager handle this by creating an image based on a template machine, then deploy that image to the target systems; at the other end of the spectrum, the standard Debian installer can be preseeded with a configuration file giving the answers to the questions asked during the installation process. As a sort of middle ground, a hybrid tool such as FAI (<emphasis>Fully Automatic Installer</emphasis>) installs machines using the packaging system, but it also uses its own infrastructure for tasks that are more specific to massive deployments (such as starting, partitioning, configuration and so on)."
msgstr "Persyaratan ini dapat dipenuhi oleh berbagai macam solusi. Di satu sisi, alat-alat generik seperti SystemImager menangani hal ini dengan menciptakan sebuah image yang didasarkan pada mesin templat, kemudian menyebarkan image ke sistem target; di ujung lain spektrum, penginstal Debian standar dapat diprabibit dengan berkas konfigurasi yang memberikan jawaban-jawaban untuk pertanyaan-pertanyaan yang ditanyakan selama proses instalasi. Sebagai semacam jalan tengah, alat hibrida seperti FAI (<emphasis>Fully Automatic Installer</emphasis>) menginstal mesin menggunakan sistem pemaketan, tetapi juga menggunakan infrastrukturnya sendiri untuk tugas-tugas yang lebih spesifik bagi penyebaran masif (seperti memulai, mempartisi, mengkonfigurasi, dan seterusnya)."

msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an i386 image won't fit on a powerpc machine, and so on)."
msgstr "Masing-masing solusi ini memiliki pro dan kontra: SystemImager bekerja secara independen dari sebarang sistem pemaketan tertentu, yang memungkinkan untuk mengatur set besar mesin menggunakan beberapa distro Linux yang berbeda. Ini juga mencakup sebuah sistem pemutakhiran yang tidak memerlukan instalasi ulang, tapi sistem pemutakhiran ini hanya dapat diandalkan jika mesin tidak diubah secara independen; dengan kata lain, pengguna harus tidak memperbarui perangkat lunak mereka sendiri, atau menginstal perangkat lunak lainnya. Demikian pula, pembaruan keamanan harus tidak otomatis, karena mereka harus pergi melalui image referensi terpusat yang dikelola oleh SystemImager. Solusi ini juga memerlukan mesin target yang homogen, bila tidak banyak image yang berbeda mesti disimpan dan dikelola (image i386 tidak akan cocok pada mesin powerpc, dan sebagainya)."

msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly simple-CDD (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even the simple-CDD solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgstr "Di sisi lain, instalasi otomatis menggunakan debian-installer dapat beradaptasi dengan spesifik dari setiap mesin: installer akan mengambil paket kernel dan perangkat lunak yang sesuai dari repositori yang relevan, mendeteksi perangkat keras yang tersedia, mempartisi seluruh hard disk untuk mengambil keuntungan dari semua ruang yang tersedia, menginstal sistem Debian yang sesuai, dan mengatur sebuah bootloader yang sesuai. Namun, pemasang standar hanya akan menginstal versi Debian standar, dengan sistem dasar dan satu set \"tugas\" yang terprapilih; ini menghalang menginstal sistem tertentu dengan aplikasi yang tidak dipaketkan. Memenuhi kebutuhan khusus ini memerlukan penyesuaian installer... Untungnya, installer sangat modular, dan ada alat untuk mengotomasi kebanyakan pekerjaan yang diperlukan untuk kustomisasi ini, yang paling penting simple-CDD (CDD adalah singkatan <emphasis>Custom Debian Derivative</emphasis>). Bahkan solusi simple-CDD, bagaimanapun, hanya menangani instalasi awal; hal ini biasanya tidak masalah karena perangkat APT memungkinkan penggelaran pemutakhiran yang efisien nanti."

msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian), in order to focus more intently on debian-installer and simple-CDD, which are more interesting in a Debian-only context."
msgstr "Kita hanya akan memberikan gambaran kasar FAI, dan melewati SystemImager sama sekali (yang tidak ada lagi di Debian), agar fokus lebih bersungguh-sungguh pada debian-installer dan simple-CDD, yang lebih menarik dalam konteks hanya Debian."

msgid "Fully Automatic Installer (FAI)"
msgstr "Fully Automatic Installer (FAI, Pemasang Otomatis Sepenuhnya)"

msgid "<primary>Fully Automatic Installer (FAI)</primary>"
msgstr "<primary>Fully Automatic Installer (FAI)</primary>"

msgid "<foreignphrase>Fully Automatic Installer</foreignphrase> is probably the oldest automated deployment system for Debian, which explains its status as a reference; but its very flexible nature only just compensates for the complexity it involves."
msgstr "<foreignphrase>Fully Automatic Installer</foreignphrase> mungkin adalah sistem penggelaran otomatis tertua untuk Debian, yang menjelaskan statusnya sebagai referensi; tetapi sifatnya yang sangat fleksibel hanya mengkompensasi kompleksitas yang melibatkannya."

msgid "FAI requires a server system to store deployment information and allow target machines to boot from the network. This server requires the <emphasis role=\"pkg\">fai-server</emphasis> package (or <emphasis role=\"pkg\">fai-quickstart</emphasis>, which also brings the required elements for a standard configuration)."
msgstr "FAI memerlukan sebuah sistem server untuk menyimpan informasi penggelaran dan memungkinkan mesin target untuk boot dari jaringan. Server ini memerlukan paket <emphasis role=\"pkg\">fai-server</emphasis> (atau <emphasis role=\"pkg\">fai-quickstart</emphasis>, yang juga membawa elemen-elemen yang diperlukan untuk sebuah konfigurasi standar)."

msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgstr "FAI menggunakan pendekatan khusus untuk menentukan berbagai profil yang dapat diinstal. Bukan hanya menduplikasi sebuah referensi instalasi, FAI adalah sebuah installer penuh, sepenuhnya dikonfigurasi melalui serangkaian berkas dan skrip yang disimpan di server; lokasi default <filename>/srv/fai/config/</filename> tidak secara otomatis diciptakan, sehingga administrator perlu menciptakannya beserta berkas-berkas yang relevan. Hampir setiap kali, berkas-berkas ini akan disesuaikan dari berkas contoh yang tersedia dalam dokumentasi untuk paket <emphasis role=\"pkg\">fai-doc</emphasis>, khususnya direktori <filename>/usr/share/doc/fai-doc/examples/simple/</filename>."

#, fuzzy
#| msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start a FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgstr "Sekali profil didefinisikan, perintah <command>fai-setup</command> menghasilkan unsur-unsur yang diperlukan untuk memulai instalasi FAI; ini sebagian besar berarti mempersiapkan atau memperbarui sistem minimal (NFS-root) yang digunakan selama instalasi. Satu alternatif adalah untuk menghasilkan CD boot tededikasi dengan <command>fai-cd</command>."

msgid "Creating all these configuration files requires some understanding of the way FAI works. A typical installation process is made of the following steps:"
msgstr "Menciptakan semua berkas konfigurasi ini memerlukan pemahaman tentang cara FAI bekerja. Suatu proses instalasi biasanya tersusun dari langkah-langkah berikut:"

msgid "fetching a kernel from the network, and booting it;"
msgstr "mengambil sebuah kernel dari jaringan, dan mem-boot itu;"

msgid "mounting the root filesystem from NFS;"
msgstr "mengait sistem berkas root dari NFS;"

msgid "executing <command>/usr/sbin/fai</command>, which controls the rest of the process (the next steps are therefore initiated by this script);"
msgstr "mengeksekusi <command>/usr/sbin/fai</command>, yang mengontrol seluruh proses (langkah berikutnya karena itu diprakarsai oleh skrip ini);"

msgid "copying the configuration space from the server into <filename>/fai/</filename>;"
msgstr "menyalin ruang konfigurasi dari server ke <filename>/fai/</filename>;"

msgid "running <command>fai-class</command>. The <filename>/fai/class/[0-9][0-9]*</filename> scripts are executed in turn, and return names of “classes” that apply to the machine being installed; this information will serve as a base for the following steps. This allows for some flexibility in defining the services to be installed and configured."
msgstr "menjalankan <command>fai-class</command>. Skrip <filename>/fai/class/[0-9][0-9]*</filename> dijalankan sesuai gilirannya, dan mengembalikan nama \"kelas\" yang berlaku untuk mesin yang diinstal; informasi ini akan berfungsi sebagai dasar untuk langkah-langkah berikut. Hal ini memungkinkan untuk beberapa fleksibilitas dalam mendefinisikan layanan yang akan diinstal dan dikonfigurasi."

msgid "fetching a number of configuration variables, depending on the relevant classes;"
msgstr "mengambil sejumlah variabel konfigurasi, tergantung pada kelas yang relevan;"

msgid "partitioning the disks and formatting the partitions, based on information provided in <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;"
msgstr "mempartisi disk dan memformat partisi, berdasarkan informasi yang diberikan dalam <filename>/fai/disk_config/<replaceable>kelas</replaceable></filename>;"

msgid "mounting said partitions;"
msgstr "mengaitkan partisi yang disebut;"

msgid "installing the base system;"
msgstr "memasang sistem dasar;"

msgid "preseeding the Debconf database with <command>fai-debconf</command>;"
msgstr "memprabibit basis data Debconf dengan <command>fai-debconf</command>;"

msgid "fetching the list of available packages for APT;"
msgstr "mengambil daftar paket yang tersedia untuk APT;"

msgid "installing the packages listed in <filename>/fai/package_config/<replaceable>class</replaceable></filename>;"
msgstr "menginstal paket-paket yang tercantum dalam <filename>/fai/package_config/<replaceable>kelas</replaceable></filename>;"

msgid "executing the post-configuration scripts, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;"
msgstr "menjalankan skrip pasca konfigurasi, <filename>/fai/scripts/<replaceable>kelas</replaceable>/[0-9][0-9]*</filename>;"

msgid "recording the installation logs, unmounting the partitions, and rebooting."
msgstr "merekam log instalasi, melepas kait partisi, dan reboot."

msgid "Preseeding Debian-Installer"
msgstr "Memprabibit Debian-Installer"

msgid "<primary>preseed</primary>"
msgstr "<primary>preseed</primary>"

msgid "<primary>preconfiguration</primary>"
msgstr "<primary>prakonfigurasi</primary>"

msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <emphasis>preseeding</emphasis>."
msgstr "Pada akhir hari, alat yang terbaik untuk menginstal sistem Debian secara logis mestinya adalah Debian installer yang resmi. Inilah mengapa, sejak dari awal, debian-installer telah dirancang untuk penggunaan otomatis, mengambil keuntungan dari infrastruktur yang disediakan oleh <emphasis role=\"pkg\">debconf</emphasis>. Yang kedua memungkinkan, di satu sisi, untuk mengurangi jumlah pertanyaan yang diajukan (pertanyaan-pertanyaan tersembunyi akan menggunakan jawaban default yang disediakan), dan di sisi lain, untuk menyediakan jawaban default secara terpisah, sehingga instalasi bisa non-interaktif. Fitur terakhir ini dikenal sebagai <emphasis>preseeding</emphasis>."

msgid "<emphasis>GOING FURTHER</emphasis> Debconf with a centralized database"
msgstr "<emphasis>LEBIH JAUH</emphasis> Debconf dengan suatu basis data terpusat"

msgid "<primary><command>debconf</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "Preseeding allows to provide a set of answers to Debconf questions at installation time, but these answers are static and do not evolve as time passes. Since already-installed machines may need upgrading, and new answers may become required, the <filename>/etc/debconf.conf</filename> configuration file can be set up so that Debconf uses external data sources (such as an LDAP directory server, or a remote file accessed via NFS or Samba). Several external data sources can be defined at the same time, and they complement one another. The local database is still used (for read-write access), but the remote databases are usually restricted to reading. The <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual page describes all the possibilities in detail (you need the <emphasis role=\"pkg\">debconf-doc</emphasis> package)."
msgstr "Prabibit memungkinkan untuk memberikan serangkaian jawaban ke pertanyaan Debconf pada saat instalasi, tetapi jawaban ini bersifat statis dan tidak berkembang seiring berjalannya waktu. Karena mesin yang sudah terpasang mungkin perlu ditingkatkan, dan jawaban baru dapat menjadi diperlukan, berkas konfigurasi <filename>/etc/debconf.conf</filename> dapat mengatur sehingga Debconf menggunakan sumber data eksternal (seperti sebuah direktori LDAP server, atau berkas jarak jauh yang diakses melalui NFS atau Samba). Beberapa sumber data eksternal dapat didefinisikan pada saat yang sama, dan mereka melengkapi satu sama lain. Basis data lokal masih digunakan (untuk akses baca-tulis), tetapi basis data jarak jauh biasanya dibatasi untuk hanya baca. Halaman manual <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> menjelaskan semua kemungkinan secara rinci (Anda perlu paket <emphasis role=\"pkg\">debconf-doc</emphasis>)."

msgid "Using a Preseed File"
msgstr "Menggunakan Berkas Preseed"

msgid "There are several places where the installer can get a preseeding file:"
msgstr "Ada beberapa tempat dimana installer bisa memperoleh berkas preseed:"

msgid "in the initrd used to start the machine; in this case, preseeding happens at the very beginning of the installation, and all questions can be avoided. The file just needs to be called <filename>preseed.cfg</filename> and stored in the initrd root."
msgstr "di initrd yang digunakan untuk memulai mesin; dalam kasus ini, prabibit terjadi pada awal instalasi, dan semua pertanyaan dapat dihindari. Berkas hanya perlu disebut <filename>preseed.cfg</filename> dan disimpan dalam root initrd."

msgid "on the boot media (CD or USB key); preseeding then happens as soon as the media is mounted, which means right after the questions about language and keyboard layout. The <literal>preseed/file</literal> boot parameter can be used to indicate the location of the preseeding file (for instance, <filename>/cdrom/preseed.cfg</filename> when the installation is done off a CD-ROM, or <filename>/hd-media/preseed.cfg</filename> in the USB-key case)."
msgstr "pada media boot (CD atau kunci USB); prabibit kemudian terjadi segera setelah media dipasang, yang berarti tepat setelah pertanyaan tentang bahasa dan tata letak papan ketik. Parameter boot <literal>preseed/file</literal> dapat digunakan untuk menunjukkan lokasi berkas prabibit (misalnya, <filename>/cdrom/preseed.cfg</filename> ketika instalasi dilakukan dari CD-ROM, atau <filename>/hd-media/preseed.cfg</filename> dalam kasus kunci USB)."

msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."
msgstr "dari jaringan; prabibit kemudian hanya terjadi setelah jaringan (secara otomatis) dikonfigurasi; parameter boot yang relevan adalah kemudian <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."

msgid "At a glance, including the preseeding file in the initrd looks like the most interesting solution; however, it is rarely used in practice, because generating an installer initrd is rather complex. The other two solutions are much more common, especially since boot parameters provide another way to preseed the answers to the first questions of the installation process. The usual way to save the bother of typing these boot parameters by hand at each installation is to save them into the configuration for <command>isolinux</command> (in the CD-ROM case) or <command>syslinux</command> (USB key)."
msgstr "Sekilas, menyertakan berkas prabibit di initrd tampak seperti solusi yang paling menarik; namun, ini jarang digunakan dalam praktek, karena menghasilkan installer initrd agak rumit. Kedua solusi yang lain lebih umum, terutama karena parameter boot menyediakan cara lain untuk memprabibit jawaban atas pertanyaan pertama dari proses instalasi. Cara yang biasa untuk menghindari kerepotan mengetik parameter boot ini di setiap instalasi adalah dengan menyimpan mereka ke dalam konfigurasi untuk <command>isolinux</command> (dalam kasus CD-ROM) atau <command>syslinux</command> (kunci USB)."

msgid "Creating a Preseed File"
msgstr "Membuat Berkas Preseed"

msgid "A preseed file is a plain text file, where each line contains the answer to one Debconf question. A line is split across four fields separated by whitespace (spaces or tabs), as in, for instance, <literal>d-i mirror/suite string stable</literal>:"
msgstr "Prabibit adalah berkas teks biasa, dimana setiap baris berisi jawaban atas satu pertanyaan Debconf. Baris dipecah ke empat bidang yang dipisahkan oleh spasi atau tab, seperti, misalnya, <literal>d-i mirror/suite string stable</literal>:"

msgid "the first field is the “owner” of the question; “d-i” is used for questions relevant to the installer, but it can also be a package name for questions coming from Debian packages;"
msgstr "bidang pertama adalah \"pemilik\" pertanyaan; \"d-i\" digunakan untuk pertanyaan-pertanyaan yang relevan dengan installer, tetapi juga bisa berupa nama paket untuk pertanyaan-pertanyaan yang datang dari paket-paket Debian;"

msgid "the second field is an identifier for the question;"
msgstr "ruas kedua adalah pengidentifikasi untuk pertanyaan;"

msgid "third, the type of question;"
msgstr "ketiga, jenis pertanyaan;"

msgid "the fourth and last field contains the value for the answer. Note that it must be separated from the third field with a single space; if there are more than one, the following space characters are considered part of the value."
msgstr "ke empat dan ruas terakhir memuat nilai untuk jawaban. Perhatikan bahwa ini harus dipisahkan dari ruas ke tiga dengan satu spasi; jika ada lebih dari satu, karakter spasi yang mengikuti dianggap bagian dari nilai."

msgid "The simplest way to write a preseed file is to install a system by hand. Then <command>debconf-get-selections --installer</command> will provide the answers concerning the installer. Answers about other packages can be obtained with <command>debconf-get-selections</command>. However, a cleaner solution is to write the preseed file by hand, starting from an example and the reference documentation: with such an approach, only questions where the default answer needs to be overridden can be preseeded; using the <literal>priority=critical</literal> boot parameter will instruct Debconf to only ask critical questions, and use the default answer for others."
msgstr "Cara termudah untuk menulis berkas preseed adalah untuk menginstal sebuah sistem dengan tangan. Kemudian <command>debconf-get-selections --installer</command> akan memberikan jawaban tentang installer. Jawaban tentang paket lainnya dapat diperoleh dengan <command>debconf-get-selections</command>. Namun, solusi yang lebih bersih adalah dengan menulis berkas preseed dengan tangan, mulai dari contoh dan dokumentasi referensi: dengan pendekatan seperti itu, hanya pertanyaan yang jawaban bakunya perlu ditimpa dapat diprabibit; menggunakan parameter boot <literal>priority=critical</literal> akan menginstruksikan Debconf untuk hanya mengajukan pertanyaan yang kritis, dan menggunakan jawaban baku bagi yang lain."

msgid "<emphasis>DOCUMENTATION</emphasis> Installation guide appendix"
msgstr "<emphasis>DOKUMENTASI</emphasis> Lampiran panduan instalasi"

#, fuzzy
#| msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"
msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/apb\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/example-preseed.txt\" />"
msgstr "Panduan instalasi, tersedia secara daring, termasuk dokumentasi yang rinci tentang penggunaan berkas preseed dalam sebuah lampiran. Ini juga mencakup sebuah sampel rinci dan komentar berkas, yang dapat berfungsi sebagai dasar untuk kustomisasi lokal. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"

msgid "Creating a Customized Boot Media"
msgstr "Membuat sebuah Media Boot Ubahan"

msgid "Knowing where to store the preseed file is all very well, but the location isn't everything: one must, one way or another, alter the installation boot media to change the boot parameters and add the preseed file."
msgstr "Mengetahui di mana untuk menyimpan berkas preseed itu baik, tapi lokasi bukan segalanya: kita harus, dengan satu cara atau lainnya, mengubah media boot instalasi untuk mengubah parameter boot dan menambahkan berkas preseed."

msgid "Booting From the Network"
msgstr "Boot dari Jaringan"

#, fuzzy
#| msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"
msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/ch04s05\" />"
msgstr "Ketika komputer di-boot dari jaringan, server mengirimkan elemen inisialisasi juga mendefinisikan parameter boot. Dengan demikian, perubahan perlu dibuat dalam konfigurasi PXE untuk server boot; lebih khusus lagi, dalam berkas konfigurasi <filename>/tftpboot/pxelinux.cfg/default</filename>. Pengaturan boot jaringan merupakan prasyarat; lihat Panduan Instalasi untuk rincian. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"

msgid "Preparing a Bootable USB Key"
msgstr "Mempersiapkan sebuah Flash Disk USB yang Dapat Di-boot"

msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>:"
msgstr "Setelah kunci USB disiapkan (lihat <xref linkend=\"sect.install-usb\" />), diperlukan beberapa operasi tambahan. Dengan asumsi kunci isi tersedia dalam <filename>/media/usbdisk/</filename>:"

msgid "copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>"
msgstr "salin berkas preseed ke <filename>/media/usbdisk/preseed.cfg</filename>"

msgid "edit <filename>/media/usbdisk/syslinux.cfg</filename> and add required boot parameters (see example below)."
msgstr "edit <filename>/media/usbdisk/syslinux.cfg</filename> dan tambahkan parameter boot yang diperlukan (lihat contoh di bawah)."

msgid "syslinux.cfg file and preseeding parameters"
msgstr "berkas syslinux.cfg dan parameter preseed"

msgid ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

msgid "Creating a CD-ROM Image"
msgstr "Membuat suatu Image CD-ROM"

msgid "<primary>debian-cd</primary>"
msgstr "<primary>debian-cd</primary>"

msgid "A USB key is a read-write media, so it was easy for us to add a file there and change a few parameters. In the CD-ROM case, the operation is more complex, since we need to regenerate a full ISO image. This task is handled by <emphasis role=\"pkg\">debian-cd</emphasis>, but this tool is rather awkward to use: it needs a local mirror, and it requires an understanding of all the options provided by <filename>/usr/share/debian-cd/CONF.sh</filename>; even then, <command>make</command> must be invoked several times. <filename>/usr/share/debian-cd/README</filename> is therefore a very recommended read."
msgstr "Kunci USB adalah media baca-tulis, jadi mudah bagi kita untuk menambahkan berkas ke sana dan mengubah beberapa parameter. Dalam kasus CD-ROM, operasi lebih kompleks, karena kita perlu melakukan regenerasi image ISO penuh. Tugas ini ditangani oleh <emphasis role=\"pkg\">debian-cd</emphasis>, tapi alat ini agak aneh untuk digunakan: dibutuhkan cermin lokal, dan itu memerlukan pemahaman tentang semua pilihan yang disediakan oleh <filename>/usr/share/debian-cd/CONF.sh</filename>; bahkan kemudian, <command>make</command> harus dipanggil beberapa kali. <filename>/usr/share/debian-cd/README</filename> sangat dianjurkan untuk dibaca."

msgid "Having said that, debian-cd always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific file is <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgstr "Setelah itu semua, debian-cd selalu beroperasi dengan cara yang sama: direktori \"image\" dengan isi eksak CD-ROM dibuat, lalu dikonversi ke berkas ISO dengan alat seperti <command>genisoimage</command>, <command>mkisofs</command>, atau <command>xorriso</command>. Direktori image diselesaikan setelah langkah <command>make image-trees</command> debian-cd. Pada saat itu, kita menyisipkan berkas preseed ke dalam direktori yang sesuai (biasanya <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR dan $CODENAME adalah parameter yang didefinisikan oleh berkas konfigurasi <filename>CONF.sh</filename>). CD-ROM menggunakan <command>isolinux</command> sebagai bootloader, dan berkas konfigurasinya harus diadaptasi dari apa yang dihasilkan oleh debian-cd, untuk memasukkan parameter boot yang diperlukan (berkas tepatnya adalah <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Kemudian proses \"normal\" dapat dilanjutkan, dan kita dapat meneruskan menghasilkan image ISO dengan <command>make image CD=1</command> (atau <command>make images</command> jika ada beberapa CD-ROM yang dihasilkan)."

msgid "Simple-CDD: The All-In-One Solution"
msgstr "Simple-CDD: Solusi Semua-Jadi-Satu"

msgid "<primary>simple-cdd</primary>"
msgstr "<primary>simple-cdd</primary>"

msgid "Simply using a preseed file is not enough to fulfill all the requirements that may appear for large deployments. Even though it is possible to execute a few scripts at the end of the normal installation process, the selection of the set of packages to install is still not quite flexible (basically, only “tasks” can be selected); more important, this only allows installing official Debian packages, and precludes locally-generated ones."
msgstr "Hanya menggunakan berkas preseed ini tidak cukup untuk memenuhi semua persyaratan yang mungkin muncul untuk penggelaran besar. Meskipun dimungkinkan untuk mengeksekusi beberapa skrip pada akhir proses penginstalan normal, pemilihan set paket yang akan diinstal ini tetap tidak cukup fleksibel (pada dasarnya, hanya \"task\" yang dapat dipilih); lebih penting, ini hanya memungkinkan menginstal paket-paket Debian yang resmi, dan mencegah dipasangnya yang dihasilkan secara lokal."

msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what Simple-CDD (in the <emphasis role=\"pkg\">simple-cdd</emphasis> package) does."
msgstr "Di sisi lain, debian-cd mampu mengintegrasikan paket eksternal, dan debian-installer dapat diperluas dengan memasukkan langkah-langkah baru dalam proses instalasi. Dengan menggabungkan kemampuan-kemampuan ini, seharusnya mungkin untuk menciptakan sebuah installer yang disesuaikan yang memenuhi kebutuhan kita; itu bahkan dapat mengkonfigurasi beberapa layanan setelah membuka paket yang diperlukan. Untungnya, hal ini tidak hanya sebuah hipotesis, karena ini adalah persis apa yang dilakukan oleh Simple-CDD (dalam paket <emphasis role=\"pkg\">simple-cdd</emphasis>)."

msgid "The purpose of Simple-CDD is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgstr "Tujuan Simple-CDD adalah untuk memungkinkan orang agar dengan mudah membuat distribusi yang berasal dari Debian, dengan memilih subset dari paket yang tersedia, memprakonfigurasi mereka dengan Debconf, menambahkan perangkat lunak khusus, dan menjalankan skrip-skrip ubahan pada akhir proses instalasi. Ini sesuai dengan filosofi \"sistem operasi universal\", karena siapa pun dapat mengadaptasi ke kebutuhan mereka sendiri."

msgid "Creating Profiles"
msgstr "Menciptakan Profil"

msgid "Simple-CDD defines “profiles” that match the FAI “classes” concept, and a machine can have several profiles (determined at installation time). A profile is defined by a set of <filename>profiles/<replaceable>profile</replaceable>.*</filename> files:"
msgstr "Simple-CDD mendefinisikan \"profil\" yang sesuai dengan konsep \"kelas\" FAI, dan sebuah mesin dapat memiliki beberapa profil (yang ditentukan pada saat instalasi). Profil didefinisikan oleh satu set berkas <filename>profiles/<replaceable>profil</replaceable>.*</filename>:"

msgid "the <filename>.description</filename> file contains a one-line description for the profile;"
msgstr "berkas <filename>.description</filename> berisi satu baris deskripsi untuk profil;"

msgid "the <filename>.packages</filename> file lists packages that will automatically be installed if the profile is selected;"
msgstr "berkas <filename>.packages</filename> berisi daftar paket yang akan secara otomatis diinstal jika profil dipilih;"

msgid "the <filename>.downloads</filename> file lists packages that will be stored onto the installation media, but not necessarily installed;"
msgstr "berkas <filename>.downloads</filename> berisi daftar paket yang akan disimpan ke media instalasi, tetapi tidak harus diinstal;"

msgid "the <filename>.preseed</filename> file contains preseeding information for Debconf questions (for the installer and/or for packages);"
msgstr "berkas <filename>.preseed</filename> berisi informasi preseed untuk pertanyaan Debconf (untuk installer dan atau paket);"

msgid "the <filename>.postinst</filename> file contains a script that will be run at the end of the installation process;"
msgstr "berkas <filename>.postinst</filename> berisi skrip yang akan dijalankan pada akhir proses instalasi;"

msgid "lastly, the <filename>.conf</filename> file allows changing some Simple-CDD parameters based on the profiles to be included in an image."
msgstr "terakhir, berkas <filename>.conf</filename> memungkinkan mengubah beberapa parameter Simple-CDD berdasarkan profil yang akan dimasukkan ke dalam image."

msgid "The <literal>default</literal> profile has a particular role, since it is always selected; it contains the bare minimum required for Simple-CDD to work. The only thing that is usually customized in this profile is the <literal>simple-cdd/profiles</literal> preseed parameter: this allows avoiding the question, introduced by Simple-CDD, about what profiles to install."
msgstr "Profil <literal>default</literal> memiliki peran tertentu, karena selalu dipilih; ini berisi minimal yang diperlukan oleh Simple-CDD untuk bekerja. Satu-satunya hal yang biasanya disesuaikan dalam profil ini adalah parameter preseed <literal>simple-cdd/profiles</literal>: hal ini memungkinkan menghindari pertanyaan, diperkenalkan oleh Simple-CDD, tentang profil apa yang akan dipasang."

msgid "Note also that the commands will need to be invoked from the parent directory of the <filename>profiles</filename> directory."
msgstr "Perhatikan juga bahwa perintah akan perlu dijalankan dari direktori induk direktori <filename>profil</filename>."

msgid "Configuring and Using <command>build-simple-cdd</command>"
msgstr "Mengkonfigurasi dan Menggunakan <command>build-simple-cdd</command>"

msgid "<primary><command>build-simple-cdd</command></primary>"
msgstr "<primary><command>build-simple-cdd</command></primary>"

msgid "<emphasis>QUICK LOOK</emphasis> Detailed configuration file"
msgstr "<emphasis>LIHAT SEKILAS</emphasis> Berkas konfigurasi rinci"

msgid "An example of a Simple-CDD configuration file, with all possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). This can be used as a starting point when creating a custom configuration file."
msgstr "Contoh berkas konfigurasi Simple-CDD, dengan semua parameter yang mungkin, disertakan dalam paket (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). Ini dapat digunakan sebagai titik awal ketika membuat berkas konfigurasi ubahan."

msgid "Simple-CDD requires many parameters to operate fully. They will most often be gathered in a configuration file, which <command>build-simple-cdd</command> can be pointed at with the <literal>--conf</literal> option, but they can also be specified via dedicated parameters given to <command>build-simple-cdd</command>. Here is an overview of how this command behaves, and how its parameters are used:"
msgstr "Simple-CDD memerlukan banyak parameter untuk beroperasi secara penuh. Mereka akan paling sering dikumpulkan dalam berkas konfigurasi, yang dapat diarahkan ke <command>build-simple-cdd</command> dengan opsi <literal>--conf</literal>, tetapi mereka dapat juga ditentukan melalui parameter khusus yang diberikan kepada <command>build-simple-cdd </command>. Berikut ini adalah gambaran bagaimana perintah ini berperilaku, dan bagaimana parameternya digunakan:"

msgid "the <literal>profiles</literal> parameter lists the profiles that will be included on the generated CD-ROM image;"
msgstr "parameter <literal>profil</literal> memuat daftar profil yang akan disertakan pada CD-ROM image yang dihasilkan;"

msgid "based on the list of required packages, Simple-CDD downloads the appropriate files from the server mentioned in <literal>server</literal>, and gathers them into a partial mirror (which will later be given to debian-cd);"
msgstr "berdasarkan daftar paket yang diperlukan, Simple-CDD mengunduh berkas-berkas yang sesuai dari server yang disebutkan di <literal>server</literal>, dan mengumpulkan mereka menjadi cermin parsial (yang akan kemudian diberikan kepada debian-cd);"

msgid "the custom packages mentioned in <literal>local_packages</literal> are also integrated into this local mirror;"
msgstr "paket ubahan yang dicantumkan dalam <literal>local_packages</literal> juga diintegrasikan ke dalam cermin lokal ini;"

msgid "debian-cd is then executed (within a default location that can be configured with the <literal>debian_cd_dir</literal> variable), with the list of packages to integrate;"
msgstr "debian-cd kemudian dijalankan (dalam lokasi baku yang dapat dikonfigurasi dengan variabel <literal>debian_cd_dir</literal>), dengan daftar paket untuk diintegrasikan;"

msgid "once debian-cd has prepared its directory, Simple-CDD applies some changes to this directory:"
msgstr "setelah debian-cd menyiapkan direktorinya, Simple-CDD menerapkan beberapa perubahan ke direktori ini:"

msgid "files containing the profiles are added in a <filename>simple-cdd</filename> subdirectory (that will end up on the CD-ROM);"
msgstr "berkas yang berisi profil ditambahkan dalam subdirektori <filename>simple-cdd</filename> (yang akan berakhir pada CD-ROM);"

msgid "other files listed in the <literal>all_extras</literal> parameter are also added;"
msgstr "berkas lain yang tercantum dalam <literal>all_extras</literal> parameter juga ditambahkan;"

msgid "the boot parameters are adjusted so as to enable the preseeding. Questions concerning language and country can be avoided if the required information is stored in the <literal>language</literal> and <literal>country</literal> variables."
msgstr "parameter boot disesuaikan sehingga memungkinkan preseed. Pertanyaan mengenai bahasa dan negara dapat dihindari jika informasi yang diperlukan disimpan dalam variabel <literal>language</literal> dan <literal>country</literal>."

msgid "debian-cd then generates the final ISO image."
msgstr "debian-cd kemudian menghasilkan image ISO akhir."

msgid "Generating an ISO Image"
msgstr "Menghasilkan suatu Image ISO"

#, fuzzy
#| msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-8.0-amd64-CD-1.iso</filename>."
msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-10-amd64-CD-1.iso</filename>."
msgstr "Setelah kita menulis berkas konfigurasi dan mendefinisikan profil, langkah yang tersisa adalah untuk menjalankan <command>build-simple-cdd --conf simple-cdd.conf</command>. Setelah beberapa menit, kita mendapatkan image yang diperlukan di <filename>images/debian-8.0-amd64-CD-1.iso</filename>."

msgid "Monitoring is a generic term, and the various involved activities have several goals: on the one hand, following usage of the resources provided by a machine allows anticipating saturation and the subsequent required upgrades; on the other hand, alerting the administrator as soon as a service is unavailable or not working properly means that the problems that do happen can be fixed sooner."
msgstr "Pemantauan adalah istilah umum, dan berbagai kegiatan yang dilibatkan memiliki beberapa tujuan: di satu sisi, mengikuti penggunaan sumber daya yang disediakan oleh sebuah mesin memungkinkan mengantisipasi saturasi dan peningkatan berikutnya yang diperlukan; di sisi lain, memperingatkan administrator segera setelah layanan ini tidak tersedia atau tidak berfungsi sebagaimana mestinya berarti bahwa masalah yang terjadi dapat lebih cepat diperbaiki."

msgid "<emphasis>Munin</emphasis> covers the first area, by displaying graphical charts for historical values of a number of parameters (used RAM, occupied disk space, processor load, network traffic, Apache/MySQL load, and so on). <emphasis>Nagios</emphasis> covers the second area, by regularly checking that the services are working and available, and sending alerts through the appropriate channels (e-mails, text messages, and so on). Both have a modular design, which makes it easy to create new plug-ins to monitor specific parameters or services."
msgstr "<emphasis>Munin</emphasis> mencakup area pertama dengan menampilkan bagan grafis untuk riwayat nilai dari sejumlah parameter (RAM yang digunakan, ruang disk yang ditempati, beban prosesor, trafik jaringan, beban Apache/MySQL, dan seterusnya). <emphasis>Nagios</emphasis> mencakup daerah yang kedua, dengan secara teratur memeriksa bahwa layanan bekerja dan tersedia, dan mengirimkan peringatan melalui saluran yang tepat (surel, pesan teks, dan sebagainya). Keduanya memiliki desain modular, yang membuat mudah untuk menciptakan plug-in baru untuk memantau parameter atau layanan tertentu."

msgid "<emphasis>ALTERNATIVE</emphasis> Zabbix, an integrated monitoring tool"
msgstr "<emphasis>ALTERNATIF</emphasis> Zabbix, alat pemantauan terintegrasi"

msgid "<primary>Zabbix</primary>"
msgstr "<primary>Zabbix</primary>"

msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"https://www.zabbix.com/\" />"
msgstr "Meskipun Munin dan Nagios sangat umum digunakan, bukan hanya mereka pemain di bidang pemantauan, dan masing-masing hanya menangani setengah dari tugas (menggambar grafik di satu sisi, memperingatkan di yang lain). Zabbix, di sisi lain, mengintegrasikan kedua bagian dari pengawasan; ini juga memiliki antarmuka web untuk mengkonfigurasi aspek yang paling umum. Itu telah berkembang pesat selama beberapa tahun terakhir, dan sekarang dapat dianggap sebagai pesaing yang layak. Di server pemantauan, Anda akan memasang <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (atau <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), mungkin bersama dengan <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> agar memiliki antarmuka web. Pada host yang dipantau Anda akan memasang <emphasis role=\"pkg\">zabbix-agent</emphasis> yang mengumpan data kembali ke server. <ulink type=\"block\" url=\"https://www.zabbix.com/\" />"

msgid "<emphasis>ALTERNATIVE</emphasis> Icinga, a Nagios fork"
msgstr "<emphasis>ALTERNATIF</emphasis> Icinga, suatu fork Nagios"

msgid "<primary>Icinga</primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"
msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"https://www.icinga.org/\" />"
msgstr "Didorong oleh keragaman dalam pendapat mengenai model pembangunan untuk Nagios (yang dikendalikan oleh perusahaan), sejumlah pengembang mem-fork Nagios dan menggunakan Icinga sebagai nama baru mereka. Icinga masih kompatibel — sejauh ini — dengan konfigurasi dan plugin Nagios, tetapi juga menambahkan fitur. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"

msgid "Setting Up Munin"
msgstr "Menyiapkan Munin"

msgid "<primary>Munin</primary>"
msgstr "<primary>Munin</primary>"

msgid "The purpose of Munin is to monitor many machines; therefore, it quite naturally uses a client/server architecture. The central host — the grapher — collects data from all the monitored hosts, and generates historical graphs."
msgstr "Tujuan dari Munin adalah untuk memantau banyak mesin; oleh karena itu, cukup alami menggunakan arsitektur klien server. Host pusat — grapher — mengumpulkan data dari semua host yang dipantau, dan menghasilkan grafik historis."

msgid "Configuring Hosts To Monitor"
msgstr "Mengkonfigurasi Host yang Akan Dimonitor"

msgid "The first step is to install the <emphasis role=\"pkg\">munin-node</emphasis> package. The daemon installed by this package listens on port 4949 and sends back the data collected by all the active plugins. Each plugin is a simple program returning a description of the collected data as well as the latest measured value. Plugins are stored in <filename>/usr/share/munin/plugins/</filename>, but only those with a symbolic link in <filename>/etc/munin/plugins/</filename> are really used."
msgstr "Langkah pertama adalah untuk menginstal paket <emphasis role=\"pkg\">munin-node</emphasis>. Daemon yang diinstal oleh paket ini mendengarkan pada port 4949 dan mengirimkan kembali data yang dikumpulkan oleh semua plugin aktif. Masing-masing plugin adalah program sederhana yang mengembalikan deskripsi dari data yang dikumpulkan serta nilai yang terakhir diukur. Plugin disimpan dalam <filename>/usr/share/munin/plugins/</filename>, tetapi hanya yang memiliki symlink di <filename>/etc/munin/plugins/</filename> yang benar-benar digunakan."

#, fuzzy
#| msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the <ulink url=\"http://gallery.munin-monitoring.org\">Plugin Gallery</ulink> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the Plugin Gallery<footnote><para><ulink type=\"block\" url=\"http://gallery.munin-monitoring.org\" /></para></footnote> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgstr "Ketika paket terinstal, set plugin aktif ditentukan berdasarkan perangkat lunak yang tersedia dan konfigurasi host saat ini. Namun, konfigurasi otomatis ini tergantung pada fitur yang harus disediakan oleh setiap plugin, dan biasanya ide yang baik untuk meninjau dan menala hasil dengan tangan. Meramban <ulink url=\"http://gallery.munin-monitoring.org\">Galeri Plugin</ulink> dapat menarik meskipun tidak semua plugin memiliki dokumentasi yang komprehensif. Namun, semua plugin adalah skrip dan kebanyakan cukp sederhana dan dikomentari dengan baik. Karena itu meramban <filename>/etc/munin/plugins/</filename> adalah cara yang baik untuk mendapatkan ide tentang apa masing-masing plugin tersebut dan menentukan mana yang harus dihapus. Demikian pula, memfungsikan plugin menarik yang ditemukan di <filename>/usr/share/munin/plugins/</filename> adalah sekadar menyiapkan sebuah taut simbolik dengan <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Perhatikan bahwa ketika nama plugin berakhir dengan tanda garis bawah \"_\", plugin memerlukan parameter. Parameter ini harus disimpan dalam nama taut simbolik; Misalnya, plugin \"if_\" harus diaktifkan dengan taut simbolik <filename>if_eth0</filename>, dan itu akan memantau lalu lintas jaringan pada antarmuka eth0."

#, fuzzy
#| msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>service munin-node restart</command>."
msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>systemctl restart munin-node</command>."
msgstr "Setelah semua plugin yang benar diatur, konfigurasi daemon harus diperbarui untuk menggambarkan kontrol akses untuk data yang dikumpulkan. Ini melibatkan direktif <literal>allow</literal> dalam berkas <filename>/etc/munin/munin-node.conf</filename>. Konfigurasi default adalah <literal>allow ^127\\.0\\.0\\.1$</literal>, dan hanya mengizinkan akses ke host lokal. Administrator biasanya akan menambahkan baris serupa yang berisi alamat IP host grapher, kemudian menjalankan ulang daemon dengan <command>service munin-node restart</command>."

msgid "<emphasis>GOING FURTHER</emphasis> Creating local plugins"
msgstr "<emphasis>LEBIH JAUH</emphasis> Membuat plugin lokal"

#, fuzzy
#| msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"
msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://guide.munin-monitoring.org/en/latest/plugin/writing.html\" />"
msgstr "Munin termasuk dokumentasi yang rinci tentang bagaimana plugin seharusnya bersikap, dan bagaimana untuk mengembangkan plugin baru. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"

msgid "A plugin is best tested when run in the same conditions as it would be when triggered by munin-node; this can be simulated by running <command>munin-run <replaceable>plugin</replaceable></command> as root. A potential second parameter given to this command (such as <literal>config</literal>) is passed to the plugin as a parameter."
msgstr "Sebuah plugin terbaik diuji ketika dijalankan dalam kondisi yang sama dengan ketika dipicu oleh munin-node; ini bisa disimulasikan dengan menjalankan <command>munin-run<replaceable>plugin</replaceable></command> sebagai root. Parameter potensial kedua yang diberikan kepada perintah ini (seperti misalnya <literal>config</literal>) dilewatkan ke plugin sebagai parameter."

msgid "When a plugin is invoked with the <literal>config</literal> parameter, it must describe itself by returning a set of fields:"
msgstr "Ketika sebuah plugin dipanggil dengan parameter <literal>config</literal>, itu harus menguraikan dirinya sendiri dengan mengembalikan sekumpulan ruas:"

msgid ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"

#, fuzzy
#| msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"https://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgstr "Berbagai ruas yang tersedia dijelaskan oleh \"Referensi plugin\" yang tersedia sebagai bagian dari \"Panduan Munin\". <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"

msgid "When invoked without a parameter, the plugin simply returns the last measured values; for instance, executing <command>sudo munin-run load</command> could return <literal>load.value 0.12</literal>."
msgstr "Ketika dipanggil tanpa parameter, plugin hanya mengembalikan nilai yang terakhir diukur; misalnya, menjalankan <command>sudo munin-run load</command> bisa mengembalikan <literal>load.value 0.12</literal>."

msgid "Finally, when a plugin is invoked with the <literal>autoconf</literal> parameter, it should return “yes” (and a 0 exit status) or “no” (with a 1 exit status) according to whether the plugin should be enabled on this host."
msgstr "Akhirnya, ketika sebuah plugin dipanggil dengan parameter <literal>autoconf</literal>, itu harus mengembalikan \"yes\" (dan status keluar 0) atau \"no\" (dengan status keluar 1) sesuai dengan apakah plugin harus diaktifkan pada host ini."

msgid "Configuring the Grapher"
msgstr "Mengkonfigurasi Pembuat Grafik"

msgid "The “grapher” is simply the computer that aggregates the data and generates the corresponding graphs. The required software is in the <emphasis role=\"pkg\">munin</emphasis> package. The standard configuration runs <command>munin-cron</command> (once every 5 minutes), which gathers data from all the hosts listed in <filename>/etc/munin/munin.conf</filename> (only the local host is listed by default), saves the historical data in RRD files (<emphasis>Round Robin Database</emphasis>, a file format designed to store data varying in time) stored under <filename>/var/lib/munin/</filename> and generates an HTML page with the graphs in <filename>/var/cache/munin/www/</filename>."
msgstr "\"grapher\" adalah sekadar komputer yang mengumpulkan data dan menghasilkan grafik yang sesuai. Perangkat lunak yang diperlukan adalah dalam paket <emphasis role=\"pkg\">munin</emphasis>. Konfigurasi standar menjalankan <command>munin-cron</command> (sekali setiap 5 menit), yang mengumpulkan data dari semua host yang tercantum dalam <filename>/etc/munin/munin.conf</filename> (hanya host lokal yang tercantum secara default), menyimpan data historis di berkas RRD (<emphasis>Round Robin Database</emphasis>, sebuah format berkas yang dirancang untuk menyimpan data yang bervariasi dalam waktu) disimpan di bawah <filename>/var/lib/munin/</filename> dan menghasilkan halaman HTML dengan grafik di <filename>/var/cache/munin/www/</filename>."

msgid "All monitored machines must therefore be listed in the <filename>/etc/munin/munin.conf</filename> configuration file. Each machine is listed as a full section with a name matching the machine and at least an <literal>address</literal> entry giving the corresponding IP address."
msgstr "Semua mesin yang dipantau oleh karena itu harus tercantum dalam berkas konfigurasi <filename>/etc/munin/munin.conf</filename>. Setiap mesin didaftar sebagai bagian penuh dengan suatu nama yang cocok dengan mesin dan setidaknya entri <literal>address</literal> yang memberikan alamat IP yang sesuai."

msgid ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"
msgstr ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"

msgid "Sections can be more complex, and describe extra graphs that could be created by combining data coming from several machines. The samples provided in the configuration file are good starting points for customization."
msgstr "Seksi dapat menjadi lebih kompleks, dan menggambarkan grafik tambahan yang dapat dibuat dengan menggabungkan data yang berasal dari beberapa mesin. Sampel yang disediakan di berkas konfigurasi adalah titik awal yang baik untuk kustomisasi."

msgid "The last step is to publish the generated pages; this involves configuring a web server so that the contents of <filename>/var/cache/munin/www/</filename> are made available on a website. Access to this website will often be restricted, using either an authentication mechanism or IP-based access control. See <xref linkend=\"sect.http-web-server\" /> for the relevant details."
msgstr "Langkah terakhir adalah untuk mempublikasikan halaman yang dihasilkan; ini melibatkan mengkonfigurasi server web sehingga isi <filename>/var/cache/munin/www/</filename> menjadi tersedia di situs web. Akses ke situs web ini akan sangat dibatasi, menggunakan mekanisme otentikasi atau kontrol akses berbasis IP. Lihat <xref linkend=\"sect.http-web-server\" /> untuk rincian yang relevan."

msgid "Setting Up Nagios"
msgstr "Menyiapkan Nagios"

msgid "<primary>Nagios</primary>"
msgstr "<primary>Nagios</primary>"

msgid "Unlike Munin, Nagios does not necessarily require installing anything on the monitored hosts; most of the time, Nagios is used to check the availability of network services. For instance, Nagios can connect to a web server and check that a given web page can be obtained within a given time."
msgstr "Tidak seperti Munin, Nagios tidak selalu membutuhkan memasang apapun pada host yang dipantau; sebagian besar waktu, Nagios digunakan untuk memeriksa ketersediaan layanan jaringan. Sebagai contoh, Nagios dapat menyambung ke server web dan memeriksa bahwa suatu halaman web dapat diperoleh dalam selang waktu tertentu."

msgid "Installing"
msgstr "Memasang"

msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios4</emphasis> and <emphasis role=\"pkg\">monitoring-plugins</emphasis> packages. Installing the packages configures the web interface and the Apache server. The <literal>authz_groupfile</literal> and <literal>auth_digest</literal> Apache modules must be enabled, for that execute:"
msgstr ""

msgid ""
"<computeroutput># </computeroutput><userinput>a2enmod authz_groupfile</userinput>\n"
"<computeroutput>Considering dependency authz_core for authz_groupfile:\n"
"Module authz_core already enabled\n"
"Enabling module authz_groupfile.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>a2enmod auth_digest\n"
"Considering dependency authn_core for auth_digest:\n"
"Module authn_core already enabled\n"
"Enabling module auth_digest.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl restart apache2\n"
"</userinput>"
msgstr ""

msgid "Adding other users is a simple matter of inserting them in the <filename>/etc/nagios4/hdigest.users</filename> file."
msgstr ""

#, fuzzy
#| msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios3/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios4/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgstr "Mengarahkan peramban ke <literal>http://<replaceable>server</replaceable>/nagios3/</literal> menampilkan antarmuka web; secara khusus, perhatikan bahwa Nagios sudah memantau beberapa parameter mesin tempat dia berjalan. Namun, beberapa fitur interaktif seperti menambahkan komentar ke host tidak bekerja. Fitur ini dinonaktifkan dalam konfigurasi default untuk Nagios, yang sangat ketat untuk alasan keamanan."

#, fuzzy
#| msgid "As documented in <filename>/usr/share/doc/nagios3/README.Debian</filename>, enabling some features involves editing <filename>/etc/nagios3/nagios.cfg</filename> and setting its <literal>check_external_commands</literal> parameter to “1”. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgid "Enabling some features involves editing <filename>/etc/nagios4/nagios.cfg</filename>. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgstr "Sebagaimana didokumentasikan di <filename>/usr/share/doc/nagios3/README.Debian</filename>, memfungksikan beberapa fitur melibatkan mengedit <filename>/etc/nagios3/nagios.cfg</filename> dan menetapkan parameter <literal>check_external_commands</literal> ke \"1\". Kita juga perlu untuk mengatur izin menulis direktori yang digunakan oleh Nagios, dengan perintah seperti berikut:"

msgid ""
"<computeroutput># </computeroutput><userinput>systemctl stop nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios4/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl start nagios4\n"
"</userinput>"
msgstr "<computeroutput># </computeroutput><userinput>systemctl stop nagios4\n</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios4/rw\n</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios4\n</userinput><computeroutput># </computeroutput><userinput>systemctl start nagios4\n</userinput>"

msgid "Configuring"
msgstr "Mengkonfigurasi"

#, fuzzy
#| msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios3/nagios.cfg</filename>."
msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios4/nagios.cfg</filename>."
msgstr "Antarmuka web Nagios agak bagus, tetapi tidak memungkinkan konfigurasi, dan tidak bisa digunakan untuk menambah host dan layanan yang dipantau. Seluruh konfigurasi diatur melalui berkas-berkas yang dirujuk dalam berkas konfigurasi pusat, <filename>/etc/nagios3/nagios.cfg</filename>."

msgid "These files should not be dived into without some understanding of the Nagios concepts. The configuration lists objects of the following types:"
msgstr "Berkas-berkas ini tidak boleh diselami tanpa pemahaman konsep-konsep Nagios. Konfigurasi memuat daftar objek jenis berikut:"

msgid "a <emphasis>host</emphasis> is a machine to be monitored;"
msgstr "suatu <emphasis>host</emphasis> adalah mesin yang akan dimonitor;"

msgid "a <emphasis>hostgroup</emphasis> is a set of hosts that should be grouped together for display, or to factor some common configuration elements;"
msgstr "suatu <emphasis>hostgroup</emphasis> adalah seperangkat host yang harus dikelompokkan menjadi satu untuk ditampilkan, atau untuk memfaktorkan beberapa elemen konfigurasi umum;"

msgid "a <emphasis>service</emphasis> is a testable element related to a host or a host group. It will most often be a check for a network service, but it can also involve checking that some parameters are within an acceptable range (for instance, free disk space or processor load);"
msgstr "suatu <emphasis>layanan</emphasis> adalah elemen yang dapat diuji terkait dengan host atau grup host. Paling sering akan berupa pengujian layanan jaringan, tetapi dapat juga melibatkan pemeriksaan bahwa beberapa parameter ada dalam rentang yang dapat diterima (misalnya, sisa ruang disk atau beban prosesor);"

msgid "a <emphasis>servicegroup</emphasis> is a set of services that should be grouped together for display;"
msgstr "suatu <emphasis>servicegroup</emphasis> adalah satu set layanan yang harus dikumpulkan bersama-sama untuk ditampilkan;"

msgid "a <emphasis>contact</emphasis> is a person who can receive alerts;"
msgstr "suatu <emphasis>contact</emphasis> adalah orang yang dapat menerima pemberitahuan;"

msgid "a <emphasis>contactgroup</emphasis> is a set of such contacts;"
msgstr "suatu <emphasis>contactgroup</emphasis> adalah sekumpulan kontak tersebut;"

msgid "a <emphasis>timeperiod</emphasis> is a range of time during which some services have to be checked;"
msgstr "suatu <emphasis>timeperiod</emphasis> adalah rentang waktu saat beberapa layanan harus diperiksa;"

msgid "a <emphasis>command</emphasis> is the command line invoked to check a given service."
msgstr "suatu <emphasis>command</emphasis> adalah baris perintah yang dipanggil untuk memeriksa layanan yang diberikan."

msgid "According to its type, each object has a number of properties that can be customized. A full list would be too long to include, but the most important properties are the relations between the objects."
msgstr "Sesuai dengan jenisnya, setiap objek memiliki sejumlah properti yang dapat disesuaikan. Daftar lengkap akan terlalu panjang untuk disertakan, tapi properti yang paling penting adalah hubungan antar objek tersebut."

msgid "A <emphasis>service</emphasis> uses a <emphasis>command</emphasis> to check the state of a feature on a <emphasis>host</emphasis> (or a <emphasis>hostgroup</emphasis>) within a <emphasis>timeperiod</emphasis>. In case of a problem, Nagios sends an alert to all members of the <emphasis>contactgroup</emphasis> linked to the service. Each member is sent the alert according to the channel described in the matching <emphasis>contact</emphasis> object."
msgstr "Suatu <emphasis>service</emphasis> menggunakan <emphasis>command</emphasis> untuk memeriksa keadaan dari suatu fitur pada <emphasis>host</emphasis> (atau <emphasis>hostgroup</emphasis>) dalam <emphasis>timeperiod</emphasis>. Bila ada masalah, Nagios mengirimkan peringatan kepada semua anggota <emphasis>contactgroup</emphasis> yang terhubung ke layanan. Setiap anggota dikirim pesan peringatan menurut saluran yang dijelaskan dalam objek <emphasis>contact</emphasis> yang cocok."

#, fuzzy
#| msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios3/conf.d/</filename> are a good source of information on how they work."
msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios4/conf.d/</filename> are a good source of information on how they work."
msgstr "Suatu sistem warisan memungkinkan berbagi dengan mudah satu set properti ke banyak objek tanpa menggandakan informasi. Selain itu, konfigurasi awal mencakup sejumlah objek standar; dalam banyak kasus, mendefinisikan host baru, layanan, dan kontak cukup dengan menurunkan dari objek generik yang disediakan. Berkas-berkas dalam <filename>/etc/nagios3/conf.d/</filename> adalah sumber yang baik informasi tentang bagaimana mereka bekerja."

msgid "The Falcot Corp administrators use the following configuration:"
msgstr "Para administrator Falcot Corp menggunakan konfigurasi berikut:"

#, fuzzy
#| msgid "<filename>/etc/nagios3/conf.d/falcot.cfg</filename> file"
msgid "<filename>/etc/nagios4/conf.d/falcot.cfg</filename> file"
msgstr "berkas <filename>/etc/nagios3/conf.d/falcot.cfg</filename>"

msgid ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"
msgstr ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"

#, fuzzy
#| msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>."
msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios4/conf.d/services_nagios2.cfg</filename>."
msgstr "Berkas konfigurasi ini menjelaskan dua host yang dipantau. Yang pertama adalah server web, dan pemeriksaan dibuat pada port HTTP (80) dan HTTP aman (443). Nagios juga memeriksa bahwa server SMTP berjalan pada port 25. Host kedua adalah server FTP, dan pemeriksaan termasuk memastikan bahwa jawaban datang dalam 20 detik. Penundaan lebih dari ini, <emphasis>peringatan</emphasis> disebarkan; lebih dari 30 detik, pesan waspada dianggap kritis. Antarmuka web Nagios juga menunjukkan bahwa layanan SSH dipantau: ini datang dari host milik hostgroup <literal>ssh-servers</literal>. Layanan standar yang cocok didefinisikan dalam <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>."

msgid "Note the use of inheritance: an object is made to inherit from another object with the “use <replaceable>parent-name</replaceable>”. The parent object must be identifiable, which requires giving it a “name <replaceable>identifier</replaceable>” property. If the parent object is not meant to be a real object, but only to serve as a parent, giving it a “register 0” property tells Nagios not to consider it, and therefore to ignore the lack of some parameters that would otherwise be required."
msgstr "Perhatikan penggunaan warisan: suatu objek dibuat untuk mewarisi dari objek lain dengan \"menggunakan <replaceable>nama orang tua\"</replaceable>. Objek induk harus dapat diidentifikasi, yang memerlukan memberikan sebuah properti \"nama <replaceable>pengenal</replaceable>\". Jika objek induk tidak dimaksudkan untuk menjadi sebuah objek yang nyata, tetapi hanya untuk melayani sebagai orang tua, memberinya sebuah properti \"register 0\" memberitahu Nagios untuk tidak untuk mempertimbangkan itu, dan karena itu untuk mengabaikan kekurangan beberapa parameter yang bila tidak demikian akan diperlukan."

msgid "<emphasis>DOCUMENTATION</emphasis> List of object properties"
msgstr "<emphasis>DOKUMENTASI</emphasis> Daftar properti obyek"

#, fuzzy
#| msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation provided by the <emphasis role=\"pkg\">nagios3-doc</emphasis> package. This documentation is directly accessible from the web interface, with the “Documentation” link in the top left corner. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation hosted on <ulink url=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/index.html\" />. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgstr "Pemahaman yang lebih mendalam dari berbagai cara di mana Nagios dapat dikonfigurasi dapat diperoleh dari dokumentasi yang disediakan oleh paket <emphasis role=\"pkg\">nagios3-doc</emphasis>. Dokumentasi ini secara langsung dapat diakses dari antarmuka web, dengan taut \"Dokumentasi\" di sudut kiri atas. Ini mencakup daftar semua jenis objek, dengan semua properti yang dapat mereka miliki. Ini juga menjelaskan bagaimana membuat plugin baru."

msgid "<emphasis>GOING FURTHER</emphasis> Remote tests with NRPE"
msgstr "<emphasis>LEBIH JAUH</emphasis> Uji jarak jauh dengan NRPE"

msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <emphasis>check_nrpe</emphasis> command."
msgstr "Banyak plugin Nagios yang memungkinkan memeriksa beberapa parameter lokal ke host; jika banyak mesin memerlukan pemeriksaan sementara suatu instalasi pusat mengumpulkan mereka, plugin NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) perlu digelar. Paket <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> perlu dipasang pada server Nagios, dan <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> pada host tempat tes lokal perlu dijalankan. Yang terakhir mendapat konfigurasinya dari <filename>/etc/nagios/nrpe.cfg</filename>. Berkas ini harus memuat daftar tes yang dapat dimulai dari jarak jauh, dan alamat IP dari mesin-mesin yang diperbolehkan untuk memicu mereka. Di sisi Nagios, memfungsikan tes jarak jauh ini cukup dengan menambahkan layanan yang cocok menggunakan perintah baru <emphasis>check_nrpe</emphasis>."

#~ msgid ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"
#~ msgstr ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"

#~ msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> and <emphasis role=\"pkg\">nagios3-doc</emphasis> packages. Installing the packages configures the web interface and creates a first <literal>nagiosadmin</literal> user (for which it asks for a password). Adding other users is a simple matter of inserting them in the <filename>/etc/nagios3/htpasswd.users</filename> file with Apache's <command>htpasswd</command> command. If no Debconf question was displayed during installation, <command>dpkg-reconfigure nagios3-cgi</command> can be used to define the <literal>nagiosadmin</literal> password."
#~ msgstr "Langkah pertama dalam menyiapkan Nagios adalah untuk memasang paket <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis>, dan <emphasis role=\"pkg\">nagios3-doc</emphasis>. Memasang paket akan mengkonfigurasi antarmuka web dan menciptakan pengguna pertama <literal>nagiosadmin</literal> (yang diminta passwordnya). Menambahkan pengguna lain cukup dengan memasukkan mereka ke dalam berkas <filename>/etc/nagios3/htpasswd.users</filename> dengan perintah <command>htpasswd</command> Apache. Jika tidak ada pertanyaan Debconf yang ditampilkan selama instalasi, <command>dpkg-reconfigure nagios3-cgi</command> dapat digunakan untuk menentukan password <literal>nagiosadmin</literal>."

#~ msgid ""
#~ "[main]\n"
#~ "reposdir=/var/tmp/yum-bootstrap/repos.d\n"
#~ "pluginconfpath=/var/tmp/yum-bootstrap/pluginconf.d\n"
#~ "cachedir=/var/cache/yum\n"
#~ "installroot=/path/to/destination/domU/install\n"
#~ "exclude=$exclude\n"
#~ "keepcache=1\n"
#~ "#debuglevel=4  \n"
#~ "#errorlevel=4\n"
#~ "pkgpolicy=newest\n"
#~ "distroverpkg=centos-release\n"
#~ "tolerant=1\n"
#~ "exactarch=1\n"
#~ "obsoletes=1\n"
#~ "gpgcheck=1\n"
#~ "plugins=1\n"
#~ "metadata_expire=1800"
#~ msgstr ""
#~ "[main]\n"
#~ "reposdir=/var/tmp/yum-bootstrap/repos.d\n"
#~ "pluginconfpath=/var/tmp/yum-bootstrap/pluginconf.d\n"
#~ "cachedir=/var/cache/yum\n"
#~ "installroot=/path/to/destination/domU/install\n"
#~ "exclude=$exclude\n"
#~ "keepcache=1\n"
#~ "#debuglevel=4  \n"
#~ "#errorlevel=4\n"
#~ "pkgpolicy=newest\n"
#~ "distroverpkg=centos-release\n"
#~ "tolerant=1\n"
#~ "exactarch=1\n"
#~ "obsoletes=1\n"
#~ "gpgcheck=1\n"
#~ "plugins=1\n"
#~ "metadata_expire=1800"

#~ msgid ""
#~ "[main]\n"
#~ "enabled=1\n"
#~ "tokeep=5"
#~ msgstr ""
#~ "[main]\n"
#~ "enabled=1\n"
#~ "tokeep=5\n"

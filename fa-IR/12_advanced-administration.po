#
# Navid Emami <me@novid.name>, 2017.
#
msgid ""
msgstr ""
"Project-Id-Version: 0\n"
"POT-Creation-Date: 2022-07-30 18:23+0200\n"
"PO-Revision-Date: 2017-10-16 16:17+0330\n"
"Last-Translator: Navid Emami <me@novid.name>\n"
"Language-Team: Persian <>\n"
"Language: fa-IR\n"
"MIME-Version: 1.0\n"
"Content-Type: application/x-publican; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: Gtranslator 2.91.7\n"

msgid "RAID"
msgstr "RAID"

msgid "LVM"
msgstr "LVM"

msgid "FAI"
msgstr "FAI"

msgid "Preseeding"
msgstr "Preseeding"

msgid "Monitoring"
msgstr "مانیتورینگ"

msgid "Virtualization"
msgstr "مجازی‌سازی"

msgid "Xen"
msgstr "Xen"

msgid "LXC"
msgstr "LXC"

msgid "Advanced Administration"
msgstr "مدیریت پیشرفته"

msgid "This chapter revisits some aspects we already described, with a different perspective: instead of installing one single computer, we will study mass-deployment systems; instead of creating RAID or LVM volumes at install time, we'll learn to do it by hand so we can later revise our initial choices. Finally, we will discuss monitoring tools and virtualization techniques. As a consequence, this chapter is more particularly targeting professional administrators, and focuses somewhat less on individuals responsible for their home network."
msgstr "این فصل به مرور مفاهیمی که تاکنون به آن‌ها پرداخته‌ایم می‌پردازد، اما با رویکردی متفاوت: بجای نصب یک رایانه تکی، به مطالعه نصب-انبوه روی سیستم‌ها می‌پردازیم؛ بجای نصب RAID یا LVM در زمان نصب، اینکار را به صورت دستی در زمان دیگری که نیاز داشته باشیم انجام می‌دهیم. در نهایت، درباره ابزارهای مانیتورینگ و تکنیک‌های مجازی‌سازی صحبت خواهیم کرد. به همین دلیل، مخاطب این فصل روی مدیرسیستم‌های حرفه‌ای تمرکز دارد و به جنبه‌های شخصی و انفرادی این کار کمتر توجه می‌کند."

msgid "RAID and LVM"
msgstr "RAID و LVM"

msgid "<primary>RAID</primary>"
msgstr "<primary>RAID</primary>"

msgid "<primary>LVM</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Logical Volume Manager</primary><see>LVM</see>"
msgstr "<primary>Logical Volume Manager</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>logical volume</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>raid volume</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>filesystem</primary><secondary>redundancy</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive reinstallation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive re-installation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgstr "قسمت <xref linkend=\"installation\" /> از نقطه نظر فرآیند نصب به بررسی این فناوری‌ها و اینکه چگونه می‌توان از ابتدا آن‌ها را به سادگی مدیریت کرد، پرداخت. پس از نصب اولیه، یک مدیرسیستم باید بتواند نیازهای رو به افزایش فضای دیسک را بدون راه‌اندازی فرآیند نصب برطرف کند. برای این منظور آن‌ها باید ابزارهای مورد نیاز برای تغییرات RAID و LVM را درک کنند."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>management</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former secures the data against hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former ensures the security and availability of the data in case of hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgstr "RAID و LVM فناوری‌هایی هستند که دستگاه‌های متصل را به شیوه‌ای انتزاعی از همتای فیزیکی خود جدا می‌سازند (درایوهای هارد-دیسک یا پارتیشن‌ها)؛ اولی با استفاده از تکنیک تکرار برای ایمن‌سازی داده در صورت بروز مشکل سخت‌افزاری و دومی با استفاده از منعطف‌ساختن مدیریت دستگاه‌ها جدا از اندازه واقعی آن‌ها کار می‌کنند. در هر دو مورد، سیستم با دستگاه‌های بلاک-محور جدید روبه‌رو می‌شود که می‌تواند برای ایجاد فایل‌سیستم‌ها یا فضای swap مورد استفاده قرار گیرد، بدون اینکه ضرورتا به یک دیسک فیزیکی نگاشت شوند. RAID و LVM از پیش‌زمینه‌های متفاوتی می‌آیند، اما عملکرد آن‌ها می‌تواند با یکدیگر تداخل داشته باشد که به همین دلیل همراه با هم نام برده می‌شوند."

msgid "<emphasis>PERSPECTIVE</emphasis> Btrfs combines LVM and RAID"
msgstr "<emphasis>چشم‌انداز</emphasis> ترکیب RAID و LVM در Btrfs"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>mount</command></primary><secondary>Btrfs</secondary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary>Btrfs</primary>"
msgstr "<primary>preseed</primary>"

#, fuzzy
#| msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a new filesystem, initially developed at Oracle, that purports to combine the featuresets of LVM and RAID and much more. It is mostly functional, and although it is still tagged “experimental” because its development is incomplete (some features aren't implemented yet), it has already seen some use in production environments. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"
msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a filesystem, initially developed at Oracle, that purports to combine the feature sets of LVM and RAID and much more. <ulink type=\"block\" url=\"https://btrfs.wiki.kernel.org/\" />"
msgstr "از آنجا که RAID و LVM دو زیرسیستم کاملا جدا از یکدیگر در کرنل هستند که در مورد دستگاه‌های بلاک-محور و فایل‌سیستم‌های آنان بکار می‌روند، <emphasis>btrfs</emphasis> فایل‌سیستم جدیدی بحساب می‌آید، که در ابتدا توسط اوراکل توسعه یافت و قصد ترکیب ویژگی‌های RAID و LVM را دارد. با اینکه هنوز برچسب “آزمایشی” روی آن وجود دارد و برخی ویژگی‌هایش به صورت کامل پیاده‌سازی نشده است، در محیط‌های واقعی کاربردهای گوناگونی دارد. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"

msgid "Among the noteworthy features are the ability to take a snapshot of a filesystem tree at any point in time. This snapshot copy doesn't initially use any disk space, the data only being duplicated when one of the copies is modified. The filesystem also handles transparent compression of files, and checksums ensure the integrity of all stored data."
msgstr "از میان ویژگی‌های آن، توانایی ایجاد snapshot از فایل‌سیستم در هر لحظه از زمان وجود دارد. این رونوشت از snapshot به صورت اولیه هیچ فضایی را اشغال نمی‌کند، داده زمانی تکرار می‌شود که یکی از این رونوشت‌ها تغییر کند. فایل‌سیستم همچنین از فشرده‌سازی شفاف فایل‌ها پشتیبانی و از checksum برای اطمینان از جامعیت داده‌های ذخیره شده استفاده می‌کند."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>filesystem</primary><secondary>snapshot</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "In both the RAID and LVM cases, the kernel provides a block device file, similar to the ones corresponding to a hard disk drive or a partition. When an application, or another part of the kernel, requires access to a block of such a device, the appropriate subsystem routes the block to the relevant physical layer. Depending on the configuration, this block can be stored on one or several physical disks, and its physical location may not be directly correlated to the location of the block in the logical device."
msgstr "در هر دو مورد RAID  و LVM، کرنل یک دستگاه بلاک-محور فراهم می‌کند، مشابه آن‌هایی که برای درایو هارد دیسک یا یک پارتیشن بکار می‌رود. زمانی که یک برنامه، یا قسمتی از کرنل، درخواست دسترسی به چنین دستگاهی را داشته باشد، زیرسیستم مرتبط با آن فرآیند مسیریابی بلاک به لایه فیزیکی مرتبط را برقرار می‌کند. با توجه به پیکربندی، این بلاک می‌تواند در یک یا چند دیسک فیزیکی ذخیره شده باشد و مکان فیزیکی آن ممکن است به صورت مستقیم مرتبط با مکان بلاک در دستگاه مجازی نباشد."

msgid "Software RAID"
msgstr "RAID نرم‌افزاری"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>Software RAID</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "<primary>Redundant Array of Independent Disks</primary><see>RAID</see>"
msgstr ""

#, fuzzy
#| msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss and ensure availability in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgstr "RAID مخفف عبارت <emphasis>Redundant Array of Independent Disks</emphasis> به معنی آرایه‌ای افزونه از دیسک‌های مستقل است. هدف این سیستم پیشگیری از بین رفتن داده در زمان نقص سخت‌افزاری هارد دیسک است. ایده اصلی آن بسیار ساده است: داده بجای اینکه در یک دیسک ذخیره گردد در چند دیسک فیزیکی انباشت می‌شود، همراه با یک سطح قابل پیکربندی از افزونگی. با توجه به این میزان از افزونگی، در زمان بروز یک رویداد سخت‌افزاری ناخواسته روی دیسک، داده می‌تواند از سایر دیسک‌های باقیمانده بازسازی شود."

msgid "<emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?"
msgstr "<emphasis>فرهنگ</emphasis> <foreignphrase>مستقل</foreignphrase> یا <foreignphrase>ارزان</foreignphrase>؟"

#, fuzzy
#| msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavour of cheapness."
msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavor of cheapness."
msgstr "حرف I در RAID سابق بر این به معنای <emphasis>inexpensive</emphasis> بود، چرا که RAID امنیت داده را به شدت و بدون نیاز به استفاده از دیسک‌های گران-قیمت بالا می‌برد. اما امروزه با توجه به نگرانی‌های فضای ذخیره‌سازی، از این حرفه بیشتر به معنای <emphasis>independent</emphasis> یاد می‌شود، که الزاما معنای ارزان بودن آن را به یاد نمی‌آورد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>Hardware RAID</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>degraded</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>reconstruction</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "RAID can be implemented either by dedicated hardware (RAID modules integrated into SCSI or SATA controller cards) or by software abstraction (the kernel). Whether hardware or software, a RAID system with enough redundancy can transparently stay operational when a disk fails; the upper layers of the stack (applications) can even keep accessing the data in spite of the failure. Of course, this “degraded mode” can have an impact on performance, and redundancy is reduced, so a further disk failure can lead to data loss. In practice, therefore, one will strive to only stay in this degraded mode for as long as it takes to replace the failed disk. Once the new disk is in place, the RAID system can reconstruct the required data so as to return to a safe mode. The applications won't notice anything, apart from potentially reduced access speed, while the array is in degraded mode or during the reconstruction phase."
msgstr "RAID می‌تواند هم به صورت سخت‌‌افزاری (افزونه‌های RAID منطبق با کارت‌های کنترل SCSI یا SATA) هم به صورت نرم‌افزاری (توسط کرنل) پیاده‌سازی شود. جدا از شیوه پیاده‌سازی آن، یک سیستم RAID در صورت وجود افزونگی کافی می‌تواند در زمان بروز نقص دیسک به کار خود ادامه دهد؛ لایه‌های بالایی آن (برنامه‌ها) حتی می‌توانند در صورت بروز مشکل به داده دسترسی داشته باشند. البته که این “حالت ناامن” می‌تواند عملکرد منفی روی سیستم بگذارد و منجر به کاهش سطح افزونگی گردد، بنابراین یک نقص دیسک دیگر، منجر به از دست دادن داده می‌شود. در عمل، تنها یک دیسک تلاش می‌کند که در این حالت تخریبی تا زمان برطرف شدن مشکل قرار بگیرد. زمانی که دیسک جدید جایگزین شود، سیستم RAID می‌تواند با بازسازی داده به حالت امن خود بازگردد. زمانی که آرایه در حالت ناامن یا فاز بازسازی داده قرار می‌گیرد، عملا وقفه‌ای در برنامه‌ها ایجاد نمی‌گردد بجز کاهش سرعت دسترسی به داده."

msgid "When RAID is implemented by hardware, its configuration generally happens within the BIOS setup tool, and the kernel will consider a RAID array as a single disk, which will work as a standard physical disk, although the device name may be different (depending on the driver)."
msgstr "زمانی که RAID به صورت سخت‌افزاری پیاده‌سازی شود، پیکربندی آن درون ابزار برپایی BIOS قرار می‌گیرد و کرنل یک آرایه RAID را به عنوان یک دیسک مجزا در نظر می‌گیرد، که مانند یک دیسک استاندارد کار خواهد کرد، با این حال نام دستگاه می‌تواند متفاوت باشد (بر اساس درایو بکار رفته)."

msgid "We only focus on software RAID in this book."
msgstr "ما تنها به RAID نرم‌افزاری در این کتاب اشاره می‌کنیم."

msgid "Different RAID Levels"
msgstr "سطوح مختلف RAID"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>level</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "RAID is actually not a single system, but a range of systems identified by their levels; the levels differ by their layout and the amount of redundancy they provide. The more redundant, the more failure-proof, since the system will be able to keep working with more failed disks. The counterpart is that the usable space shrinks for a given set of disks; seen the other way, more disks will be needed to store a given amount of data."
msgstr "RAID در حقیقت یک سیستم مجزا نیست، بلکه بازه‌ای از سیستم‌ها در سطوح مختلف است؛ این سطوح با توجه به ساختار و میزان افزونگی داده با یکدیگر فرق دارند. هر چه افزونگی بیشتر باشد، توانایی مواجه به دیسک‌های خراب در زمان نقص سخت‌افزاری بالاتر می‌رود. نقطه مقابل آن زمانی است که فضای موجود برای مجموعه‌ای از دیسک‌ها کاهش یابد؛ که در این صورت به دیسک‌های بیشتری برای ذخیره‌سازی داده نیاز است."

msgid "Linear RAID"
msgstr "RAID خطی"

#, fuzzy
#| msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That's about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That is about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgstr "با اینکه زیرسیستم RAID در کرنل از “RAID خطی” پشتیبانی می‌کند، این یک RAID کارآمد بحساب نمی‌آید چرا که شامل هیچ سطحی از افزونگی داده نیست. کرنل صرفا با قرار دادن دیسک‌های مختلف در کنار یکدیگر و ایجاد یک فضای بزرگ‌تر یک دیسک مجازی (یک دستگاه بلاک-محور) بوجود می‌آورد. این تنها عملکرد آن است. از این تنظیم به ندرت استفاده می‌شود (بجز موارد خاص که در ادامه خواهید دید)، بخصوص زمانی که در صورت نبود افزونگی داده، نقص در یک دیسک باعث غیرقابل دسترس شدن داده در تمامی دیسک‌ها و کل سیستم می‌گردد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>linear</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "RAID-0"
msgstr "RAID-0"

msgid "This level doesn't provide any redundancy either, but disks aren't simply stuck on end one after another: they are divided in <emphasis>stripes</emphasis>, and the blocks on the virtual device are stored on stripes on alternating physical disks. In a two-disk RAID-0 setup, for instance, even-numbered blocks of the virtual device will be stored on the first physical disk, while odd-numbered blocks will end up on the second physical disk."
msgstr "این سطح نیز هیچ افزونگی داده‌ای را فراهم نمی‌کند ولی برخلاف سطح قبل دیسک‌ها به صورت پیوسته پشت سر هم قرار نمی‌گیرند: بلکه به <emphasis>stripes</emphasis> تقسیم می‌شوند و بلاک‌های دستگاه مجازی روی این stripe از دیسک‌های فیزیکی قرار می‌گیرند. برای نمونه، در یک تنظیم RAID-0 با دو دیسک، بلاک‌های شماره زوج از دستگاه مجازی روی دیسک فیزیکی اول و بلاک‌های شماره فرد روی دیسک فیزیکی دوم ذخیره می‌شوند."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>stripes</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. However, RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. The disks are utilized entirely by the RAID device, so they should have the same size not to lose performance."
msgstr "هدف این سیستم افزایش قابلیت اعتماد نیست، چرا که (مانند حالت خطی) موجودیت داده به محض بروز نقص در دیسک به خطر می‌افتد، هدف آن افزایش عملکرد دیسک است: طی دسترسی ترتیبی به بخش بزرگی از داده‌های به هم پیوسته، کرنل می‌تواند به صورت موازی عملیات خواندن و نوشتن را روی هر دو دیسک انجام دهد که اینکار منجر به افزایش نرخ انتقال داده می‌گردد. با این حال، استفاده از RAID-0 در حال کاهش است، به صورتی که کاربرد آن با LVM جایگزین شده است (در ادامه خواهید دید)."

msgid "RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>0</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "RAID-1"
msgstr "RAID-1"

msgid "This level, also known as “RAID mirroring”, is both the simplest and the most widely used setup. In its standard form, it uses two physical disks of the same size, and provides a logical volume of the same size again. Data are stored identically on both disks, hence the “mirror” nickname. When one disk fails, the data is still available on the other. For really critical data, RAID-1 can of course be set up on more than two disks, with a direct impact on the ratio of hardware cost versus available payload space."
msgstr "این سطح، که با نام “RAID Mirroring” نیز شناخته می‌شود، ساده‌ترین و پرکاربردترین تنظیم مورد استفاده است. در حالت استاندارد، از دو دیسک فیزیکی هم اندازه استفاده می‌کند تا یک فضای منطقی به همان اندازه را فراهم کند. داده به صورت کاملا یکسان روی هر دو دیسک ذخیره می‌شود، که همان عبارت “mirror” است. زمانی که یک دیسک خراب شود داده از دیسک دیگر قابل دسترس است. برای داده‌های بسیار حیاتی، RAID-1 می‌تواند با بیش از دو دیسک تنظیم شود، که تاثیر مستقیم روی نرخ هزینه سخت‌افزار در مقابل فضای موجود خواهد گذاشت."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>1</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>mirror</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "<emphasis>NOTE</emphasis> Disks and cluster sizes"
msgstr "<emphasis>یادداشت</emphasis> دیسک‌ها و اندازه‌های خوشه"

msgid "If two disks of different sizes are set up in a mirror, the bigger one will not be fully used, since it will contain the same data as the smallest one and nothing more. The useful available space provided by a RAID-1 volume therefore matches the size of the smallest disk in the array. This still holds for RAID volumes with a higher RAID level, even though redundancy is stored differently."
msgstr "اگر دو دیسک با اندازه مختلف در این سطح بکار گرفته شوند، از دیسک بزرگ‌تر به طور کامل استفاده نخواهد شد چرا که تنها شامل تمام داده‌های دیسک کوچک‌تر است. بنابراین فضای قابل استفاده در یک تنظیم RAID-1 برابر با کوچک‌ترین دیسک موجود در آرایه است. این الگو در رابطه با سایر سطوح RAID پیشرفته‌تر نیز صادق است، با اینکه ممکن است از فرآیند افزونگی دیگری استفاده کنند."

msgid "It is therefore important, when setting up RAID arrays (except for RAID-0 and “linear RAID”), to only assemble disks of identical, or very close, sizes, to avoid wasting resources."
msgstr "به همین دلیل، مهم است که در زمان برپایی آرایه‌های RAID (بجز RAID-0 و خطی) از دیسک‌های با اندازه مشابه یا بسیار نزدیک به هم استفاده شود تا هیچ فضای اضافی تلف نگردد."

msgid "<emphasis>NOTE</emphasis> Spare disks"
msgstr "<emphasis>یادداشت</emphasis> دیسک‌های کمکی"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary>spare disk</primary>"
msgstr "<primary>preseed</primary>"

msgid "RAID levels that include redundancy allow assigning more disks than required to an array. The extra disks are used as spares when one of the main disks fails. For instance, in a mirror of two disks plus one spare, if one of the first two disks fails, the kernel will automatically (and immediately) reconstruct the mirror using the spare disk, so that redundancy stays assured after the reconstruction time. This can be used as another kind of safeguard for critical data."
msgstr "سطوح RAID که شامل افزونگی داده هستند اجازه استفاده از دیسک‌های بیشتر در آرایه دیسک‌ها را فراهم می‌کنند. از این دیسک‌ها به منظور پشتیبان در زمان بروز نقص برای یکی از دیسک‌های موجود استفاده می‌شود. برای نمونه، در یک تنظیم دو دیسکه همراه با دیسک کمکی، اگر یکی از دیسک‌های اولیه معیوب شود، کرنل به صورت خودکار (و بلافاصله) فضای موجود را با استفاده از دیسک کمکی بازسازی می‌کند، به منظور اینکه اطمینان از عملیات افزونگی داده پس از زمان بازسازی حاصل گردد. از این روش می‌توان به عنوان گام اضافه برای نگهداری از داده‌های بسیار حساس استفاده کرد."

msgid "One would be forgiven for wondering how this is better than simply mirroring on three disks to start with. The advantage of the “spare disk” configuration is that the spare disk can be shared across several RAID volumes. For instance, one can have three mirrored volumes, with redundancy assured even in the event of one disk failure, with only seven disks (three pairs, plus one shared spare), instead of the nine disks that would be required by three triplets."
msgstr "ممکن است به نظر آید که چطور این پیکربندی در مقایسه با استفاده از سه دیسک برای mirroring ممکن است مفید باشد. مزیت استفاده از پیکربندی “دیسک کمکی” این است که می‌تواند بین چندین فضای ذخیره‌سازی RAID به اشتراک گذاشته شود. برای نمونه، می‌توان از سه فضای ذخیره‌سازی mirrored استفاده کرد، همراه با افزونگی داده که در صورت بروز نقص در یکی از دیسک‌ها بکار می‌آید، همراه با هفت دیسک (سه زوج به همراه یک دیسک کمکی) بجای نه دیسک که مورد نیاز سه خانواده سه زوجی است."

msgid "This RAID level, although expensive (since only half of the physical storage space, at best, is useful), is widely used in practice. It is simple to understand, and it allows very simple backups: since both disks have identical contents, one of them can be temporarily extracted with no impact on the working system. Read performance is often increased since the kernel can read half of the data on each disk in parallel, while write performance isn't too severely degraded. In case of a RAID-1 array of N disks, the data stays available even with N-1 disk failures."
msgstr "این سطح RAID، با وجود هزینه بالا (از آنجا که در بهترین حالت از نصف فضای ذخیره‌سازی استفاده می‌شود) در عمل بسیار مورد استفاده قرار می‌گیرد. درک آن بسیار ساده است و امکان ایجاد پشتیبان‌های ساده وجود دارد: از آنجا که هر دو دیسک شامل محتوای یکسانی هستند، یکی از آن‌ها بدون ایجاد کوچکترین تاثیر منفی روی سیستم می‌تواند تخلیه شود. عملیات خواندن نیز بسیار سریع‌تر خواهد بود چرا که در هر لحظه کرنل به صورت همزمان نصف داده را از هر دو دیسک می‌تواند بخواند، با اینکه عملیات نوشتن آنطور که به نظر می‌آید تاثیر منفی ندارد. در مورد آرایه RAID-1 از N دیسک، حتی در صورت معیوب شدن N-1 دیسک داده کماکان قابل دسترس خواهد بود."

#, fuzzy
#| msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgid "<emphasis>CAUTION</emphasis> RAID is not Backup"
msgstr "<emphasis>نکته</emphasis> RAID، دیسک‌ها و پارتیشن‌ها"

#, fuzzy
#| msgid "<primary>debian-cd</primary>"
msgid "<primary>backup</primary>"
msgstr "<primary>debian-cd</primary>"

msgid "RAID systems are not backup mechanisms. While RAID increases the redundancy - and therefore the availability of a system - and protects against disk failures, backups are done to protect data from being altered, deleted, getting corrupted, etc., and to be able to restore them if necessary. To demonstrate this: If you remove one or all files by accident, a RAID will mirror this change, but it will not provide the means to restore the file(s). So while there is clearly an overlap, they are not the same and should be used in conjunction with each other."
msgstr ""

msgid "RAID-4"
msgstr "RAID-4"

msgid "This RAID level, not widely used, uses N disks to store useful data, and an extra disk to store redundancy information. If that disk fails, the system can reconstruct its contents from the other N. If one of the N data disks fails, the remaining N-1 combined with the “parity” disk contain enough information to reconstruct the required data."
msgstr "این سطح RAID، که کاربرد زیادی ندارد، از N دیسک برای ذخیره‌سازی داده مفید و از یک دیسک اضافی برای ذخیره‌سازی اطلاعات افزونگی استفاده می‌کند. اگر این دیسک اضافی معیوب شود، سیستم می‌تواند با آن N دیسک دیگر محتوای خود را بازسازی کند، به صورتی که N-1 دیسک باقیمانده همراه با دیسک “parity” شامل اطلاعات کافی برای بازسازی تمام اطلاعات هستند."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>4</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>parity</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "RAID-4 isn't too expensive since it only involves a one-in-N increase in costs and has no noticeable impact on read performance, but writes are slowed down. Furthermore, since a write to any of the N disks also involves a write to the parity disk, the latter sees many more writes than the former, and its lifespan can shorten dramatically as a consequence. Data on a RAID-4 array is safe only up to one failed disk (of the N+1)."
msgstr "RAID-4 هزینه زیادی ندارد چرا که تنها شامل یک هزینه افزایشی یک در N می‌باشد که تاثیر بسزایی روی عملیات خواندن ندارد ولی عملیات نوشتن را کند می‌کند. علاوه بر این، از آنجا که نوشتن روی هر یک از N دیسک به معنای نوشتن روی دیسک parity است، این دیسک اضافی شاهد نوشتن‌های بیشتری نسبت به سایر دیسک‌ها است و همین دلیل نیز باعث کاهش طول عمر مفید آن می‌گردد. داده روی آرایه RAID-4 تنها در صورت معیوب شدن یک دیسک از N+1 دیسک موجود ایمن خواهد بود."

msgid "RAID-5"
msgstr "RAID-5"

msgid "RAID-5 addresses the asymmetry issue of RAID-4: parity blocks are spread over all of the N+1 disks, with no single disk having a particular role."
msgstr "RAID-5 مشکل عدم تقارن در RAID-4 را برطرف می‌کند: بلاک‌های parity بین تمام N+1 دیسک دیگر پخش می‌شوند به صورتی که تنها یک دیسک نقش منحصربفرد نداشته باشد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>5</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Read and write performance are identical to RAID-4. Here again, the system stays functional with up to one failed disk (of the N+1), but no more."
msgstr "عملیات خواندن و نوشتن درست مانند  RAID-4 است. در اینجا نیز، سیستم تا زمانی بکار خود ادامه می‌دهد که تنها یک دیسک معیوب از N+1 دیسک موجود باشد ولی نه بیشتر."

msgid "RAID-6"
msgstr "RAID-6"

msgid "RAID-6 can be considered an extension of RAID-5, where each series of N blocks involves two redundancy blocks, and each such series of N+2 blocks is spread over N+2 disks."
msgstr "RAID-6 می‌تواند به عنوان افزونه‌ای برای RAID-5 در نظر گرفته شود، به صورتی که هر سری از N بلاک شامل دو بلاک افزونگی داده است و هر یک از این N+2 بلاک میان N+2 دیسک تقسیم می‌شود."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>6</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "This RAID level is slightly more expensive than the previous two, but it brings some extra safety since up to two drives (of the N+2) can fail without compromising data availability. The counterpart is that write operations now involve writing one data block and two redundancy blocks, which makes them even slower."
msgstr "این سطح RAID در مقایسه با دو سطح قبلی هزینه بیشتری در پی دارد، ولی امنیت بیشتری نیز به همراه می‌آورد چرا که تا دو درایو از N+2 دیسک موجود در صورت معیوب شدن می‌توانند بکار خود ادامه دهند. نقطه مقابل آن این است که عملیات نوشتن شامل یک بلاک داده و دو بلاک افزونگی دیگر است، که این آرایه را در مقایسه با سطوح دیگر کندتر نیز می‌کند."

msgid "RAID-1+0"
msgstr "RAID-1+0"

#, fuzzy
#| msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there's no problem with that."
msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there is no problem with that."
msgstr "این به خودی خود یک سطح RAID بحساب نمی‌آید، بلکه بیشتر یک گروه‌بندی از سطوح دیگر است. با استفاده از 2N دیسک، مجموعه اول به N دیسک RAID-1 تقسیم می‌شود؛ سپس این فضای ذخیره‌سازی به یک واحد مجزا تبدیل می‌شود خواه با “RAID خطی” یا استفاده از LVM. این گزینه آخر چیزی بیش از RAID خالص است، اما مشکلی در استفاده از آن وجود ندارد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>1+0</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "RAID-1+0 can survive multiple disk failures: up to N in the 2×N array described above, provided that at least one disk keeps working in each of the RAID-1 pairs."
msgstr "RAID-1+0 می‌تواند از چندین نقص دیسک فرار کند: تا N دیسک از 2N آرایه تعریف شده بالا که برای هر کدام از زوج‌های RAID-1 فراهم شده است، می‌تواند معیوب شود."

msgid "<emphasis>GOING FURTHER</emphasis> RAID-10"
msgstr "<emphasis>مطالعه بیشتر</emphasis> RAID-10"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>10</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "RAID-10 is generally considered a synonym of RAID-1+0, but a Linux specificity makes it actually a generalization. This setup allows a system where each block is stored on two different disks, even with an odd number of disks, the copies being spread out along a configurable model."
msgstr "RAID-10 بیشتر به عنوان مترادفی برای RAID-1+0 استفاده می‌شود، اما در لینوکس به عنوان یک استاندارد بکار گرفته می‌شود. این تنظیم شامل سیستمی است که هر بلاک آن درون دو دیسک متفاوت ذخیره می‌شود، حتی با تعداد دیسک‌های فرد، عملیات رونوشت‌گیری با استفاده از مدل قابل پیکربندی به صورت بهینه انجام می‌شود."

msgid "Performances will vary depending on the chosen repartition model and redundancy level, and of the workload of the logical volume."
msgstr "عملکرد کلی سیستم با توجه به سطح افزونگی و مدل پارتیشن‌بندی متفاوت خواهد بود، همچنین فضای ذخیره‌سازی منطقی نیز روی آن تاثیرگذار است."

msgid "Obviously, the RAID level will be chosen according to the constraints and requirements of each application. Note that a single computer can have several distinct RAID arrays with different configurations."
msgstr "به طور مشخص، سطح RAID با توجه به محدودیت‌ها و نیازمندی‌های سیستم موجود انتخاب می‌شود. نکته اینکه یک رایانه می‌تواند شامل چندین آرایه RAID منحصربفرد به همراه پیکربندی‌های متفاوت باشد."

msgid "Setting up RAID"
msgstr "برپایی RAID"

msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>create</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Setting up RAID volumes requires the <emphasis role=\"pkg\">mdadm</emphasis> package; it provides the <command>mdadm</command> command, which allows creating and manipulating RAID arrays, as well as scripts and tools integrating it to the rest of the system, including the monitoring system."
msgstr "برپایی آرایه‌های RAID نیازمند بسته <emphasis role=\"pkg\">mdadm</emphasis> است؛ که شامل دستور <command>mdadm</command> برای ایجاد و ویرایش این آرایه‌ها می‌باشد همراه با ابزارهای جانبی و اسکریپت‌هایی که آن را با سایر قسمت‌های سیستم از جمله مانیتورینگ یکپارچه می‌سازد."

msgid "Our example will be a server with a number of disks, some of which are already used, the rest being available to setup RAID. We initially have the following disks and partitions:"
msgstr "مثال ما شامل سروری با چندین دیسک است که برخی از آن‌ها استفاده شده و برخی دیگر که آزاد هستند به عنوان RAID بکار گرفته می‌شوند. در حالت اولیه دیسک‌ها و پارتیشن‌های زیر را در اختیار داریم:"

msgid "the <filename>sdb</filename> disk, 4 GB, is entirely available;"
msgstr "دیسک ۴ گیگابایت <filename>sdb</filename> کاملا موجود است؛"

msgid "the <filename>sdc</filename> disk, 4 GB, is also entirely available;"
msgstr "دیسک ۴ گیگابایت <filename>sdc</filename> کاملا موجود است؛"

msgid "on the <filename>sdd</filename> disk, only partition <filename>sdd2</filename> (about 4 GB) is available;"
msgstr "در دیسک <filename>sdd</filename> تنها پارتیشن ۴ گیگابایت <filename>sdd2</filename> موجود است؛"

msgid "finally, a <filename>sde</filename> disk, still 4 GB, entirely available."
msgstr "در نهایت، دیسک ۴ گیگابایت <filename>sde</filename> که کاملا موجود است."

msgid "<emphasis>NOTE</emphasis> Identifying existing RAID volumes"
msgstr "<emphasis>یادداشت</emphasis> شناسایی آرایه‌های موجود RAID"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/proc</filename></primary><secondary><filename>/proc/mdstat</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The <filename>/proc/mdstat</filename> file lists existing volumes and their states. When creating a new RAID volume, care should be taken not to name it the same as an existing volume."
msgstr "فایل <filename>/proc/mdstat</filename> فهرستی از آرایه‌های موجود و شرایط آن‌ها را نگهداری می‌کند. در زمان ایجاد یک آرایه جدید RAID، باید دقت کرد که نام آن با نام آرایه‌ای موجود برابر نباشد."

msgid "We're going to use these physical elements to build two volumes, one RAID-0 and one mirror (RAID-1). Let's start with the RAID-0 volume:"
msgstr "با استفاده از این دیسک‌ها می‌خواهیم دو فضای ذخیره‌سازی بوجود آوریم، یکی RAID-0 و دیگری RAID-1. بیایید با آرایه RAID-0 شروع کنیم:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md0 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
#| "<computeroutput>/dev/md0:\n"
#| "        Version : 1.2\n"
#| "  Creation Time : Wed May  6 09:24:34 2015\n"
#| "     Raid Level : raid0\n"
#| "     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 2\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:24:34 2015\n"
#| "          State : clean \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 0\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "     Chunk Size : 512K\n"
#| "\n"
#| "           Name : mirwiz:0  (local to host mirwiz)\n"
#| "           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
#| "         Events : 0\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       16        0      active sync   /dev/sdb\n"
#| "       1       8       32        1      active sync   /dev/sdc\n"
#| "# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
#| "<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
#| "Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
#| "Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
#| "Superblock backups stored on blocks: \n"
#| "        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
#| "\n"
#| "Allocating group tables: done                            \n"
#| "Writing inode tables: done                            \n"
#| "Creating journal (32768 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
#| "<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
#| "/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc\n"
"</userinput><computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0\n"
"</userinput><computeroutput>/dev/md0: 7.99GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0\n"
"</userinput><computeroutput>/dev/md0:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 01:54:24 2022\n"
"        Raid Level : raid0\n"
"        Array Size : 8378368 (7.99 GiB 8.58 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 01:54:24 2022\n"
"             State : clean \n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"            Layout : -unknown-\n"
"        Chunk Size : 512K\n"
"\n"
"Consistency Policy : none\n"
"\n"
"              Name : debian:0  (local to host debian)\n"
"              UUID : a75ac628:b384c441:157137ac:c04cd98c\n"
"            Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8        0        0      active sync   /dev/sdb\n"
"       1       8       16        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0\n"
"</userinput><computeroutput>mke2fs 1.46.2 (28-Feb-2021)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 2094592 4k blocks and 524288 inodes\n"
"Filesystem UUID: ef077204-c477-4430-bf01-52288237bea0\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/raid-0\n"
"</userinput><computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0\n"
"</userinput><computeroutput># </computeroutput><userinput>df -h /srv/raid-0\n"
"</userinput><computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.8G   24K  7.4G   1% /srv/raid-0\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:24:34 2015\n"
"     Raid Level : raid0\n"
"     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:24:34 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"     Chunk Size : 512K\n"
"\n"
"           Name : mirwiz:0  (local to host mirwiz)\n"
"           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       16        0      active sync   /dev/sdb\n"
"       1       8       32        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
"Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
"Superblock backups stored on blocks: \n"
"        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0\n"
"</computeroutput>"

#, fuzzy
#| msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It's also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It is also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgstr "دستور <command>mdadm --create</command> نیازمند چند پارامتر است: نام آرایه‌ای که قصد ایجادش را داریم (<filename>/dev/md*</filename> که MD به معنی <foreignphrase>Multiple Device</foreignphrase> است)، سطح RAID، تعداد دیسک‌ها (که اجباری است و از RAID-1 به بالا معنا دارد) و درایوهای فیزیکی قابل استفاده. زمانی که دستگاه ایجاد گردد، مانند یک پارتیشن عادی می‌توانیم از آن استفاده کرده، فایل سیستم ایجاد کنیم و آن را متصل سازیم. نکته اینکه ایجاد آرایه RAID-0 روی <filename>md0</filename> تصادفی است و این شماره‌گذاری هیچ ارتباطی به سطوح مختلف RAID ندارد. همچنین امکان ایجاد آرایه‌های نامگذاری شده RAID نیز با استفاده از پارامترهایی نظیر <filename>/dev/md/linear</filename> بجای <filename>/dev/md0</filename> در دستور <command>mdadm</command> وجود دارد."

msgid "Creation of a RAID-1 follows a similar fashion, the differences only being noticeable after the creation:"
msgstr "ایجاد یک آرایه RAID-1 مشابه قبل است که تفاوت آن تنها پس از فرآیند ایجاد مشخص می‌گردد:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: Note: this array has metadata at the start and\n"
#| "    may not be suitable as a boot device.  If you plan to\n"
#| "    store '/boot' on this device please ensure that\n"
#| "    your boot-loader understands md/v1.x metadata, or use\n"
#| "    --metadata=0.90\n"
#| "mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
#| "Continue creating array? </computeroutput><userinput>y</userinput>\n"
#| "<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
#| "mdadm: array /dev/md1 started.\n"
#| "# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "        Version : 1.2\n"
#| "  Creation Time : Wed May  6 09:30:19 2015\n"
#| "     Raid Level : raid1\n"
#| "     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
#| "  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 2\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:30:40 2015\n"
#| "          State : clean, resyncing (PENDING) \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 0\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 0\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       1       8       64        1      active sync   /dev/sde\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "          State : clean\n"
#| "[...]\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde\n"
"</userinput><computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdc2) exceeds size (4189184K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y\n"
"</userinput><computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1\n"
"</userinput><computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:08:09 2022\n"
"             State : clean, resync\n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 0\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 13% complete\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 17\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       1       8       48        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:30:19 2015\n"
"     Raid Level : raid1\n"
"     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
"  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:30:40 2015\n"
"          State : clean, resyncing (PENDING) \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       1       8       64        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"

msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgstr "<emphasis>نکته</emphasis> RAID، دیسک‌ها و پارتیشن‌ها"

#, fuzzy
#| msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions, and do not require full disks."
msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions as well, and do not require full disks."
msgstr "همانطور که در مثال نیز مشخص است، دستگاه‌های RAID بدون پارتیشن‌بندی‌های دیسک نیز می‌توانند ایجاد گردند و نیازمند دیسک‌های کامل نمی‌باشند."

msgid "A few remarks are in order. First, <command>mdadm</command> notices that the physical elements have different sizes; since this implies that some space will be lost on the bigger element, a confirmation is required."
msgstr "چند نکته باقی می‌ماند. اول، <command>mdadm</command> تشخیص می‌دهد که دستگاه‌های فیزیکی شامل اندازه‌های متفاوت هستند؛ از آنجا که این امر منجر به گم شدن فضا در دیسک بزرگتر می‌شود، تاییدیه آن مورد نیاز است."

msgid "More importantly, note the state of the mirror. The normal state of a RAID mirror is that both disks have exactly the same contents. However, nothing guarantees this is the case when the volume is first created. The RAID subsystem will therefore provide that guarantee itself, and there will be a synchronization phase as soon as the RAID device is created. After some time (the exact amount will depend on the actual size of the disks…), the RAID array switches to the “active” or “clean” state. Note that during this reconstruction phase, the mirror is in a degraded mode, and redundancy isn't assured. A disk failing during that risk window could lead to losing all the data. Large amounts of critical data, however, are rarely stored on a freshly created RAID array before its initial synchronization. Note that even in degraded mode, the <filename>/dev/md1</filename> is usable, and a filesystem can be created on it, as well as some data copied on it."
msgstr "مهمتر از آن به حالت دیسک mirror توجه کنید. حالت عادی در آرایه RAID که به صورت mirror باشد مشابهت کامل محتوا در هر دو دیسک است. با این حال، ضمانتی برای بوجود آمدن این حالت در زمان ایجاد آرایه وجود ندارد. زیرمجموعه RAID خود این ضمانت را ایجاد می‌کند و به محض اینکه دستگاه RAID ساخته شود عملیات همگام‌سازی صورت می‌گیرد. بعد از گذشت زمان (که وابسته به اندازه‌ دیسک‌ها است) آرایه RAID به حالت “فعال” یا “تمیز” تغییر می‌یابد. نکته اینکه در زمان این فاز بازسازی، mirror در یک حالت ناپایدار قرار می‌گیرد که افزونگی داده در آن تضمین نمی‌شود . دیسکی که در این بازه زمانی دچار مشکل گردد ممکن است به حذف داده بینجامد. با توجه به این موضوع، قبل از فاز همگام‌سازی معمولا داده‌های بزرگ و حساس روی آرایه RAID قرار نمی‌گیرند. حتی در حالت ناپایدار نیز، <filename>/dev/md1</filename> قابل استفاده است و یک فایل سیستم می‌تواند روی آن ایجاد گردد و داده‌های روی آن قرار گیرند."

msgid "<emphasis>TIP</emphasis> Starting a mirror in degraded mode"
msgstr "<emphasis>نکته</emphasis> آغاز یک mirror در حالت ناپایدار"

msgid "Sometimes two disks are not immediately available when one wants to start a RAID-1 mirror, for instance because one of the disks one plans to include is already used to store the data one wants to move to the array. In such circumstances, it is possible to deliberately create a degraded RAID-1 array by passing <filename>missing</filename> instead of a device file as one of the arguments to <command>mdadm</command>. Once the data have been copied to the “mirror”, the old disk can be added to the array. A synchronization will then take place, giving us the redundancy that was wanted in the first place."
msgstr "بعضی وقت‌ها در زمان آغاز mirror از RAID-1 دو دیسک بلافاصله قابل دسترس نخواهند بود، برای نمونه به این دلیل که یکی از دیسک‌ها برای ذخیره داده‌هایی استفاده شده است که دیگری می‌خواهد از آن mirror بگیرد. در چنین شرایطی، امکان ارجاع یک آرایه RAID-1 ناپایدار با استفاده از پارامتر <filename>missing</filename> بجای نام دستگاه در <command>mdadm</command> وجود دارد. زمانی که از داده در “mirror” رونوشت گرفته شود، دیسک قدیمی می‌تواند به آرایه اضافه گردد. سپس همگام‌سازی صورت می‌گیرد که امکان افزونگی داده با استفاده از دیسک قدیمی را فراهم می‌آورد."

msgid "<emphasis>TIP</emphasis> Setting up a mirror without synchronization"
msgstr "<emphasis>نکته</emphasis> برپایی یک mirror بدون همگام‌سازی"

msgid "RAID-1 volumes are often created to be used as a new disk, often considered blank. The actual initial contents of the disk is therefore not very relevant, since one only needs to know that the data written after the creation of the volume, in particular the filesystem, can be accessed later."
msgstr "آرایه‌های RAID-1 اغلب به منظور ایجاد یک دیسک جدید و خالی استفاده می‌شوند. از این رو محتوای اولیه دیسک خیلی حائز اهمیت نیست، از این رو باید دانست داده‌ای که پس از ایجاد فایل سیستم در آرایه قرار می‌گیرد قابل دسترس خواهد بود."

msgid "One might therefore wonder about the point of synchronizing both disks at creation time. Why care whether the contents are identical on zones of the volume that we know will only be read after we have written to them?"
msgstr "شاید این سوال پیش بیاید که هدف از همگام‌سازی دو دیسک در زمان ایجاد آرایه چیست. چرا به یکسان بودن محتوایی که روی ناحیه‌های این آرایه قرار می‌گیرد اهمیت بدهیم وقتی می‌دانیم تنها پس از نوشتن روی آن است که می‌توان به آن‌ها دسترسی داشت؟"

msgid "Fortunately, this synchronization phase can be avoided by passing the <literal>--assume-clean</literal> option to <command>mdadm</command>. However, this option can lead to surprises in cases where the initial data will be read (for instance if a filesystem is already present on the physical disks), which is why it isn't enabled by default."
msgstr "خوشبختانه، این فاز همگام‌سازی می‌تواند با استفاده از گزینه <literal>--assume-clean</literal> در <command>mdadm</command> در نظر گرفته نشود. اگرچه، این گزینه ممکن است منجر به سردرگمی در مواردی شود که داده اولیه خوانده خواهد شد (برای نمونه، وقتی یک فایل سیستم هم اکنون روی دیسک موجود باشد)، به همین دلیل است که به صورت پیشفرض فعال نیست."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>RAID</primary><secondary>failing</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Now let's see what happens when one of the elements of the RAID-1 array fails. <command>mdadm</command>, in particular its <literal>--fail</literal> option, allows simulating such a disk failure:"
msgstr "اکنون بیایید ببینیم در زمان بروز مشکل برای یکی از آرایه‌های RAID-1 چه اتفاقی می‌افتد. <command>mdadm</command> و به طور خاص گزینه <literal>--fail</literal> آن، امکان شبیه‌سازی این نقص دیسک را بوجود می‌آورد:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Update Time : Wed May  6 09:39:39 2015\n"
#| "          State : clean, degraded \n"
#| " Active Devices : 1\n"
#| "Working Devices : 1\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 19\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       0        0        2      removed\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde\n"
"</userinput><computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 2\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:15:34 2022\n"
"             State : clean, degraded \n"
"    Active Devices : 1\n"
"   Working Devices : 1\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       -       0        0        1      removed\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:39:39 2015\n"
"          State : clean, degraded \n"
" Active Devices : 1\n"
"Working Devices : 1\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       0        0        2      removed\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "The contents of the volume are still accessible (and, if it is mounted, the applications don't notice a thing), but the data safety isn't assured anymore: should the <filename>sdd</filename> disk fail in turn, the data would be lost. We want to avoid that risk, so we'll replace the failed disk with a new one, <filename>sdf</filename>:"
msgstr "محتوای آرایه هنوز قابل دسترس است (و در صورت اتصال به فایل سیستم، وقفه‌ای در برنامه‌ها ایجاد نمی‌شود) اما امنیت داده دیگر تضمین نمی‌شود: در صورت بروز نقص برای دیسک <filename>sdd</filename> داده از بین می‌رود. به منظور پیشگیری از این خطر دیسک معیوب را با <filename>sdf</filename> جایگزین می‌کنیم:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
#| "<computeroutput>mdadm: added /dev/sdf\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "   Raid Devices : 2\n"
#| "  Total Devices : 3\n"
#| "    Persistence : Superblock is persistent\n"
#| "\n"
#| "    Update Time : Wed May  6 09:48:49 2015\n"
#| "          State : clean, degraded, recovering \n"
#| " Active Devices : 1\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 1\n"
#| "\n"
#| " Rebuild Status : 28% complete\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 26\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      spare rebuilding   /dev/sdf\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Update Time : Wed May  6 09:49:08 2015\n"
#| "          State : clean \n"
#| " Active Devices : 2\n"
#| "Working Devices : 2\n"
#| " Failed Devices : 1\n"
#| "  Spare Devices : 0\n"
#| "\n"
#| "           Name : mirwiz:1  (local to host mirwiz)\n"
#| "           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "         Events : 41\n"
#| "\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      active sync   /dev/sdf\n"
#| "\n"
#| "       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:25:34 2022\n"
"             State : clean, degraded, recovering \n"
"    Active Devices : 1\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 1\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"    Rebuild Status : 47% complete\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 39\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"           Version : 1.2\n"
"     Creation Time : Mon Feb 28 02:07:48 2022\n"
"        Raid Level : raid1\n"
"        Array Size : 4189184 (4.00 GiB 4.29 GB)\n"
"     Used Dev Size : 4189184 (4.00 GiB 4.29 GB)\n"
"      Raid Devices : 2\n"
"     Total Devices : 3\n"
"       Persistence : Superblock is persistent\n"
"\n"
"       Update Time : Mon Feb 28 02:25:34 2022\n"
"             State : clean\n"
"    Active Devices : 2\n"
"   Working Devices : 2\n"
"    Failed Devices : 1\n"
"     Spare Devices : 0\n"
"\n"
"Consistency Policy : resync\n"
"\n"
"              Name : debian:1  (local to host debian)\n"
"              UUID : 2dfb7fd5:e09e0527:0b5a905a:8334adb8\n"
"            Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      active sync   /dev/sdf\n"
"\n"
"       1       8       48        -      faulty   /dev/sde\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"   Raid Devices : 2\n"
"  Total Devices : 3\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:48:49 2015\n"
"          State : clean, degraded, recovering \n"
" Active Devices : 1\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 1\n"
"\n"
" Rebuild Status : 28% complete\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:49:08 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

msgid "Here again, the kernel automatically triggers a reconstruction phase during which the volume, although still accessible, is in a degraded mode. Once the reconstruction is over, the RAID array is back to a normal state. One can then tell the system that the <filename>sde</filename> disk is about to be removed from the array, so as to end up with a classical RAID mirror on two disks:"
msgstr "در اینجا نیز، کرنل به صورت خودکار فاز بازسازی آرایه را آغاز می‌کند که طی آن با وجود قابل دسترس بودن، آرایه در یک حالت ناپایدار قرار دارد. زمانی که بازسازی تمام شود، آرایه RAID به حالت عادی خود باز می‌گردد. به منظور سازگاری با حالت کلاسیک RAID که از دو دیسک برای mirror استفاده می‌کند، می‌توان دیسک <filename>sde</filename> را حذف کرد؛"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
#| "<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
#| "# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
#| "<computeroutput>/dev/md1:\n"
#| "[...]\n"
#| "    Number   Major   Minor   RaidDevice State\n"
#| "       0       8       50        0      active sync   /dev/sdd2\n"
#| "       2       8       80        1      active sync   /dev/sdf</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde\n"
"</userinput><computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1\n"
"</userinput><computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       34        0      active sync   /dev/sdd2\n"
"       2       8       64        1      active sync   /dev/sdf\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf</computeroutput>"

msgid "From then on, the drive can be physically removed when the server is next switched off, or even hot-removed when the hardware configuration allows hot-swap. Such configurations include some SCSI controllers, most SATA disks, and external drives operating on USB or Firewire."
msgstr "از این لحظه، درایو می‌تواند در زمان خاموش شدن سرور یا در صورت پشتیبانی سخت‌افزاری از how-swap به صورت دستی جدا گردد. چنین پیکربندی شامل برخی کنترلرهای SCSI، اغلب دیسک‌های SATA و درایوهای خارجی که روی USB یا Firewire است."

msgid "Backing up the Configuration"
msgstr "پشتیبان‌گیری از پیکربندی"

#, fuzzy
#| msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. In reality this leads to the RAID starting from the individual disks alternately - distributing the data also alternately, depending on which disk started the RAID in degraded mode Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgstr "اکثر اطلاعات جانبی درباره آرایه‌های RAID به صورت مستقیم روی همین دیسک‌ها ذخیره‌سازی می‌شود، تا کرنل در زمان راه‌اندازی اولیه سیستم بتواند به صورت خودکار اجزای آرایه را تشکیل داده و آن را تنظیم کند. با این حال، پشتیبان‌گیری از این پیکربندی توصیه می‌شود چرا که این فرآیند تشخیص اولیه خالی از خطا نیست و تنها انتظار می‌رود که در موارد بسیار معدود دچار نقص گردد. در مثال ما، اگر نقص دیسک <filename>sde</filename> واقعی (در مقابل شبیه‌سازی شده) باشد و سیستم بدون حذف <filename>sde</filename> راه‌اندازی مجدد گردد، این دیسک ممکن است به فعالیت خود پس از عملیات تشخیص اولیه ادامه دهد. کرنل شامل سه دیسک فیزیکی است که هر کدام ادعا می‌کنند نصف فضای RAID را در اختیار دارند. حالت مبهم دیگر ترکیب آرایه‌های RAID از دو سرور مختلف در قالب یک سرور است. اگر این آرایه‌ها قبل از انتقال دیسک‌ها درست کار کنند، کرنل قادر خواهد بود که اجزای آن را شناسایی و پیکربندی کند؛ اما اگر دیسک‌های انتقال یافته درون آرایه <filename>md1</filename> از سرور قدیم قرار بگیرند، در صورتی که سرور جدید آرایه <filename>md1</filename> داشته باشد، یکی از mirrorها نامگذاری مجدد خواهد شد."

msgid "Backing up the configuration is therefore important, if only for reference. The standard way to do it is by editing the <filename>/etc/mdadm/mdadm.conf</filename> file, an example of which is listed here:"
msgstr "از این رو، پشتیبان‌گیری از فایل پیکربندی اهمیت می‌یابد. شیوه استاندارد اینکار ویرایش فایل <filename>/etc/mdadm/mdadm.conf</filename> است که مثالی از آن را در ادامه مشاهده می‌کنید:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/mdadm/mdadm.conf</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "<command>mdadm</command> configuration file"
msgstr "فایل پیکربندی<command>mdadm</command>"

#, fuzzy
#| msgid ""
#| "# mdadm.conf\n"
#| "#\n"
#| "# Please refer to mdadm.conf(5) for information about this file.\n"
#| "#\n"
#| "\n"
#| "# by default (built-in), scan all partitions (/proc/partitions) and all\n"
#| "# containers for MD superblocks. alternatively, specify devices to scan, using\n"
#| "# wildcards if desired.\n"
#| "DEVICE /dev/sd*\n"
#| "\n"
#| "# auto-create devices with Debian standard permissions\n"
#| "CREATE owner=root group=disk mode=0660 auto=yes\n"
#| "\n"
#| "# automatically tag new arrays as belonging to the local system\n"
#| "HOMEHOST &lt;system&gt;\n"
#| "\n"
#| "# instruct the monitoring daemon where to send mail alerts\n"
#| "MAILADDR root\n"
#| "\n"
#| "# definitions of existing MD arrays\n"
#| "ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
#| "\n"
#| "# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100\n"
#| "# by mkconf 3.2.5-3"
msgid ""
"<![CDATA[# mdadm.conf\n"
"#\n"
"# !NB! Run update-initramfs -u after updating this file.\n"
"# !NB! This will ensure that initramfs has an uptodate copy.\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST <system>\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md/0  metadata=1.2 UUID=a75ac628:b384c441:157137ac:c04cd98c name=debian:0\n"
"ARRAY /dev/md/1  metadata=1.2 UUID=2dfb7fd5:e09e0527:0b5a905a:8334adb8 name=debian:1\n"
"# This configuration was auto-generated on Mon, 28 Feb 2022 01:53:48 +0100 by mkconf\n"
"]]>"
msgstr ""
"# mdadm.conf\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# auto-create devices with Debian standard permissions\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
"\n"
"# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100\n"
"# by mkconf 3.2.5-3"

msgid "One of the most useful details is the <literal>DEVICE</literal> option, which lists the devices where the system will automatically look for components of RAID volumes at start-up time. In our example, we replaced the default value, <literal>partitions containers</literal>, with an explicit list of device files, since we chose to use entire disks and not only partitions, for some volumes."
msgstr "یکی از جزئیات کاربری آن گزینه <literal>DEVICE</literal> است، که دستگاه‌های مورد نیاز برای جستجوی خودکار اجزای آرایه‌های RAID در سیستم را مشخص می‌کند. در مثال ما، ما گزینه پیشفرض <literal>partitions containers</literal> را با فهرستی از دستگاه‌ها جایگزین کردیم چرا که قصد استفاده از تمام دیسک و نه قسمت‌هایی از پارتیشن آن را برای برخی آرایه‌ها داشتیم."

msgid "The last two lines in our example are those allowing the kernel to safely pick which volume number to assign to which array. The metadata stored on the disks themselves are enough to re-assemble the volumes, but not to determine the volume number (and the matching <filename>/dev/md*</filename> device name)."
msgstr "دو خط آخر در مثال ما به کرنل اجازه می‌دهند که با استفاده از شماره آرایه عملیات تشخیص و راه‌اندازی آن‌ها را انجام دهد. اطلاعات جانبی ذخیره شده روی دیسک برای جمع‌آوری آرایه‌ها کافی است، ولی نه برای تشخیص شماره آن‌ها (و نام دستگاه <filename>/dev/md*</filename> منطبق با آن)."

msgid "Fortunately, these lines can be generated automatically:"
msgstr "خوشبختانه، این خطوط به صورت خودکار تولید می‌شوند:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
#| "<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
#| "ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?\n"
"</userinput><computeroutput>ARRAY /dev/md/0  metadata=1.2 UUID=a75ac628:b384c441:157137ac:c04cd98c name=debian:0\n"
"ARRAY /dev/md/1  metadata=1.2 UUID=2dfb7fd5:e09e0527:0b5a905a:8334adb8 name=debian:1\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"

msgid "The contents of these last two lines doesn't depend on the list of disks included in the volume. It is therefore not necessary to regenerate these lines when replacing a failed disk with a new one. On the other hand, care must be taken to update the file when creating or deleting a RAID array."
msgstr "محتوای این دو خط آخر وابسته به تعداد دیسک‌های استفاده شده در آرایه نیست. پس هنگام جایگزینی یک دیسک معیوب با جدید نیازی به تولید مجدد این خطوط نیست. از طرف دیگر، در زمان ایجاد یا حذف یک آرایه RAID، این فایل باید بروزرسانی گردد."

msgid "LVM, the <emphasis>Logical Volume Manager</emphasis>, is another approach to abstracting logical volumes from their physical supports, which focuses on increasing flexibility rather than increasing reliability. LVM allows changing a logical volume transparently as far as the applications are concerned; for instance, it is possible to add new disks, migrate the data to them, and remove the old disks, without unmounting the volume."
msgstr "LVM که مخفف <emphasis>Logical Volume Manager</emphasis> است، روشی دیگر برای انتزاع دستگاه‌های منطقی از نمونه‌های فیزیکی است که بجای قابلیت اعتماد روی افزایش انعطاف‌پذیری تمرکز دارد. LVM امکان تغییر یک دستگاه منطقی را به شیوه‌ای شفاف برای برنامه‌های کاربردی آن بوجود می‌آورد؛ برای نمونه، امکان افزودن دیسک‌های جدید، مهاجرت داده به آن‌ها و حذف دیسک‌های قدیمی بدون قطع اتصال دستگاه مجازی وجود دارد."

msgid "LVM Concepts"
msgstr "مفاهیم LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>concept</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>PV</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Physical Volume</primary><see>PV</see>"
msgstr "<primary>Logical Volume Manager</primary>"

msgid "This flexibility is attained by a level of abstraction involving three concepts."
msgstr "این انعطاف‌پذیری از طریق یک سطح انتزاعی همراه با سه مفهوم بدست می‌آید."

msgid "First, the PV (<emphasis>Physical Volume</emphasis>) is the entity closest to the hardware: it can be partitions on a disk, or a full disk, or even any other block device (including, for instance, a RAID array). Note that when a physical element is set up to be a PV for LVM, it should only be accessed via LVM, otherwise the system will get confused."
msgstr "اول، PV یا <emphasis>Physical Volume</emphasis> نزدیک‌ترین موجودیت به سخت‌افزار است: می‌تواند پارتیشن‌های روی یک دیسک، یک دیسک کامل یا حتی سایر دستگاه‌های بلاک-محور (از جمله یک آرایه RAID) باشد. به یاد داشته باشید زمانی که یک عنصر فیزیکی به عنوان PV برای LVM تنظیم می‌شود، تنها باید توسط LVM قابل دسترس باشد در غیر اینصورت سیستم دچار سردرگمی می‌گردد."

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>VG</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Volume Group</primary><see>VG</see>"
msgstr "<primary>Logical Volume Manager</primary>"

#, fuzzy
#| msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there's no risk of using them directly."
msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there is no risk of using them directly."
msgstr "تعدادی از PVها می‌توانند درون یک خوشه VG یا <emphasis>Volume Group</emphasis> قرار بگیرند که می‌تواند با دیسک‌های مجازی و توسعه‌یافته مقایسه گردد. VGها انتزاعی هستند و درون سلسله‌مراتب <filename>/dev</filename> به عنوان یک فایل ظاهر نمی‌شوند، بنابراین خطری در استفاده مستقیم از آن‌ها وجود ندارد."

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>LV</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Logical Volume Manager</primary>"
msgid "<primary>Logical Volume</primary><see>LV</see>"
msgstr "<primary>Logical Volume Manager</primary>"

msgid "The third kind of object is the LV (<emphasis>Logical Volume</emphasis>), which is a chunk of a VG; if we keep the VG-as-disk analogy, the LV compares to a partition. The LV appears as a block device with an entry in <filename>/dev</filename>, and it can be used as any other physical partition can be (most commonly, to host a filesystem or swap space)."
msgstr "سومین مفهوم نیز LV یا <emphasis>Logical Volume</emphasis> نام دارد، که تکه‌ای از یک VG به حساب می‌آید؛ اگر VG را با یک دیسک مقایسه کنیم، LV مانند یک پارتیشن خواهد بود. LV به عنوان یک دستگاه بلاک-محور همراه با مدخلی در <filename>/dev</filename> ظاهر می‌شود، که می‌تواند به عنوان هر پارتیشن فیزیکی دیگر مورد استفاده قرار گیرد (بیشتر در مورد یک فایل سیستم میزبان یا فضای swap)."

msgid "The important thing is that the splitting of a VG into LVs is entirely independent of its physical components (the PVs). A VG with only a single physical component (a disk for instance) can be split into a dozen logical volumes; similarly, a VG can use several physical disks and appear as a single large logical volume. The only constraint, obviously, is that the total size allocated to LVs can't be bigger than the total capacity of the PVs in the volume group."
msgstr "نکته مهم در تقسیم یک VG به LV این است که کاملا مستقل از اجزای فیزیکی آن (PV) انجام می‌شود. یک VG تنها با یک قسمت فیزیکی (مانند یک دیسک) می‌تواند به چندین دستگاه منطقی تقسیم شود؛ به طور مشابه، یک VG با چندین دیسک فیزیکی می‌تواند به عنوان یک دستگاه منطقی بزرگ ظاهر شود. تنها محدودیت مشخص این است که اندازه کل اختصاص یافته به LVها نمی‌تواند بیشتر از مجموع اندازه PVها در گروه دستگاه‌ها باشد."

msgid "It often makes sense, however, to have some kind of homogeneity among the physical components of a VG, and to split the VG into logical volumes that will have similar usage patterns. For instance, if the available hardware includes fast disks and slower disks, the fast ones could be clustered into one VG and the slower ones into another; chunks of the first one can then be assigned to applications requiring fast data access, while the second one will be kept for less demanding tasks."
msgstr "اغلب منطقی است که به منظور داشتن همگنی بین اجزای فیزیکی یک VG، آن را به دستگاه‌های مجازی تقسیم کرد که الگوهای مشابهی در کارکرد داشته باشند. برای نمونه، اگر سخت‌افزار موجود شامل دیسک‌های سریع و کند باشد، دیسک‌های سریع می‌توانند درون یک VG و دیسک‌های کند درون دیگری قرار گیرند؛ تکه‌های اولی می‌توانند به برنامه‌هایی اختصاص یابند که نیازمند دسترسی سریع به دیسک هستند، در صورتی که از دومی برای سایر وظایف متداول دیسک استفاده می‌شود."

msgid "In any case, keep in mind that an LV isn't particularly attached to any one PV. It is possible to influence where the data from an LV are physically stored, but this possibility isn't required for day-to-day use. On the contrary: when the set of physical components of a VG evolves, the physical storage locations corresponding to a particular LV can be migrated across disks (while staying within the PVs assigned to the VG, of course)."
msgstr "در هر صورت، به خاطر بسپارید که یک LV به طور مشخص به هیچ PV متصل نیست. امکان تاثیرگذاری روی جایی که داده از یک LV به صورت فیزیکی می‌آید وجود دارد، اما این امکان برای کاربردهای روزانه الزامی نیست. از طرف دیگر، زمانی که مجموعه فیزیکی از اجزای یک VG گسترش می‌یابند، مکان ذخیره‌سازی فیزیکی منطبق با یک LV می‌توانند بین چند دیسک مهاجرت کنند (به صورتی که درون PVهای اختصاص‌یافته به VG قرار داشته باشند)."

msgid "Setting up LVM"
msgstr "برپایی LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>setup</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Let us now follow, step by step, the process of setting up LVM for a typical use case: we want to simplify a complex storage situation. Such a situation usually happens after some long and convoluted history of accumulated temporary measures. For the purposes of illustration, we'll consider a server where the storage needs have changed over time, ending up in a maze of available partitions split over several partially used disks. In more concrete terms, the following partitions are available:"
msgstr "بیایید فرآیند گام به گام برپایی LVM برای یک کاربرد متداول را دنبال کنیم: می‌خواهیم یک موقعیت ذخیره‌سازی پیچیده را ساده کنیم. چنین موقعیتی معمولا با گذشت زمان و گره خوردن مقیاس‌های موقتی انباشتگی صورت می‌گیرد. برای این منظور، سروری را در نظر می‌گیریم که نیازهای ذخیره‌سازی آن طی زمان تغییر کرده است که پارتیشن‌های موجود آن بین چندین دیسک فیزیکی مختلف به مانند یک مسیر مارپیچ قرار گرفته‌اند. به عبارت دیگر، پارتیشن‌های زیر موجود هستند:"

msgid "on the <filename>sdb</filename> disk, a <filename>sdb2</filename> partition, 4 GB;"
msgstr "درون دیسک <filename>sdb</filename>،‌ یک پارتیشن ۴ گیگابایت به نام <filename>sdb2</filename>؛"

msgid "on the <filename>sdc</filename> disk, a <filename>sdc3</filename> partition, 3 GB;"
msgstr "درون دیسک <filename>sdc</filename>،‌ یک پارتیشن ۳ گیگابایت به نام <filename>sdc3</filename>؛"

msgid "the <filename>sdd</filename> disk, 4 GB, is fully available;"
msgstr "دیسک <filename>sdd</filename>،‌ با ظرفیت ۴ گیگابایت کاملا موجود؛"

msgid "on the <filename>sdf</filename> disk, a <filename>sdf1</filename> partition, 4 GB; and a <filename>sdf2</filename> partition, 5 GB."
msgstr "درون دیسک <filename>sdf</filename>،‌ یک پارتیشن ۴ گیگابایت به نام <filename>sdf1</filename> و یک پارتیشن ۵ گیگابایت به نام <filename>sdf2</filename>."

msgid "In addition, let's assume that disks <filename>sdb</filename> and <filename>sdf</filename> are faster than the other two."
msgstr "علاوه بر این، تصور می‌کنیم که دیسک‌های <filename>sdb</filename> و <filename>sdf</filename> سریع‌تر از دو دیسک دیگر هستند."

msgid "Our goal is to set up three logical volumes for three different applications: a file server requiring 5 GB of storage space, a database (1 GB) and some space for back-ups (12 GB). The first two need good performance, but back-ups are less critical in terms of access speed. All these constraints prevent the use of partitions on their own; using LVM can abstract the physical size of the devices, so the only limit is the total available space."
msgstr "هدف ما برپایی یه دستگاه منطقی برای سه برنامه مختلف است: یک سرور فایل که به ۵ گیگابایت فضای ذخیره‌سازی نیاز دارد، یک پایگاه‌داده (۱ گیگابایت) و فضایی برای پشتیبان‌گیری (۱۲ گیگابایت). دوتای اول به عملکرد بالا نیاز دارند اما پشتیبان‌گیری چنین حساسیتی در دسترسی سریع ندارد. تمام این محدودیت‌ها از استفاده پارتیشن‌ها به صورت مستقیم جلوگیری می‌کنند؛ استفاده از LVM می‌تواند اندازه فیزیکی از دستگاه‌ها را انتزاعی کند، که تنها محدودیت آن مجموع فضای ذخیره‌سازی است."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">lvm2</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create PV</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The required tools are in the <emphasis role=\"pkg\">lvm2</emphasis> package and its dependencies. When they're installed, setting up LVM takes three steps, matching the three levels of concepts."
msgstr "ابزارهای مورد نیاز در بسته <emphasis role=\"pkg\">lvm2</emphasis> و وابستگی‌های آن قرار دارند. زمانی که نصب شوند، برپایی LVM شامل سه گام می‌شود که با سه سطح از مفاهیم آن مرتبط است."

msgid "First, we prepare the physical volumes using <command>pvcreate</command>:"
msgstr "ابتدا دستگاه‌های فیزیکی را با استفاده از <command>pvcreate</command> آماده‌سازی می‌کنیم:"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>pvcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>pvdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
#| "# </computeroutput><userinput>pvdisplay</userinput>\n"
#| "<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
#| "  --- NEW Physical volume ---\n"
#| "  PV Name               /dev/sdb2\n"
#| "  VG Name               \n"
#| "  PV Size               4.00 GiB\n"
#| "  Allocatable           NO\n"
#| "  PE Size               0   \n"
#| "  Total PE              0\n"
#| "  Free PE               0\n"
#| "  Allocated PE          0\n"
#| "  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
#| "\n"
#| "# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
#| "  Physical volume \"/dev/sdd\" successfully created\n"
#| "  Physical volume \"/dev/sdf1\" successfully created\n"
#| "  Physical volume \"/dev/sdf2\" successfully created\n"
#| "# </computeroutput><userinput>pvdisplay -C</userinput>\n"
#| "<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
#| "  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
#| "  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
#| "  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
#| "  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
#| "  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdb2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay\n"
"</userinput><computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               yK0K6K-clbc-wt6e-qk9o-aUh9-oQqC-k1T71B\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdc3\" successfully created.\n"
"  Physical volume \"/dev/sdd\" successfully created.\n"
"  Physical volume \"/dev/sdf1\" successfully created.\n"
"  Physical volume \"/dev/sdf2\" successfully created.\n"
"# </computeroutput><userinput>pvdisplay -C\n"
"</userinput><computeroutput>  PV         VG Fmt  Attr PSize PFree\n"
"  /dev/sdb2     lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3     lvm2 ---  3.00g 3.00g\n"
"  /dev/sdd      lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1     lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf2     lvm2 ---  5.00g 5.00g\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
"  Physical volume \"/dev/sdd\" successfully created\n"
"  Physical volume \"/dev/sdf1\" successfully created\n"
"  Physical volume \"/dev/sdf2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay -C</userinput>\n"
"<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
"  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
"  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
"  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
"</computeroutput>"

msgid "So far, so good; note that a PV can be set up on a full disk as well as on individual partitions of it. As shown above, the <command>pvdisplay</command> command lists the existing PVs, with two possible output formats."
msgstr "تا اینجا مشکلی نیست؛ به یاد داشته باشید که یک PV می‌تواند روی یک دیسک کامل یا پارتیشن‌های انفرادی ایجاد گردد. دستور <command>pvdisplay</command> فهرستی از PVها را با دو قالب خروجی ممکن نمایش می‌دهد."

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>vgcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>vgdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "Now let's assemble these physical elements into VGs using <command>vgcreate</command>. We'll gather only PVs from the fast disks into a <filename>vg_critical</filename> VG; the other VG, <filename>vg_normal</filename>, will also include slower elements."
msgstr "اکنون بیایید این عناصر فیزیکی را با استفاده از <command>vgcreate</command> درون VG قرار دهیم. PV دیسک‌های سریع را درون VG به نام <filename>vg_critical</filename> و دیسک‌های کند را درون VG به نام <filename>vg_normal</filename> قرار می‌دهیم."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create VG</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
#| "<computeroutput>  No volume groups found\n"
#| "# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay</userinput>\n"
#| "<computeroutput>  --- Volume group ---\n"
#| "  VG Name               vg_critical\n"
#| "  System ID             \n"
#| "  Format                lvm2\n"
#| "  Metadata Areas        2\n"
#| "  Metadata Sequence No  1\n"
#| "  VG Access             read/write\n"
#| "  VG Status             resizable\n"
#| "  MAX LV                0\n"
#| "  Cur LV                0\n"
#| "  Open LV               0\n"
#| "  Max PV                0\n"
#| "  Cur PV                2\n"
#| "  Act PV                2\n"
#| "  VG Size               8.09 GiB\n"
#| "  PE Size               4.00 MiB\n"
#| "  Total PE              2071\n"
#| "  Alloc PE / Size       0 / 0   \n"
#| "  Free  PE / Size       2071 / 8.09 GiB\n"
#| "  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
#| "\n"
#| "# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
#| "<computeroutput>  Volume group \"vg_normal\" successfully created\n"
#| "# </computeroutput><userinput>vgdisplay -C</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
#| "  vg_critical   2   0   0 wz--n-  8.09g  8.09g\n"
#| "  vg_normal     3   0   0 wz--n- 12.30g 12.30g\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1\n"
"</userinput><computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay\n"
"</userinput><computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               7.99 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2046\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2046 / 7.99 GiB\n"
"  VG UUID               JgFWU3-emKg-9QA1-stPj-FkGX-mGFb-4kzy1G\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2\n"
"</userinput><computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C\n"
"</userinput><computeroutput><![CDATA[  VG          #PV #LV #SN Attr   VSize   VFree  \n"
"  vg_critical   2   0   0 wz--n-   7.99g   7.99g\n"
"  vg_normal     3   0   0 wz--n- <11.99g <11.99g\n"
"]]></computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  No volume groups found\n"
"# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               8.09 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2071\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2071 / 8.09 GiB\n"
"  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
"  vg_critical   2   0   0 wz--n-  8.09g  8.09g\n"
"  vg_normal     3   0   0 wz--n- 12.30g 12.30g\n"
"</computeroutput>"

msgid "Here again, commands are rather straightforward (and <command>vgdisplay</command> proposes two output formats). Note that it is quite possible to use two partitions of the same physical disk into two different VGs. Note also that we used a <filename>vg_</filename> prefix to name our VGs, but it is nothing more than a convention."
msgstr "در اینجا نیز دستورات واضح هستند و <command>vgdisplay</command> شامل دو قالب خروجی است. به یاد داشته باشید که امکان استفاده از دو پارتیشن یک دیسک فیزیکی درون دو VG مختلف وجود دارد. ما از یک پیشوند <filename>vg_</filename> برای نامگذاری VGها استفاده کردیم، اما چیزی بیشتر از رعایت یک استاندارد نیست."

#, fuzzy
#| msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB, respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgstr "اکنون دو “دیسک مجازی” به اندازه‌های ۸ و ۱۲ گیگابایت داریم. حال بیایید آن‌ها را به “پارتیشن‌های مجازی” یا LV تقسیم کنیم. اینکار با استفاده از دستور <command>lvcreate</command> و شیوه نگارشی پیچیده‌تر از گام‌های قبلی صورت می‌گیرد:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>create LV</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>lvcreate</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>lvdisplay</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_files\" created\n"
#| "# </computeroutput><userinput>lvdisplay</userinput>\n"
#| "<computeroutput>  --- Logical volume ---\n"
#| "  LV Path                /dev/vg_critical/lv_files\n"
#| "  LV Name                lv_files\n"
#| "  VG Name                vg_critical\n"
#| "  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
#| "  LV Write Access        read/write\n"
#| "  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
#| "  LV Status              available\n"
#| "  # open                 0\n"
#| "  LV Size                5.00 GiB\n"
#| "  Current LE             1280\n"
#| "  Segments               2\n"
#| "  Allocation             inherit\n"
#| "  Read ahead sectors     auto\n"
#| "  - currently set to     256\n"
#| "  Block device           253:0\n"
#| "\n"
#| "# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_base\" created\n"
#| "# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
#| "<computeroutput>  Logical volume \"lv_backups\" created\n"
#| "# </computeroutput><userinput>lvdisplay -C</userinput>\n"
#| "<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_base    vg_critical -wi-a---  1.00g                                           \n"
#| "  lv_files   vg_critical -wi-a---  5.00g                                           \n"
#| "  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>lvdisplay\n"
"</userinput><computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical\n"
"</userinput><computeroutput>  Logical volume \"lv_files\" created.\n"
"# </computeroutput><userinput>lvdisplay\n"
"</userinput><computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                Nr62xe-Zu7d-0u3z-Yyyp-7Cj1-Ej2t-gw04Xd\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time debian, 2022-03-01 00:17:46 +0100\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical\n"
"</userinput><computeroutput>  Logical volume \"lv_base\" created.\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 11.98G vg_normal\n"
"</userinput><computeroutput>  Rounding up size to full physical extent 11.98 GiB\n"
"  Rounding up size to full physical extent 11.98 GiB\n"
"  Logical volume \"lv_backups\" created.\n"
"# </computeroutput><userinput>lvdisplay -C\n"
"</userinput><computeroutput>  LV         VG          Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_base    vg_critical -wi-a-----  1.00g                                                    \n"
"  lv_files   vg_critical -wi-a-----  5.00g                                                    \n"
"  lv_backups vg_normal   -wi-a----- 11.98g             \n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
"<computeroutput>  Logical volume \"lv_backups\" created\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"

msgid "Two parameters are required when creating logical volumes; they must be passed to the <command>lvcreate</command> as options. The name of the LV to be created is specified with the <literal>-n</literal> option, and its size is generally given using the <literal>-L</literal> option. We also need to tell the command what VG to operate on, of course, hence the last parameter on the command line."
msgstr "برای ایجاد دستگاه‌های منطقی دو پارامتر مورد نیاز است؛ که باید به صورت گزینه‌ها به <command>lvcreate</command> ارسال گردند. نام LV که قصد ایجاد آن را داریم با گزینه <literal>-n</literal> و اندازه آن با گزینه <literal>-L</literal> مشخص می‌شود. البته، به دستور باید اعلام کنیم که از کدام VG می‌خواهیم استفاده شود."

msgid "<emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> options"
msgstr "<emphasis>مطالعه بیشتر</emphasis> گزینه‌های <command>lvcreate</command>"

msgid "The <command>lvcreate</command> command has several options to allow tweaking how the LV is created."
msgstr "دستور <command>lvcreate</command> شامل چندین گزینه است که چگونگی ایجاد LV را مشخص می‌کند."

msgid "Let's first describe the <literal>-l</literal> option, with which the LV's size can be given as a number of blocks (as opposed to the “human” units we used above). These blocks (called PEs, <emphasis>physical extents</emphasis>, in LVM terms) are contiguous units of storage space in PVs, and they can't be split across LVs. When one wants to define storage space for an LV with some precision, for instance to use the full available space, the <literal>-l</literal> option will probably be preferred over <literal>-L</literal>."
msgstr "ابتدا بیایید گزینه <literal>-l</literal> را توضیح دهیم، که با استفاده از آن اندازه LV می‌تواند به عنوان تعداد بلاک‌ها در مقایسه با واحدهای “انسانی” که در بالا استفاده کردیم مشخص گردد. این بلاک‌ها که به نام PE یا <emphasis>physical extents</emphasis> در زبان LVM مطرح می‌شوند، واحدهای پیوسته از فضای ذخیره‌سازی در PVها می‌باشند که نمی‌توان آن‌ها را بین LVها تقسیم کرد. زمانی که می‌خواهیم یک فضای ذخیره‌سازی با دقت بالا را برای یک LV تعریف کنیم، برای نمونه استفاده از فضای کامل، استفاده از گزینه <literal>-l</literal> بر <literal>-L</literal> اولویت پیدا می‌کند."

#, fuzzy
#| msgid "It's also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgid "It is also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgstr "همچنین امکان اشاره به مکان فیزیکی یک LV به صورتی که محدوده آن درون یک PV مشخص ذخیره‌سازی شود وجود دارد (البته، با استفاده از گزینه‌های اختصاص‌یافته به VG). از آنجا که می‌دانیم <filename>sdb</filename> از <filename>sdf</filename> سریع‌تر است، اگر بخواهیم برای سرور پایگاه‌داده در مقایسه با سرور فایل برتری قائل شویم می‌توانیم <filename>lv_base</filename> را در‌ آن ذخیره کنیم. پس دستور آن می‌شود: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. به یاد داشته باشید که در صورت نبود محدوده کافی در PV این دستور ناموفق خواهد بود. در مثال ما، برای پیشگیری از این وضعیت، شاید بخواهیم <filename>lv_base</filename> را قبل از <filename>lv_files</filename> ایجاد یا برخی فضای موجود در <filename>sdb2</filename> را با دستور <command>pvmove</command> آزاد کنیم."

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>pvmove</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "Logical volumes, once created, end up as block device files in <filename>/dev/mapper/</filename>:"
msgstr "گروه‌های مجازی، زمانی که ایجاد گردند، به عنوان فایل‌های دستگاه درون <filename>/dev/mapper/</filename> قرار می‌گیرند:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/dev</filename></primary><secondary><filename>/dev/mapper/</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>device</primary><secondary>block</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
#| "<computeroutput>total 0\n"
#| "crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
#| "lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
#| "# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
#| "<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
#| "brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
#| "brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"crw------- 1 root root 10, 236 Mar  1 00:17 control\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:19 vg_critical-lv_base -> ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:17 vg_critical-lv_files -> ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Mar  1 00:19 vg_normal-lv_backups -> ../dm-2 ]]>\n"
"# </computeroutput><userinput>ls -l /dev/dm-*\n"
"</userinput><computeroutput>brw-rw---- 1 root disk 253, 0 Mar  1 00:17 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Mar  1 00:19 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Mar  1 00:19 /dev/dm-2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"

#, fuzzy
#| msgid "<emphasis>NOTE</emphasis> Autodetecting LVM volumes"
msgid "<emphasis>NOTE</emphasis> Auto-detecting LVM volumes"
msgstr "<emphasis>یادداشت</emphasis> شناسایی خودکار گروه‌های LVM"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>lvm2-activation.service</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>vgchange</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "When the computer boots, the <filename>lvm2-activation</filename> systemd service unit executes <command>vgchange -aay</command> to “activate” the volume groups: it scans the available devices; those that have been initialized as physical volumes for LVM are registered into the LVM subsystem, those that belong to volume groups are assembled, and the relevant logical volumes are started and made available. There is therefore no need to edit configuration files when creating or modifying LVM volumes."
msgstr "زمانی که رایانه راه‌اندازی می‌شود واحد سرویس systemd به نام <filename>lvm2-activation</filename> به اجرای <command>vgchange -aay</command> می‌پردازد که اینکار گروه‌های مجازی را “فعال” می‌کند: ابتدا به پویش دستگاه‌های موجود می‌پردازد؛ آن‌هایی که توسط گروه‌های فیزیکی برای LVM آماده‌سازی شده‌اند درون زیرسیستم آن قرار می‌گیرند، آن‌هایی که متعلق به گروه‌های مجازی باشند گردآوری شده و گروه‌های مجازی آن أغاز و قابل استفاده می‌گردند. بنابراین هنگام ایجاد یا تغییر گروه‌های LVM نیازی به ویرایش فایل‌های پیکربندی نیست."

msgid "Note, however, that the layout of the LVM elements (physical and logical volumes, and volume groups) is backed up in <filename>/etc/lvm/backup</filename>, which can be useful in case of a problem (or just to sneak a peek under the hood)."
msgstr "با این حال، به یاد داشته باشید که ساختار عناصر LVM (گروه‌های فیزیکی و منطقی همراه با گروه‌های دستگاه) در <filename>/etc/lvm/backup</filename> پشتیبان‌گیری می‌شوند، که می‌تواند در زمان بروز مشکل (یا اطلاع از عملکرد آن) مورد استفاده قرار گیرد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/lvm/backup</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "To make things easier, convenience symbolic links are also created in directories matching the VGs:"
msgstr "برای ساده‌تر کردن کارها، پیوندهای نمادین متعارف نیز در دایرکتوری‌های شامل VGها ایجاد شده است:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
#| "<computeroutput>total 0\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
#| "# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
#| "<computeroutput>total 0\n"
#| "lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:19 lv_base -> ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:17 lv_files -> ../dm-0 ]]>\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal\n"
"</userinput><computeroutput><![CDATA[total 0\n"
"lrwxrwxrwx 1 root root 7 Mar  1 00:19 lv_backups -> ../dm-2 ]]>\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"

msgid "The LVs can then be used exactly like standard partitions:"
msgstr "سپس LVها می‌توانند مانند پارتیشن‌های استاندارد مورد استفاده قرار گیرند:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
#| "<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
#| "Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
#| "Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
#| "[...]\n"
#| "Creating journal (32768 blocks): done\n"
#| "Writing superblocks and filesystem accounting information: done \n"
#| "# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
#| "<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
#| "<computeroutput>[...]\n"
#| "/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
#| "/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
#| "/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups\n"
"</userinput><computeroutput>mke2fs 1.46.2 (28-Feb-2021)\n"
"Discarding device blocks: done                            \n"
"Creating filesystem with 3140608 4k blocks and 786432 inodes\n"
"Filesystem UUID: 7eaf0340-b740-421e-96b2-942cdbf29cb3\n"
"Superblock backups stored on blocks: \n"
"\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (16384 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"\n"
"# </computeroutput><userinput>mkdir /srv/backups\n"
"</userinput><computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups\n"
"</userinput><computeroutput># </computeroutput><userinput>df -h /srv/backups\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   24K   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab\n"
"</userinput><computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
"Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
"[...]\n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"

msgid "From the applications' point of view, the myriad small partitions have now been abstracted into one large 12 GB volume, with a friendlier name."
msgstr "از دید برنامه‌های کاربردی، تعداد بیشمار پارتیشن‌ها اکنون به یک دستگاه بزرگ ۱۲ گیگابایت تبدیل شده است که نام راحت‌تری نیز دارد."

msgid "LVM Over Time"
msgstr "LVM در گذر زمان"

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>lvresize</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>resize2fs</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>volume</primary><secondary>resize</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>resize LV</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Even though the ability to aggregate partitions or physical disks is convenient, this is not the main advantage brought by LVM. The flexibility it brings is especially noticed as time passes, when needs evolve. In our example, let's assume that new large files must be stored, and that the LV dedicated to the file server is too small to contain them. Since we haven't used the whole space available in <filename>vg_critical</filename>, we can grow <filename>lv_files</filename>. For that purpose, we'll use the <command>lvresize</command> command, then <command>resize2fs</command> to adapt the filesystem accordingly:"
msgstr "با اینکه توانایی گردآوری پارتیشن‌‌ها یا دیسک‌های فیزیکی بسیار متداول است، این تنها مزیت استفاده از LVM نیست. انعطاف‌پذیری آن در گذر زمان و تغییر رویکرد ذخیره‌سازی، مشخص می‌شود. در مثال ما، تصور کنیم که فایل‌های بزرگ جدیدی قرار است درون سرور فایل قرار گیرند که LV اختصاص‌یافته به آن گنجایش کافی را ندارد. از آنجا که از تمام فضای <filename>vg_critical</filename> استفاده نکرده‌ایم، می‌توانیم <filename>lv_files</filename> را گسترش دهیم. برای این منظور، با استفاده از دستور <command>lvresize</command> و <command>resize2fs</command> برای سازگاری فایل سیستم اینکار صورت می‌گیرد:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 5.00g\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
#| "# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
#| "  Logical volume lv_files successfully resized\n"
#| "# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
#| "<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
#| "  lv_files vg_critical -wi-ao-- 7.00g\n"
#| "# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
#| "<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
#| "Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
#| "old_desc_blocks = 1, new_desc_blocks = 1\n"
#| "The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
#| "\n"
#| "# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
#| "<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  4.9G  4.2G  485M  90% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files\n"
"</userinput><computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 5.00g                                                    \n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 7.99g 1.99g\n"
"# </computeroutput><userinput>lvresize -L 6G vg_critical/lv_files\n"
"</userinput><computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 6.00 GiB (1536 extents).\n"
"  Logical volume vg_critical/lv_files successfully resized.\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files\n"
"</userinput><computeroutput>  LV       VG          Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n"
"  lv_files vg_critical -wi-ao---- 6.00g                                                    \n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files\n"
"</userinput><computeroutput>resize2fs 1.46.2 (28-Feb-2021)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1572864 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/\n"
"</userinput><computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.9G  4.2G  1.5G  75% /srv/files\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
"# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
"  Logical volume lv_files successfully resized\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 7.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"

msgid "<emphasis>CAUTION</emphasis> Resizing filesystems"
msgstr "<emphasis>احتیاط</emphasis> تغییر اندازه فایل سیستم‌ها"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>filesystem</primary><secondary>resize</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It's rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It is rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgstr "تمام فایل سیستم‌ها نمی‌توانند به صورت آنلاین تغییر اندازه یابند؛ تغییر اندازه یک دستگاه ابتدا نیازمند قطع اتصال آن به فایل سیستم سپس اتصال مجدد آن می‌شود. البته، اگر کسی بخواهد فضای اختصاص‌یافته به یک LV را کاهش دهد، ابتدا فایل سیستم باید کاهش پیدا کند؛ این ترتیب در زمان افزایش اندازه برعکس می‌شود: گروه مجازی قبل از فایل سیستم موجود در آن باید افزایش پیدا کند. این فرآیند بسیار واضح است، چرا که در هر زمان اندازه فایل سیستم نباید از اندازه دستگاه بلاک-محور روی آن بیشتر باشد (خواه این دستگاه یک پارتیشن فیزیکی باشد یا یک گروه مجازی)."

msgid "The ext3, ext4 and xfs filesystems can be grown online, without unmounting; shrinking requires an unmount. The reiserfs filesystem allows online resizing in both directions. The venerable ext2 allows neither, and always requires unmounting."
msgstr "فایل سیستم‌های ext3، ext4 و zfs بدون نیاز به قطع اتصال می‌توانند افزایش یابند؛ کاهش اندازه نیازمند قطع اتصال است. فایل سیستم reiserfs امکان تغییر اندازه آنلاین را در دو جهت فراهم می‌سازد. ext2 مقدس، اما نیازمند قطع اتصال در دو جهت است."

msgid "We could proceed in a similar fashion to extend the volume hosting the database, only we've reached the VG's available space limit:"
msgstr "برای توسعه گروهی که از پایگاه‌داده میزبانی می‌کند نیز به همین ترتیب می‌توان اقدام کرد، با این تفاوت که به انتهای فضای ذخیره‌سازی موجود VG رسیدیم:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
#| "  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/\n"
"</userinput><computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  974M  883M   25M  98% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput>  VG          #PV #LV #SN Attr   VSize VFree   \n"
"  vg_critical   2   2   0 wz--n- 7.99g 1016.00m\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
"  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"

#, fuzzy
#| msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb1</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb3</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgstr "ایرادی ندارد، چرا که LVM امکان افزودن گروه‌های فیزیکی را به گروه‌های دستگاه موجود فراهم می‌سازد. برای نمونه، شاید متوجه شده‌ایم پارتیشن <filename>sdb1</filename>، که خارج از LVM استفاده می‌شود، تنها شامل بایگانی‌هایی است که می‌تواند به <filename>lv_backups</filename> انتقال یابد. اکنون می‌توانیم آن را بازیابی کرده و درون گروه مجازی قرار دهیم، در نتیجه برخی فضای موجود را احیا می‌کنیم. اینکار با استفاده از دستور <command>vgextend</command> صورت می‌گیرد. البته که پارتیشن ابتدا باید به صورت یک گروه فیزیکی آماده شود. زمانی که VG گسترش یافت، از دستورات مشابه می‌توانیم برای افزایش اندازه گروه مجازی و فایل سیستم استفاده کنیم:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
#| "<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
#| "# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
#| "<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
#| "# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
#| "<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
#| "  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
#| "# </computeroutput><userinput>[...]</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
#| "<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
#| "/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb3\n"
"</userinput><computeroutput>  Physical volume \"/dev/sdb3\" successfully created.\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb3\n"
"</userinput><computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical\n"
"</userinput><computeroutput><![CDATA[  VG          #PV #LV #SN Attr   VSize   VFree \n"
"  vg_critical   3   2   0 wz--n- <12.99g <5.99g ]]>\n"
"# </computeroutput><userinput>lvresize -L 2G vg_critical/lv_base\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_base\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/\n"
"</userinput><computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  886M  991M  48% /srv/base\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"

msgid "<emphasis>GOING FURTHER</emphasis> Advanced LVM"
msgstr "<emphasis>مطالعه بیشتر</emphasis> LVM پیشرفته"

#, fuzzy
#| msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance, to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgstr "LVM همچنین کاربردهای پیشرفته‌تری نیز دارد که جزئیات آن می‌تواند به صورت دستی وارد شود. برای نمونه، یک مدیر سیستم می‌تواند اندازه بلاک‌های گروه‌های فیزیکی و منطقی را تغییر دهد، همین طور ساختار فیزیکی آن‌ها را. همچنین امکان انتقال بلاک‌ها بین PVها موجود است، برای نمونه به منظور بهبود عملکرد یا آزاد کردن یک PV زمانی که نیاز به استخراج یک دیسک فیزیکی منطبق با خود از VG باشد (خواه با تاثیر روی VG دیگر یا حذف از LVM به صورت کلی). صفحات راهنمای دستورات معمولا واضح بوده و جزئیات بیشتری را مطرح می‌کنند. یک نقطه شروع مناسب صفحه راهنمای <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> است."

msgid "RAID or LVM?"
msgstr "RAID یا LVM؟"

msgid "RAID and LVM both bring indisputable advantages as soon as one leaves the simple case of a desktop computer with a single hard disk where the usage pattern doesn't change over time. However, RAID and LVM go in two different directions, with diverging goals, and it is legitimate to wonder which one should be adopted. The most appropriate answer will of course depend on current and foreseeable requirements."
msgstr "RAID و LVM بدون تردید دارای مزایایی هستند که در صورت کنار گذاشتن فرضیه یک رایانه رومیزی همراه با یک هارد دیسک که کارکرد آن در طول زمان تغییر نمی‌کند، مشخص می‌شوند. اگرچه، RAID و LVM در دو جهت مختلف حرکت می‌کنند و اهداف متفاوتی نیز دارند، که گاهی تصمیم‌گیری درباره استفاده صحیح از آن‌ها را دشوار می‌سازد. مناسب‌ترین پاسخ بستگی به نیازمندی‌های فعلی و قابل پیشبینی سیستم در آینده دارد."

msgid "There are a few simple cases where the question doesn't really arise. If the requirement is to safeguard data against hardware failures, then obviously RAID will be set up on a redundant array of disks, since LVM doesn't really address this problem. If, on the other hand, the need is for a flexible storage scheme where the volumes are made independent of the physical layout of the disks, RAID doesn't help much and LVM will be the natural choice."
msgstr "موارد ساده‌ای وجود دارد که پرسشی درباره آن‌ها مطرح نمی‌شود. اگر نیازمندی این باشد که داده برابر نقص سخت‌افزاری محافظت گردد، به طور مشخص باید از RAID به صورت آرایه‌ای از دیسک‌ها استفاده کرد، چرا که LVM درباره این مشکل راهکاری ندارد. از طرف دیگر، اگر نیازمندی این باشد که یک طرح ذخیره‌سازی انعطاف‌پذیر از گروه‌های مستقل ساختار فیزیکی دیسک‌ها تشکیل گردد، به طور مشخص باید از LVM استفاده کرد چرا که RAID درباره این مشکل راهکاری ندارد."

msgid "<emphasis>NOTE</emphasis> If performance matters…"
msgstr "<emphasis>یادداشت</emphasis> اگر عملکرد مهم باشد..."

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>SSD</primary>"
msgstr "<primary>RAID</primary>"

msgid "<primary>Solid State Drives</primary><see>SSD</see>"
msgstr ""

#, fuzzy
#| msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgstr "اگر سرعت ورودی/خروجی مطرح باشد، به خصوص در مورد زمان دسترسی، استفاده از LVM و/یا RAID با یکی از ترکیبات موجود ممکن است روی عملکرد تاثیر منفی بگذارد، که اینکار روی تصمیم‌گیری درباره انتخاب هر کدام اثر می‌گذارد. اگرچه، این تفاوت‌ها در عملکرد بسیار ناچیز بوده و تنها در چند مورد خاص قابل اندازه‌گیری می‌باشند. اگر عملکرد مهم باشد، بهترین گزینه استفاده از رسانه ذخیره‌سازی غیرقابل-چرخش است (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> یا SSD)؛ هزینه آن‌ها برای هر مگابایت از دیسک‌ها معمولی بالاتر و ظرفیت کمتری نسبت به آن‌ها دارند، اما عملکرد فوق‌العاده‌ای درباره دسترسی تصادفی به حافظه دارند. اگر الگوی کارکرد شامل عملیات ورودی/خروجی باشد که در میان فایل سیستم پراکنده شده است، برای نمونه پرس و جوهای پیچیده پایگاه‌داده، آنگاه مزیت اجرای آن‌ها روی SSD به مراتب بیشتر از استفاده LVM یا RAID می‌تواند باشد. در این مواقع، انتخاب باید توسط سایر نیازمندی‌ها بجز سرعت صورت بگیرد، چرا که جنبه عملکرد آن به راحتی توسط SSD مدیریت می‌شود."

msgid "The third notable use case is when one just wants to aggregate two disks into one volume, either for performance reasons or to have a single filesystem that is larger than any of the available disks. This case can be addressed both by a RAID-0 (or even linear-RAID) and by an LVM volume. When in this situation, and barring extra constraints (for instance, keeping in line with the rest of the computers if they only use RAID), the configuration of choice will often be LVM. The initial set up is barely more complex, and that slight increase in complexity more than makes up for the extra flexibility that LVM brings if the requirements change or if new disks need to be added."
msgstr "سومین کارکرد قابل ذکر زمانی است که بخواهیم دو دیسک را در یک گروه بزرگ‌تر قرار دهیم، خواه به دلایل عملکرد یا داشتن یک فایل سیستم بزرگ‌تر از فضای هر کدام از دیسک‌ها). اینکار با استفاده از یک RAID-0 (یا حتی RAID خطی) و گروه LVM انجام می‌شود. در این موقعیت، بجز محدودیت‌های خارجی (برای نمونه، در چارچوب سایر رایانه‌ها بودن اگر آن‌ها نیز تنها از RAID استفاده کنند)، پیکربندی مورد نظر معمولا LVM خواهد بود. راه‌اندازی اولیه به ندرت پیچیده‌تر خواهد بود و این افزایش پیچیدگی در صورت تغییر نیازمندی‌ها یا افزودن دیسک‌ها جدید قابلیت انعطاف‌پذیری بیشتری را فراهم می‌آورد."

msgid "Then of course, there is the really interesting use case, where the storage system needs to be made both resistant to hardware failure and flexible when it comes to volume allocation. Neither RAID nor LVM can address both requirements on their own; no matter, this is where we use both at the same time — or rather, one on top of the other. The scheme that has all but become a standard since RAID and LVM have reached maturity is to ensure data redundancy first by grouping disks in a small number of large RAID arrays, and to use these RAID arrays as LVM physical volumes; logical partitions will then be carved from these LVs for filesystems. The selling point of this setup is that when a disk fails, only a small number of RAID arrays will need to be reconstructed, thereby limiting the time spent by the administrator for recovery."
msgstr "البته یک مورد بسیار جالب نیز وجود دارد، که سیستم ذخیره‌سازی هم باید برابر نقص سخت‌افزاری مقاوم هم انعطاف‌پذیری لازم درباره اختصاص گروه‌های مختلف را داشته باشد. هیچ یک از راهکارهای RAID یا LVM به تنهایی نمی‌توانند این نیازمندی را پوشش دهند؛ اینجا است که از هر دو به صورت همزمان استفاده می‌کنیم - یا یکی بر فراز دیگری. طرح کلی در این مورد و با توجه به اینکه این دو فناوری به بلوغ رسیده‌اند این است که ابتدا با گروه‌بندی دیسک‌ها در تعداد آرایه‌های کوچک RAID از افزونگی داده اطمینان حاصل کرد سپس از این آرایه‌ها برای گروه‌های فیزیکی LVM استفاده کنیم؛ پارتیشن‌های منطقی از این LVها برای فایل سیستم بوجود می‌آیند. نتیجه این تنظیم این است که هنگامی که یک دیسک دچار نقص می‌گردد، تنها تعداد کمی از آرایه‌های RAID نیازمند بازسازی هستند، که اینکار زمان سپری شده توسط مدیر سیستم برای بازیابی را کاهش می‌دهد."

#, fuzzy
#| msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited videoclips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited video clips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgstr "بیایید یک مثال واقعی را بررسی کنیم: دپارتمان روابط عمومی در شرکت فالکوت نیازمند یک سیستم برای ویرایش تصویر است، اما هزینه‌های دپارتمان اجازه سرمایه‌گذاری در سخت‌افزارهای گران قیمت را نمی‌دهد. تصمیم گرفته شد که از سخت‌افزار مربوط به کار گرافیکی (مانیتور و کارت گرافیک) و از سخت‌افزار متداول تنها برای ذخیره‌سازی اطلاعات استفاده شود. اگرچه، همانطور که مشخص است، ویدیو دیجیتال نیازمندی ذخیره‌سازی مربوط به خود را دارد: حجم داده قابل ذخیره‌سازی زیاد است، پس نرخ تبادل داده برای خواندن و نوشتن در عملکرد کلی سیستم تاثیرگذار خواهد بود (برای نمونه، بیش از زمان دسترسی متداول). این محدودیت‌ها باید با سخت‌افزار عمومی موجود برطرف گردند که در این مورد دو هارد دیسک ۳۰۰ گیگابایت از نوع SATA می‌باشند؛ داده سیستمی همراه با داده کاربری باید برابر نقص‌های سخت‌افزاری نیز مقاوم گردند. ویدیو کلیپ‌های ویرایش شده باید از امنیت واقعی برخوردار بوده، اما ویدیوهای اولیه که منتظر ویرایش هستند از اهمیت کمتری برخوردارند، چرا که نسخه اصلی آن‌ها روی نوارهای ویدیویی موجود است."

msgid "RAID-1 and LVM are combined to satisfy these constraints. The disks are attached to two different SATA controllers to optimize parallel access and reduce the risk of a simultaneous failure, and they therefore appear as <filename>sda</filename> and <filename>sdc</filename>. They are partitioned identically along the following scheme:"
msgstr "از RAID-1 و LVM به صورت ترکیبی برای رفع این محدودیت‌ها استفاده شده است. دیسک‌ها به دو کنترلر مختلف SATA به منظور بهینه‌سازی دسترسی موازی و کاهش خطر نقص همزمان متصل شده‌اند که به عنوان <filename>sda</filename> و <filename>sdc</filename> ظاهر می‌شوند. آن‌ها با توجه به طرح زیر پارتیشن‌بندی شده‌اند:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
#| "<computeroutput>\n"
#| "Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
#| "Units: sectors of 1 * 512 = 512 bytes\n"
#| "Sector size (logical/physical): 512 bytes / 512 bytes\n"
#| "I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
#| "Disklabel type: dos\n"
#| "Disk identifier: 0x00039a9f\n"
#| "\n"
#| "Device    Boot     Start       End   Sectors Size Id Type\n"
#| "/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
#| "/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
#| "/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
#| "/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
#| "/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
#| "/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>sfdisk -l /dev/sda\n"
"</userinput><computeroutput>Disk /dev/sda: 894.25 GiB, 960197124096 bytes, 1875385008 sectors\n"
"Disk model: SAMSUNG MZ7LM960\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: gpt\n"
"Disk identifier: BB14C130-9E9A-9A44-9462-6226349CA012\n"
"\n"
"Device         Start        End   Sectors   Size Type\n"
"/dev/sda1        2048       4095      2048     1M BIOS boot\n"
"/dev/sda2        4096  100667391 100663296    48G Linux RAID\n"
"/dev/sda3   100667392  134221823  33554432    16G Linux RAID\n"
"/dev/sda4   134221824  763367423 629145600   300G Linux RAID\n"
"/dev/sda5   763367424 1392513023 629145600   300G Linux RAID\n"
"/dev/sda6  1392513024 1875384974 482871951 230.3G Linux LVM\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"

msgid "The first partitions of both disks are BIOS boot partitions."
msgstr ""

#, fuzzy
#| msgid "The first partitions of both disks (about 1 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgid "The next two partitions <filename>sda2</filename> and <filename>sdc2</filename> (about 48 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgstr "اولین پارتیشن‌های هر دو دیسک (حدود ۱ گیگابایت) به صورت یک آرایه RAID-1 در آمده‌اند، <filename>md0</filename>. از این mirror به طور مستقیم برای ذخیره‌سازی فایل سیستم root استفاده می‌شود."

#, fuzzy
#| msgid "The <filename>sda2</filename> and <filename>sdc2</filename> partitions are used as swap partitions, providing a total 2 GB of swap space. With 1 GB of RAM, the workstation has a comfortable amount of available memory."
msgid "The <filename>sda3</filename> and <filename>sdc3</filename> partitions are assembled into a RAID-0 volume, <filename>md1</filename>, and used as swap partition, providing a total 32 GB of swap space. Modern systems can provide plenty of RAM and our system won't need hibernation. So with this amount added, our system will unlikely run out of memory."
msgstr "پارتیشن‌های <filename>sda2</filename> و <filename>sdc2</filename> به عنوان swap استفاده شده‌اند، که مجموع فضای ۲ گیگابایت برای swap را فراهم می‌کنند. با ۱ گیگابایت RAM، رایانه مقدار کافی حافظه موجود را خواهد داشت."

#, fuzzy
#| msgid "The <filename>sda5</filename> and <filename>sdc5</filename> partitions, as well as <filename>sda6</filename> and <filename>sdc6</filename>, are assembled into two new RAID-1 volumes of about 100 GB each, <filename>md1</filename> and <filename>md2</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 200 GB of safe space."
msgid "The <filename>sda4</filename> and <filename>sdc4</filename> partitions, as well as <filename>sda5</filename> and <filename>sdc5</filename>, are assembled into two new RAID-1 volumes of about 300 GB each, <filename>md2</filename> and <filename>md3</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 600 GB of safe space."
msgstr "پارتیشن‌های <filename>sda5</filename> و <filename>sdc5</filename> همراه با <filename>sda6</filename> و <filename>sdc6</filename> هر کدام به یک آرایه RAID-1 به اندازه ۱۰۰ گیگابایت تقسیم شده‌اند که به نام‌های <filename>md1</filename> و <filename>md2</filename> موجود هستند. هر یک از این mirrorها به عنوان گروه‌های فیزیکی برای LVM راه‌اندازی شده‌اند که به گروه آرایه <filename>vg_raid</filename> اختصاص یافته‌اند. بنابراین این VG شامل ۲۰۰ گیگابایت فضای امن است."

#, fuzzy
#| msgid "The remaining partitions, <filename>sda7</filename> and <filename>sdc7</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 200 GB of space."
msgid "The remaining partitions, <filename>sda6</filename> and <filename>sdc6</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 460 GB of space."
msgstr "پارتیشن‌های باقیمانده <filename>sda7</filename> و <filename>sdc7</filename> به طور مستقیم به عنوان گروه‌های فیزیکی <filename>vg_bulk</filename> نامگذاری شده‌اند، که در نهایت فضایی معادل ۲۰۰ گیگابایت را شامل می‌شوند."

msgid "Once the VGs are created, they can be partitioned in a very flexible way. One must keep in mind that LVs created in <filename>vg_raid</filename> will be preserved even if one of the disks fails, which will not be the case for LVs created in <filename>vg_bulk</filename>; on the other hand, the latter will be allocated in parallel on both disks, which allows higher read or write speeds for large files."
msgstr "زمانی که VGها ایجاد شوند، به روشی بسیار منعطف می‌توانند پارتیشن‌بندی گردند. به یاد داشته باشید که LVهای ایجاد شده در <filename>vg_raid</filename> در صورت نقص دیسک‌ها نیز نگهداری می‌شوند که این مورد درباره LVهای ایجاد شده در <filename>vg_bulk</filename> صادق نیست؛ از طرف دیگر، مورد دوم به صورت موازی در اختیار هر دو دیسک قرار می‌گیرد، که امکان خواندن یا نوشتن فایل‌های بزرگ را فراهم می‌آورد."

#, fuzzy
#| msgid "We will therefore create the <filename>lv_usr</filename>, <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgid "We will therefore create the <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgstr "بنابراین گروه‌های منطقی <filename>lv_usr</filename>، <filename>lv_var</filename> و <filename>lv_home</filename> را در <filename>vg_raid</filename> ایجاد می‌کنیم تا از فایل سیستم‌های متناسب پشتیبانی گردد؛ از یک گروه منطقی دیگر بنام <filename>lv_movies</filename> برای ذخیره‌سازی ویدیوهای ویرایش شده استفاده می‌شود. از VG دیگر به منظور تقسیم <filename>lv_rushes</filename> برای داده‌های ورودی از دوربین‌های دیجیتال و یک <filename>lv_tmp</filename> برای فایل‌های موقت استفاده خواهد شد. مکان ذخیره‌سازی ناحیه کاری خود انتخاب دیگری است: با اینکه عملکرد خوب برای این آرایه مورد نیاز است، آیا ارزش دارد که کار را در قبال یک نقص سخت‌افزاری حین ویرایش ویدیو از دست بدهیم؟ با توجه به پاسخ این پرسش، LV مرتبط با یک VG یا دیگری ایجاد خواهد شد."

#, fuzzy
#| msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications. Should new software be installed later on (for editing audio clips, for instance), the LV hosting <filename>/usr/</filename> can be grown painlessly."
msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications."
msgstr "اکنون هم افزونگی داده برای داده‌های مهم فراهم شده هم انعطاف‌پذیری بهتری در مورد تقسیم فضای موجود بین برنامه‌ها وجود دارد. زمانی که نرم‌افزار مربوطه نصب شود (برای نمونه، ویرایش کلیپ‌های صوتی) LV که از <filename>/usr/</filename> میزبانی می‌کند می‌تواند به آرامی رشد کند."

msgid "<emphasis>NOTE</emphasis> Why three RAID-1 volumes?"
msgstr "<emphasis>یادداشت</emphasis> چرا از سه آرایه RAID-1 استفاده شد؟"

msgid "We could have set up one RAID-1 volume only, to serve as a physical volume for <filename>vg_raid</filename>. Why create three of them, then?"
msgstr "می‌توانستیم از یک آرایه RAID-1 به منظور گروه فیزیکی برای <filename>vg_raid</filename> استفاده می‌کردیم. پس چرا سه تا ایجاد کردیم؟"

msgid "The rationale for the first split (<filename>md0</filename> vs. the others) is about data safety: data written to both elements of a RAID-1 mirror are exactly the same, and it is therefore possible to bypass the RAID layer and mount one of the disks directly. In case of a kernel bug, for instance, or if the LVM metadata become corrupted, it is still possible to boot a minimal system to access critical data such as the layout of disks in the RAID and LVM volumes; the metadata can then be reconstructed and the files can be accessed again, so that the system can be brought back to its nominal state."
msgstr "منطق تقسیم اول (<filename>md0</filename> مقابل دیگران) درباره امنیت داده است: داده‌ای که روی هر دو عنصر یک mirror از RAID-1 نوشته می‌شود کاملا یکسان است و این امکان وجود دارد که با نادیده‌گرفتن ساختار RAID یکی از دیسک‌ها را به صورت مستقیم متصل کنیم. در صورتی که یک باگ کرنل موجود باشد، برای نمونه اگر اطلاعات جانبی LVM خراب شود، امکان راه‌اندازی حداقلی سیستم برای دسترسی به داده حیاتی مانند ساختار دیسک‌های موجود در آرایه‌های RAID و LVM موجود است؛ این اطلاعات جانبی می‌تواند بازسازی شده و فایل‌ها دوباره قابل دسترس شوند، تا سیستم به حالت عادی خود بازگردد."

#, fuzzy
#| msgid "The rationale for the second split (<filename>md1</filename> vs. <filename>md2</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md2</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md2</filename> and integrate its components <filename>sda6</filename> and <filename>sdc6</filename> into the bulk VG (which grows by 200 GB instead of 100 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgid "The rationale for the second split (<filename>md2</filename> vs. <filename>md3</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md3</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md3</filename> and integrate its components <filename>sda5</filename> and <filename>sdc5</filename> into the bulk VG (which grows by 600 GB instead of 300 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgstr "منطق تقسیم دوم (<filename>md1</filename> مقابل <filename>md2</filename>) از صراحت کمتری برخوردار است، که بیشتر درباره نامشخص بودن آینده دلالت دارد. زمانی که این سیستم جمع‌آوری شد، نیازمندی‌های دقیق ذخیره‌سازی از همان ابتدا مشخص نبود؛ چرا که می‌توانست در گذر زمان تغییر کند. در این مورد، از قبل نمی‌دانیم چه فضایی بابت ذخیره‌سازی ویدیوهای اولیه و کلیپ‌های ویرایش شده نهایی لازم است. اگر یک کلیپ مشخص نیازمند تصاویر اولیه بسیاری باشد و VG اختصاص‌یافته به داده تکراری کمتر از نصف فضای ذخیره‌سازی را داشته باشد، می‌توانیم از برخی فضای استفاده نشده آن بهره ببریم. می‌توانیم یکی از گروه‌های فیزیکی مانند <filename>md2</filename> را از <filename>vg_raid</filename> حذف کرده و به طور مستقیم به <filename>vg_bulk</filename> اختصاص دهیم (اگر مدت زمان انتظار رفته از عملیات به اندازه‌ای کم باشد که این کاهش عملکرد سیستم را تحمل کنیم) یا تنظیم RAID را روی <filename>md2</filename> لغو کرده و اجزای آن یعنی <filename>sda6</filename> و <filename>sdc6</filename> را درون VG دیگر قرار دهیم (که بجای ۱۰۰ می‌شود ۲۰۰ گیگابایت)؛ گروه منطقی <filename>lv_rushes</filename> سپس می‌تواند متناسب با نیازمندی‌های افزایش یابد."

msgid "<primary>virtualization</primary>"
msgstr "<primary>مجازی‌سازی</primary>"

msgid "Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security."
msgstr "مجازی‌سازی یکی از بزرگترین پیشرفت‌های علوم رایانه در سال‌های اخیر است. این عبارت شامل چندین مفهوم انتزاعی و تکنیکی است که در شبیه‌سازی رایانه‌های مجازی به طوری که مستقل از سخت‌افزار واقعی عمل می‌کنند. یک سرور فیزیکی می‌تواند شامل چندین سرور مجازی باشد که جدا از یکدیگر فعالیت می‌کنند. برنامه‌های کاربردی آن بسیار هستند که از این انزوا مشتق می‌شوند: برای نمونه، محیط‌های آزمایشی همراه با پیکربندی‌های متفاوت یا جداسازی سرویس‌های میزبانی بین چندین ماشین مجازی برای امنیت."

msgid "There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:"
msgstr "راهکارهای گوناگون مجازی‌سازی هر کدام با نقاط قوت و ضعف خود وجود دارند. تمرکز این کتاب روی Xen، LXC و KVM است اما سایر پیاده‌سازی‌های قابل ذکر عبارتند از:"

#, fuzzy
#| msgid "<primary>LXC</primary>"
msgid "<primary>Xen</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>VMWare</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>Nagios</primary>"
msgid "<primary>Bochs</primary>"
msgstr "<primary>Nagios</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>QEMU</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary>VirtualBox</primary>"
msgstr "<primary>مجازی‌سازی</primary>"

msgid "<primary>KVM</primary>"
msgstr "<primary>KVM</primary>"

msgid "<primary>LXC</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"
msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"https://qemu.org/\" />"
msgstr "QEMU یک شبیه‌ساز نرم‌افزاری برای یک رایانه کامل است؛ عملکرد آن چیزی بیشتر از سرعت است که می‌توان به آن دست یافت، اما این فرآیند امکان اجرای سیستم عامل‌های تغییرنیافته یا آزمایشی را روی سخت‌افزار شبیه‌سازی شده فراهم می‌کند. همچنین امکان شبیه‌سازی چندین معماری گوناگون سخت‌افزاری را نیز فراهم می‌کند: برای نمونه، یک سیستم <emphasis>amd64</emphasis> می‌تواند یک رایانه <emphasis>arm</emphasis> را شبیه‌سازی کند. QEMU نرم‌افزار آزاد است. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"

msgid "Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64)."
msgstr "... نیز یک ماشین مجازی آزاد دیگر است، اما تنها به شبیه‌سازی معماری‌های x86 می‌پردازد (i386 و amd64)."

#, fuzzy
#| msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"
msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as “snapshotting“ a running virtual machine. <ulink type=\"block\" url=\"https://vmware.com/\" />"
msgstr "VMWare یک ماشین مجازی انحصاری است؛ به عنوان یکی از قدیمی‌ترین گزینه‌های موجود، یکی از شناخته‌شده‌ترین راهکارهای مجازی‌سازی است. بر اساس مبانی مشترک با QEMU کار می‌کند. VMWare قابلیت‌های پیشرفته‌ای از جمله snapshot گرفتن از یک ماشین مجازی در حال اجرا را فراهم می‌کند. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"

#, fuzzy
#| msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler. While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"
msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <ulink url=\"https://bugs.debian.org/794466\">#794466</ulink>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some “snapshotting“ and other interesting features. <ulink type=\"block\" url=\"https://www.virtualbox.org/\" />"
msgstr "VirtualBox یک ماشین مجازی است که تقریبا از نرم‌افزار آزاد تشکیل شده است (برخی اجزای اضافی آن شامل مجوزهای انحصاری می‌باشند). متاسفانه در قسمت “contrib” از دبیان قرار دارد زیرا شامل فایل‌های کامپایل شده‌ای است که بدون استفاده از یک کامپایلر انحصاری قابل بازسازی نیستند. با اینکه از VMWare جوان‌تر و محدود به معماری‌های i386 و amd64 است، شامل ویژگی‌های جالبی از جمله snapshot گرفتن و سایر قابلیت‌های هیجان‌انگیز می‌باشد. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"

#, fuzzy
#| msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgid "<emphasis>HARDWARE</emphasis> Virtualization support"
msgstr "<emphasis>مطالعه بیشتر</emphasis> مجازی‌سازی انبوه"

msgid "Some computers might not have hardware virtualization support; when they do, it should be enabled in the BIOS."
msgstr ""

msgid "To know if you have virtualization support enabled, you can check if the relevant flag is enabled with <command>grep</command>. If the following command for your processor returns some text, you already have virtualization support enabled:"
msgstr ""

msgid "For Intel processors you can execute <command>grep vmx /proc/cpuinfo</command> to check for Intel's Virtual Machine Extensions."
msgstr ""

msgid "For AMD processors you can execute <command>grep svm /proc/cpuinfo</command> to check for AMD's Secure Virtual Machine."
msgstr ""

msgid "If that is not the case, you can access the BIOS of your system and check for entries like “Intel Virtualization Technology”/“Intel VT-x” or “SVM mode” (AMD) - usually to be found in the CPU configuration in the Advanced section."
msgstr ""

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary>paravirtualization</primary>"
msgstr "<primary>مجازی‌سازی</primary>"

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary>hypervisor</primary>"
msgstr "<primary>libvirt</primary>"

msgid "Xen <indexterm><primary>Xen</primary></indexterm> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen."
msgstr "Xen <indexterm><primary>Xen</primary></indexterm> یک راهکار “paravirtualization” است که شامل یک لایه انتزاعی سبک به نام “hypervisor” می‌باشد که بین سخت‌افزار و سیستم‌های بالایی قرار می‌گیرد؛ این لایه مانند یک داور دسترسی از ماشین‌های مجازی به سخت‌افزار را کنترل می‌کند. اگرچه، تنها شامل چند دستورالعمل کوتاه است، باقی عملیات به طور مستقیم از طرف سخت‌افزار و به نیابت از سیستم‌های دیگر انجام می‌شوند. مزیت اصلی آن این است که عملکرد کاهش نمی‌یابد و سیستم‌های تقریبا با سرعت اصلی سخت‌افزار اجرا می‌شوند؛ اشکال اصلی آن این است که کرنل‌های سیستم عامل‌ها به منظور استفاده از hypervisor باید سازگار با Xen باشند."

#, fuzzy
#| msgid "<primary>deployment</primary>"
msgid "<primary>dom0</primary>"
msgstr "<primary>گسترش</primary>"

#, fuzzy
#| msgid "<primary>deployment</primary>"
msgid "<primary>domU</primary>"
msgstr "<primary>گسترش</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>virtualization</primary><secondary>host</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>virtualization</primary><secondary>guest</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgid "Let's spend some time on terms. The hypervisor is the lowest layer, which runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgstr "بیایید چند عبارت را بررسی کنیم. hypervisor پایین‌ترین لایه است که به صورت مستقیم روی سخت‌افزار اجرا می‌شود، حتی پایین‌تر از کرنل. این hypervisor می‌تواند سایر نرم‌افزارها را درون چندین <emphasis>دامنه</emphasis> قرار دهد، که می‌توانند به عنوان ماشین‌های مجازی دیده شوند. یکی از این دامنه‌ها (اولین آن‌ها که آغاز می‌شود) به عنوان <emphasis>dom0</emphasis> شناخته می‌شود و نقش ویژه‌ای دارد، چرا که تنها این دامنه می‌تواند hypervisor و اجرای سایر دامنه‌ها را کنترل کند. سایر دامنه‌ها به عنوان <emphasis>domU</emphasis> شناخته می‌شوند. به عبارت دیگر، و از دید کاربر، <emphasis>dom0</emphasis> به عنوان “میزبان” برای سایر سیستم‌های مجازی عمل می‌کند در صورتی که <emphasis>domU</emphasis> به عنوان یک “میهمان” دیده می‌شود."

msgid "<emphasis>CULTURE</emphasis> Xen and the various versions of Linux"
msgstr "<emphasis>فرهنگ</emphasis> Xen و نسخه‌های مختلف از لینوکس"

msgid "Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <emphasis>paravirt_ops</emphasis> or <emphasis>pv_ops</emphasis> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially."
msgstr "Xen در ابتدا به عنوان چندین اصلاحیه خارج از ساختار اصلی کرنل لینوکس توسعه یافت. در همان زمان، چندین راهکاری مجازی‌سازی دیگر (از جمله KVM) نیازمند چندین عملکرد عمومی مجازی‌سازی بودند تا یکپارچه‌سازی خود را تسهیل نمایند و کرنل لینوکس این مجموعه قابلیت‌ها را گردآوری کرد (که به عنوان رابط <emphasis>paravirt_ops</emphasis> یا <emphasis>pv_ops</emphasis> شناخته می‌شوند). از آنجا که اصلاحیه‌های Xen این قابلیت‌ها را به شیوه‌ای دیگر پیاده‌سازی می‌کردند، نتوانستند به صورت رسمی پذیرفته شوند."

#, fuzzy
#| msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgstr "Xensource، شرکتی که پشت Xen است، مجبور شد تا Xen را به یک چارچوب جدید انتقال دهد به صورتی که اصلاحیه‌های Xen بتوانند درون کرنل رسمی لینوکس ادغام شوند. این کار به معنی بازنویسی قسمت اعظمی از کد بود و با اینکه Xensource به نسخه کارآمدی مبتنی بر رابط paravirt_ops رسیده بود، اصلاحیه‌ها با سرعت کمی درون کرنل رسمی قرار می‌گرفتند. این ادغام در لینوکس ۳.۰ کامل شد. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"

#, fuzzy
#| msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgstr "از آنجا که <emphasis role=\"distribution\">Jessie</emphasis> مبتنی بر نسخه ۳.۱۶ از کرنل لینوکس است، بسته‌های استاندارد <emphasis role=\"pkg\">linux-image-686-pae</emphasis> و <emphasis role=\"pkg\">linux-image-amd64</emphasis> شامل کدهای لازم می‌باشند و اصلاحیه‌های مختص به توزیع‌های <emphasis role=\"distribution\">Squeeze</emphasis> و قبل از آن دیگر مورد نیاز نیستند. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"

msgid "<emphasis>CULTURE</emphasis> Xen and non-Linux kernels"
msgstr "<emphasis>فرهنگ</emphasis> Xen کرنل‌های غیر از لینوکس"

#, fuzzy
#| msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"https://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgstr "Xen نیازمند ایجاد تغییرات در تمام سیستم عامل‌هایی است که قصد اجرای روی آن را دارند؛ تمام کرنل‌های به این سطح از بلوغ نرسیده‌اند. بسیاری از آن‌ها کاملا کارآمد هستند، به عنوان dom0 و domU: لینوکس ۳.۰ به بعد، NetBSD ۴.۰ به بعد و OpenSolaris. سایر کرنل‌ها تنها به عنوان domU می‌توانند کار کنند. وضعیت هر کدام از سیستم عامل‌‌ها را می‌توانید در صفحه ویکی Xen مشاهده کنید: <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"

msgid "However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows)."
msgstr "با این حال، اگر Xen بتواند روی قابلیت‌های سخت‌افزاری موجود برای مجازی‌سازی تکیه کند (که تنها در پردازنده‌های جدید مشاهده می‌شوند)، حتی سیستم عامل‌های غیر-قابل تغییر مانند ویندوز نیز می‌توانند به عنوان domU استفاده گردند."

msgid "<emphasis>NOTE</emphasis> Architectures compatible with Xen"
msgstr "<emphasis>یادداشت</emphasis> معماری‌های سازگار با Xen"

msgid "Xen is currently only available for the i386, amd64, arm64 and armhf architectures."
msgstr "Xen هم اکنون تنها برای معماری‌های i386، amd64، arm64 و armhf موجود می‌باشد."

msgid "Using Xen under Debian requires three components:"
msgstr "استفاده از Xen تحت دبیان نیازمند سه مولفه است:"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-hypervisor</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>."
msgid "The hypervisor itself. According to the available hardware, the appropriate package providing <emphasis role=\"pkg\">xen-hypervisor</emphasis> will be either <emphasis role=\"pkg\">xen-hypervisor-4.14-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.14-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.14-arm64</emphasis>."
msgstr "خود hypervisor. با توجه به سخت‌افزار موجود، بسته مناسب آن یکی از گزینه‌های <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>، <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis> یا <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis> خواهد بود."

#, fuzzy
#| msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <emphasis role=\"distribution\">Jessie</emphasis>."
msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 5.10 version present in <emphasis role=\"distribution\">Bullseye</emphasis>."
msgstr "کرنلی که روی آن hypervisor اجرا می‌شود. هر کرنل جدیدتر از ۳.۰ اینکار را انجام می‌دهد، از جمله ۳.۱۶ موجود در <emphasis role=\"distribution\">Jessie</emphasis>."

msgid "The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <emphasis role=\"pkg\">libc6-xen</emphasis> package."
msgstr "معماری i386 همچنین نیازمند یک کتابخانه استاندارد است که بتواند از اصلاحیه‌های موجود در Xen بهره‌مند شود؛ این کتابخانه در بسته <emphasis role=\"pkg\">libc6-xen</emphasis> موجود است."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-utils</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <emphasis role=\"pkg\">xen-linux-system-amd64</emphasis>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.4</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:"
msgid "The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.14</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the GRUB bootloader menu, so as to start the chosen kernel in a Xen dom0. Note, however, that this entry is not usually set to be the first one in the list, but it will be selected by default."
msgstr "به منظور اینکه انتخاب این مولفه‌ها بدون دردسر انجام شود، چندین بسته کاربردی (از جمله <emphasis role=\"pkg\">xen-linux-system-amd64</emphasis>) ایجاد شده‌اند؛ این بسته‌ها با ترکیب خوبی از hypervisor مناسب و بسته‌های کرنل آن قرار گرفته‌اند. hypervisor همچنین شامل بسته <emphasis role=\"pkg\">xen-utils-4.4</emphasis> است که ابزار لازم برای کنترل آن از طریق dom0 را فراهم می‌آورد. این عمل در حقیقت کتابخانه استاندارد را نصب می‌کند. طی نصب این بسته‌ها، اسکریپت‌های پیکربندی همچنین یک مدخل درون منوی راه‌اندازی Grub ایجاد می‌کنند تا کرنل انتخابی برای آغاز dom0 مشخص گردد. به یاد داشته باشید که این مدخل معمولا به عنوان گزینه اول در فهرست قرار نمی‌گیرد، به همین منظور به صورت پیشفرض انتخاب نمی‌گردد. اگر این عملکرد مطلوب شما نباشد، دستورات زیر می‌توانند آن را تغییر دهند:"

msgid "Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps."
msgstr "زمانی که این پیشنیازها نصب شوند، گام بعدی آزمایش عملکرد خود dom0 است؛ این عمل شامل راه‌اندازی مجدد hypervisor و کرنل Xen می‌باشد. سیستم باید به شیوه استاندارد خود راه‌اندازی شده، همراه با چندین پیام که طی گام‌های اولیه راه‌اندزای در کنسول نمایش می‌یابند."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">xen-tools</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>xen-create-image</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/xen-tools/xen-tools.conf</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Now is the time to actually install useful systems on the domU systems, using the tools from <emphasis role=\"pkg\">xen-tools</emphasis>. This package provides the <command>xen-create-image</command> command, which largely automates the task. The only mandatory parameter is <literal>--hostname</literal>, giving a name to the domU; other options are important, but they can be stored in the <filename>/etc/xen-tools/xen-tools.conf</filename> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <command>xen-create-image</command> invocation. Important parameters of note include the following:"
msgstr "اکنون زمان آن فرا رسیده است که با استفاده از ابزار موجود در <emphasis role=\"pkg\">xen-tools</emphasis> به نصب سیستم‌های کاربردی روی domU بپردازیم. این بسته شامل دستور <command>xen-create-image</command> می‌باشد که تقریبا این فرآیند را خودکارسازی می‌کند. تنها پارامتر ضروری آن <literal>--hostname</literal> است که نام domU را مشخص می‌کند؛ سایر گزینه‌ها نیز مهم هستند ولی می‌توانند درون فایل پیکربندی <filename>/etc/xen-tools/xen-tools.conf</filename> قرار بگیرند که نبود آن‌ها خطایی را در هنگام اجرای دستور صادر نمی‌کند. بنابراین مهم است که محتوای این فایل را قبل از ایجاد هر image بررسی کرده یا از پارامترهای اضافی هنگام فراخوانی <command>xen-create-image</command> استفاده کنیم. پارامترهای مهم قابل ذکر عبارتند از:"

msgid "<literal>--memory</literal>, to specify the amount of RAM dedicated to the newly created system;"
msgstr "<literal>--memory</literal>، برای مشخص کردن میزان RAM اختصاص یافته به سیستم جدید؛"

msgid "<literal>--size</literal> and <literal>--swap</literal>, to define the size of the “virtual disks” available to the domU;"
msgstr "<literal>--size</literal> و <literal>--swap</literal>، برای تعریف اندازه \"دیسک‌های مجازی\" موجود برای domU؛"

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary><command>debootstrap</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

#, fuzzy
#| msgid "<literal>--debootstrap</literal>, to cause the new system to be installed with <command>debootstrap</command>; in that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">jessie</emphasis>)."
msgid "<literal>--debootstrap-cmd</literal>, to specify the which debootstrap command is used. The default is <command>debootstrap</command> if debootstrap and cdebootstrap are installed. In that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">bullseye</emphasis>)."
msgstr "<literal>--debootstrap</literal>، برای نصب شدن سیستم جدید با استفاده از <command>debootstrap</command>؛ در این مورد، از گزینه <literal>--dist</literal> اغلب استفاده می‌شود (همراه با نام یک توزیع مانند <emphasis role=\"distribution\">jessie</emphasis>)."

msgid "<emphasis>GOING FURTHER</emphasis> Installing a non-Debian system in a domU"
msgstr "<emphasis>مطالعه بیشتر</emphasis> نصب یک سیستم غیر دبیان در یک domU"

msgid "In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <literal>--kernel</literal> option."
msgstr "در مورد یک سیستم غیر-لینوکس، برای تعریف کرنل مورد استفاده domU باید دقت کرد، با استفاده از گزینه <literal>--kernel</literal>."

msgid "<literal>--dhcp</literal> states that the domU's network configuration should be obtained by DHCP while <literal>--ip</literal> allows defining a static IP address."
msgstr "<literal>--dhcp</literal> بیان می‌کند که پیکربندی شبکه domU باید از طریق DHCP انجام شود در صورتی که <literal>--ip</literal> امکان استفاده از نشانی ایستای IP را فراهم می‌کند."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LVM</primary><secondary>Xen</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <literal>--dir</literal> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <literal>--lvm</literal> option, followed by the name of a volume group; <command>xen-create-image</command> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive."
msgstr "در انتها، از یک روش ذخیره‌سازی به منظور ایجاد image استفاده کرد (آن‌هایی که به عنوان درایوهای هارد دیسک از domU در نظر گرفته می‌شوند). ساده‌ترین روش، با توجه به گزینه <literal>--dir</literal>، ایجاد یک فایل در dom0 به ازای هر دستگاه domU فراهم‌کننده آن است. برای سیستم‌هایی که از LVM استفاده می‌کنند، گزینه جایگزین استفاده از <literal>--lvm</literal> است، که همراه با نام یک گروه آرایه آورده می‌شود؛ سپس <command>xen-create-image</command> اقدام به ایجاد یک گروه منطقی درون آرایه‌ها می‌کند که این گروه منطقی به عنوان یک درایو هارد دیسک به domU معرفی می‌گردد."

msgid "<emphasis>NOTE</emphasis> Storage in the domU"
msgstr "<emphasis>یادداشت</emphasis> ذخیره‌سازی در domU"

msgid "Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <command>xen-create-image</command>, however, so editing the Xen image's configuration file is in order after its initial creation with <command>xen-create-image</command>."
msgstr "علاوه بر پارتیشن‌ها، آرایه‌های RAID و گروه‌های منطقی موجود در LVM، هارد دیسک‌های کامل نیز می‌توانند به domU انتقال یابند. این عملیات توسط <command>xen-create-image</command> به صورت خودکار انجام نمی‌شوند، با این حال، ویرایش فایل پیکربندی Xen معمولا پس از فراخوانی اولیه <command>xen-create-image</command> صورت می‌گیرد."

msgid "Once these choices are made, we can create the image for our future Xen domU:"
msgstr "زمانی که این انتخاب‌ّها صورت گیرد، می‌توانیم image خود را برای domU بعدی در Xen ایجاد کنیم:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
#| "<computeroutput>\n"
#| "[...]\n"
#| "General Information\n"
#| "--------------------\n"
#| "Hostname       :  testxen\n"
#| "Distribution   :  jessie\n"
#| "Mirror         :  http://ftp.debian.org/debian/\n"
#| "Partitions     :  swap            128Mb (swap)\n"
#| "                  /               2G    (ext3)\n"
#| "Image type     :  sparse\n"
#| "Memory size    :  128Mb\n"
#| "Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
#| "Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
#| "[...]\n"
#| "Logfile produced at:\n"
#| "         /var/log/xen-tools/testxen.log\n"
#| "\n"
#| "Installation Summary\n"
#| "---------------------\n"
#| "Hostname        :  testxen\n"
#| "Distribution    :  jessie\n"
#| "MAC Address     :  00:16:3E:8E:67:5C\n"
#| "IP-Address(es)  :  dynamic\n"
#| "RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
#| "Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=bullseye --role=udev\n"
"</userinput><computeroutput>\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  bullseye\n"
"Mirror         :  http://deb.debian.org/debian\n"
"Partitions     :  swap            512M  (swap)\n"
"                  /               2G    (ext4)\n"
"Image type     :  sparse\n"
"Memory size    :  256M\n"
"Bootloader     :  pygrub\n"
"\n"
"[...]\n"
"Logfile produced at:\n"
"\t /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  bullseye\n"
"MAC Address     :  00:16:3E:C2:07:EE\n"
"IP Address(es)  :  dynamic\n"
"SSH Fingerprint :  SHA256:K+0QjpGzZOacLZ3jX4gBwp0mCESt5ceN5HCJZSKWS1A (DSA)\n"
"SSH Fingerprint :  SHA256:9PnovvGRuTw6dUcEVzzPKTITO0+3Ki1Gs7wu4ke+4co (ECDSA)\n"
"SSH Fingerprint :  SHA256:X5z84raKBajUkWBQA6MVuanV1OcV2YIeD0NoCLLo90k (ED25519)\n"
"SSH Fingerprint :  SHA256:VXu6l4tsrCoRsXOqAwvgt57sMRj2qArEbOzHeydvV34 (RSA)\n"
"Root Password   :  FS7CUxsY3xkusv7EkbT9yae\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  jessie\n"
"Mirror         :  http://ftp.debian.org/debian/\n"
"Partitions     :  swap            128Mb (swap)\n"
"                  /               2G    (ext3)\n"
"Image type     :  sparse\n"
"Memory size    :  128Mb\n"
"Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
"Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  jessie\n"
"MAC Address     :  00:16:3E:8E:67:5C\n"
"IP-Address(es)  :  dynamic\n"
"RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
"Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
"</computeroutput>"

msgid "We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters."
msgstr "اکنون دارای یک ماشین مجازی هستیم که اجرا نمی‌شود (و تنها از فضای موجود در هارد دیسک dom0 استفاده می‌کند). البته که می‌توانیم با استفاده از پارامترهای گوناگون دیگر image بیشتری بسازیم."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>network models</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgstr "قبل از اینکه این ماشین‌های مجازی را روشن کنیم باید راجع به چگونگی دسترسی به آن‌ها تصمیم بگیریم. آن‌ها می‌توانند به عنوان ماشین‌های ایزوله شده تنها از طریق کنسول سیستم خود تعریف شوند، اما این روش به ندرت از الگوی کارکرد تبعیت می‌کند. اکثر مواقع، یک domU به عنوان یک سرور راه دور در نظر گرفته می‌شود که تنها از طریق یک شبکه قابل دسترسی است. با این حال، اختصاص یک کارت شبکه به هر domU ممکن است مناسب نباشد؛ به همین دلیل است که Xen امکان ایجاد رابط‌های مجازی که هر دامنه قادر به مشاهده و استفاده استاندارد از آن‌ها باشد را می‌دهد. به یاد داشته باشید که این کارت‌ها، با وجود مجازی بودن، تنها زمانی مفید هستند که به یک شبکه متصل گردند. Xen دارای چندین مدل شبکه برای این منظور است:"

msgid "The simplest model is the <emphasis>bridge</emphasis> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch."
msgstr "ساده‌ترین مدل <emphasis>bridge</emphasis> است؛ تمام کارت‌های شبکه eth0 (در سیستم‌های dom0 و domU) طوری عمل می‌کنند گویی به سوئیچ اصلی Ethernet متصل شده‌اند."

msgid "Then comes the <emphasis>routing</emphasis> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network."
msgstr "سپس مدل <emphasis>routing</emphasis> قرار دارد، به صورتی که dom0 به عنوان یک مسیریاب میان سیستم‌های domU و شبکه خارجی (فیزیکی) قرار می‌گیرد."

msgid "Finally, in the <emphasis>NAT</emphasis> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0."
msgstr "در نهایت، در مدل <emphasis>NAT</emphasis>، dom0 بین سیستم‌های domU و باقی شبکه قرار می‌گیرد، اما سیستم‌های domU به صورت مستقیم از خارج قابل دسترس نیستند و ترافیک از طریق ترجمه نشانی شبکه یا NAT با استفاده از dom0 انتقال می‌یابد."

msgid "These three networking nodes involve a number of interfaces with unusual names, such as <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> and <filename>xenbr0</filename>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model."
msgstr "این سه مدل شبکه دارای تعدادی رابط با نام‌های نامتعارف هستند از جمله <filename>vif*</filename>، <filename>veth*</filename>، <filename>peth*</filename> و <filename>xenbr0</filename>. hypervisor موجود در Xen با توجه به لایه تعریف شده آن‌ها را مرتب‌سازی می‌کند که این کار با استفاده از ابزارهای سمت-کاربر صورت می‌گیرد. از آنجا که مدل‌های NAT و routing تنها برای موارد خاص کاربرد دارند، تنها به بررسی مدل bridge می‌پردازیم."

#, fuzzy
#| msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.4</emphasis> package recommends it) to replace the existing eth0 entry:"
msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils</emphasis> package recommends it) to replace the existing <replaceable>eth0</replaceable> entry (be careful to use the correct network device name):"
msgstr "پیکربندی استاندارد بسته‌های Xen تغییری در پیکربندی شبکه در کل سیستم ایجاد نمی‌کند. با این حال، فرآیند پس‌زمینه <command>xend</command> طوری پیکربندی شده است تا رابط‌های مجازی شبکه را با هر شبکه bridge از پیش موجود یکپارچه سازد (در صورت وجود چندین bridge گزینه <filename>xenbr0</filename> اولویت می‌یابد). برای اینکار نیازمند برپایی یک bridge در <filename>/etc/network/interfaces</filename> هستیم تا مدخل موجود eth0 جایگزین گردد (که نیازمند نصب بسته <emphasis role=\"pkg\">bridge-utils</emphasis> می‌باشد، به همین دلیل است که بسته <emphasis role=\"pkg\">xen-utils-4.4</emphasis> توصیه می‌شود):"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/network/interfaces</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">bridge-utils</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary><literal>xenbr0</literal></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid ""
#| "auto xenbr0\n"
#| "iface xenbr0 inet dhcp\n"
#| "    bridge_ports eth0\n"
#| "    bridge_maxwait 0\n"
#| "    "
msgid ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports <replaceable>eth0</replaceable>\n"
"    bridge_maxwait 0"
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>xl</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them."
msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them. You might need to increase the default memory by editing the variable memory from configuration file (in this case, <filename>/etc/xen/testxen.cfg</filename>). Here we have set it to 1024 (megabytes)."
msgstr "پس از راه‌اندازی مجدد و اطمینان از اینکه bridge به طور خودکار ایجاد شده است، اکنون می‌توانیم domU را با استفاده از ابزار کنترلی Xen، به خصوص دستور <command>xl</command>، آغاز کنیم. این دستور امکان چندین تغییر روی دامنه‌ها را همراه با فهرست‌سازی آن‌ها و آغاز/پایان هر کدام فراهم می‌آورد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/xen/testxen.cfg</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   463     1     r-----      9.8\n"
#| "# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
#| "<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
#| "# </computeroutput><userinput>xl list</userinput>\n"
#| "<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
#| "Domain-0                                     0   366     1     r-----     11.4\n"
#| "testxen                                      1   128     1     -b----      1.1</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl list\n"
"</userinput><computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  3918     2     r-----      35.1\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg\n"
"</userinput><computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list\n"
"</userinput><computeroutput>Name                                        ID   Mem VCPUs\tState\tTime(s)\n"
"Domain-0                                     0  2757     2     r-----      45.2\n"
"testxen                                      3  1024     1     r-----       1.3\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   463     1     r-----      9.8\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   366     1     r-----     11.4\n"
"testxen                                      1   128     1     -b----      1.1</computeroutput>"

msgid "<emphasis>TOOL</emphasis> Choice of toolstacks to manage Xen VM"
msgstr "<emphasis>ابزار</emphasis> انتخاب جعبه ابزار برای مدیریت ماشین مجازی Xen"

msgid "<primary><command>xm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "<primary><command>xe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

msgid "<primary><command>virsh</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">libvirt</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of libvirt and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of <emphasis role=\"pkg\">libvirt</emphasis> and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgstr "در دبیان ۷ و نسخه‌های قبل از آن، <command>xm</command> ابزار خط فرمان مرجع برای مدیریت ماشین‌های مجازی Xen بود. اکنون با <command>xl</command> جایگزین شده است که در اکثر موارد با آن سازگاری دارد. اما این گزینه‌ها تنها ابزار موجود برای اینکار نیستند: <command>virsh</command> از libvirt و <command>xe</command> از XAPI موجود در XenServer (بسته تجاری Xen) ابزارهای جایگزین هستند."

msgid "<emphasis>CAUTION</emphasis> Only one domU per image!"
msgstr "<emphasis>احتیاط</emphasis> تنها یک domU برای هر image!"

#, fuzzy
#| msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is, however, quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgstr "از آنجا که امکان استفاده از چندین سیستم domU مختلف به صورت موازی وجود دارد، تمام آن‌ها نیازمند استفاده از image مخصوص به خود هستند، چرا که برای هر domU انتظار می‌رود که روی سخت‌افزار مختص به خود اجرا گردد (جدا از تکه بسیار کوچک کرنل که با hypervisor تعامل می‌کند). به طور مشخص، امکان استفاده همزمان از دو سیستم domU که از یک فضای ذخیره‌سازی اشتراکی بهره می‌برند وجود ندارد. اگر سیستم‌های domU در یک زمان واحد اجرا نشوند، تقریبا امکان استفاده از یک پارتیشن swap یا پارتیشنی که از فایل سیستم <filename>/home</filename> میزبانی می‌کند وجود دارد."

msgid "Note that the <filename>testxen</filename> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly."
msgstr "به یاد داشته باشید که domU ایجاد شده بنام ... از حافظه واقعی گرفته شده از RAM که معمولا برای dom0 در نظر گرفته می‌شود، استفاده می‌کند و نه یک حافظه شبیه‌سازی شده. بنابراین هنگام راه‌اندازی یک سرور مبتی بر Xen باید دقت عمل در تخصیص حافظه به خرج داد."

msgid "Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <filename>hvc0</filename> console, with the <command>xl console</command> command:"
msgstr "بسیار خوب! ماشین مجازی ما راه‌اندازی شد. دو روش دسترسی به آن وجود دارد. روش معمول اتصال ... به آن است، مانند یک ماشین حقیقی که از طریق شبکه متصل می‌شویم؛ اینکار نیازمند برپایی یک سرور DHCP یا پیکربندی DNS جداگانه است. روش دیگر، که در صورت اشتباه بودن پیکربندی شبکه می‌تواند تنها روش ممکن باشد، استفاده از کنسول <filename>hvc0</filename> همراه با دستور <command>xl console</command> است:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary><literal>hvc0</literal></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
#| "<computeroutput>[...]\n"
#| "\n"
#| "Debian GNU/Linux 8 testxen hvc0\n"
#| "\n"
#| "testxen login: </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 11 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 8 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"

msgid "One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> key combination."
msgstr "در این حالت می‌توان یک نشست جداگانه برای دسترسی به ماشین مجازی ایجاد کرد. قطع اتصال این کنسول با استفاده از کلید ترکیبی <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> انجام می‌شود."

msgid "<emphasis>TIP</emphasis> Getting the console straight away"
msgstr "<emphasis>نکته</emphasis> دسترسی مستقیم به کنسول"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>console</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Sometimes one wishes to start a domU system and get to its console straight away; this is why the <command>xl create</command> command takes a <literal>-c</literal> switch. Starting a domU with this switch will display all the messages as the system boots."
msgstr "بعضی وقت‌ها شاید بخواهیم پس از راه‌اندازی سیستم domU مستقیم وارد کنسول آن شویم؛ به همین دلیل است که دستور <command>xl create</command> یک گزینه <literal>-c</literal> را نیز می‌پذیرد. آغاز یک domU همراه با این سوئیچ تمام پیام‌های موجود هنگام راه‌اندازی آن را نمایش می‌دهد."

#, fuzzy
#| msgid "<emphasis>TOOL</emphasis> OpenXenManager"
msgid "<emphasis>TOOL</emphasis> Graphical Xen managers"
msgstr "<emphasis>ابزار</emphasis> OpenXenManager"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>manager</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <command>xl</command> command."
msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package), a graphical interface allowing remote management of Xen domains via Xen's API, is no longer provided by Debian due to the lack of upstream development. If you are looking for a replacement, <emphasis role=\"pkg\">virt-manager</emphasis> provides support to handle Xen VMs as well."
msgstr "OpenXenManager (در بسته <emphasis role=\"pkg\">openxenmanager</emphasis>) یک ابزار گرافیکی است که امکان مدیریت دامنه‌های Xen را با استفاده از API آن فراهم می‌کند. همچنین امکان کنترل دامنه‌های Xen از راه‌دور نیز وجود دارد. این بسته تقریبا تمام ویژگی‌های دستور <command>xl</command> را فراهم می‌کند."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">openxenmanager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

msgid "Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <command>xl pause</command> and <command>xl unpause</command> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <command>xl save</command> and <command>xl restore</command> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system."
msgstr "زمانی که domU آغاز گردد، می‌تواند مانند هر سرور دیگری مورد استفاده قرار گیرد (چرا که یک سیستم گنو/لینوکس است). اگرچه، وضعیت ماشین مجازی آن برخی قابلیت‌های دیگر را فراهم می‌کند. برای نمونه، یک domU با استفاده از دستورات <command>xl pause</command> و <command>xl unpause</command> می‌تواند به صورت موقت متوقف شده یا ادامه یابد. به یاد داشته باشید که یک domU متوقف شده با اینکه از قدرت پردازنده استفاده نمی‌کند، اما هم اکنون حافظه اختصاص یافته به خود را دارد. استفاده از دستورات <command>xl save</command> و <command>xl restore</command> نیز قابل توجه است: ذخیره‌سازی یک domU تمام منابع اختصاص یافته به آن، از جمله RAM، را آزادسازی می‌کند. در زمان بازگرداندن (یا ادامه، به این منظور) یک domU چیزی به جز گذشت زمان را احساس نمی‌کند. اگر هنگام خاموش کردن dom0 یک domU در حال اجرا باشد، اسکریپت‌های پیکربندی به صورت خودکار domU را ذخیره‌سازی کرده تا در راه‌اندازی بعدی از سر گرفته شود. این عمل البته ناملایمت‌های عملیات hibernate کردن یک رایانه لپ‌تاپ را به همراه دارد، برای نمونه؛ به طور مشخص اگر domU به مدت زمان طولانی در حالت تعلیق باشد، ارتباطات شبکه ممکن است منقضی گردند. به یاد داشته باشید که Xen به شدت ناسازگار با بخش مدیریت قدرت ACPI است، که عملیات متوقف‌سازی سیستم میزبان (dom0) را انجام می‌دهد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>ACPI</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>ACPI</primary>"
msgstr "<primary>RAID</primary>"

msgid "Halting or rebooting a domU can be done either from within the domU (with the <command>shutdown</command> command) or from the dom0, with <command>xl shutdown</command> or <command>xl reboot</command>."
msgstr "متوقف‌سازی یا راه‌اندازی مجدد یک domU می‌تواند از طریق خود آن (با دستور <command>shutdown</command>) یا از طریق dom0 با استفاده از <command>xl shutdown</command> یا <command>xl reboot</command> انجام شود."

msgid "Most of the <command>xl</command> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> manual page."
msgstr "اغلب زیردستورات <command>xl</command> شامل یک یا چند آرگومان هستند، که بیشتر شامل نام یک domU می‌شود. این آرگومان‌ها به خوبی در صفحه راهنمای <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> توضیح داده شده‌اند."

msgid "<emphasis>GOING FURTHER</emphasis> Advanced Xen"
msgstr "<emphasis>مطالعه بیشتر</emphasis> Xen پیشرفته"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Xen</primary><secondary>documentation</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"
msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"https://xenproject.org/help/documentation/\" />"
msgstr "Xen دارای قابلیت‌های بسیاری است که نمی‌توان در چند پاراگراف به آن‌ها اشاره کرد. به طور مشخص، سیستم بسیار پویا است و امکان تنظیم چندین پارامتر برای یک دامنه هنگام اجرای آن وجود دارد (از جمله میزان حافظه اختصاص یافته، هارد درایوهای قابل مشاهده، رفتار زمان‌بند وظایف و از این قبیل). یک domU حتی می‌تواند بین چندین سرور انتقال یابد به گونه‌‌ای که نه خاموش گردد و نه ارتباط شبکه را از دست دهد! برای اطلاع از تمام این جنبه‌های پیشرفته، منبع اولیه اطلاعات، مستندات رسمی Xen است. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>Linux Containers</primary><see>LXC</see>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>kernel</primary><secondary>control groups</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <emphasis>control groups</emphasis>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system."
msgstr "با اینکه از آن برای ساخت “ماشین‌های مجازی” استفاده می‌شود، LXC به طور دقیق یک سیستم مجازی‌سازی نیست، بلکه سیستمی برای جدا کردن گروهی از فرآیندها نسبت به یکدیگر می‌باشد که درون یک میزبان اجرا می‌شوند. این سیستم از پیشرفت‌های اخیر در کرنل لینوکس بهره می‌برد، که بنام <emphasis>گروه‌های کنترل</emphasis> شناخته می‌شوند، به این معنی که مجموعه‌های مختلف از فرآیندها که “گروه” نامیده می‌شوند دید متفاوتی نسبت به جنبه‌های کلی سیستم دارند. از جمله این جنبه‌ها می‌توان به شناسه‌های فرآیند، پیکربندی شبکه و نقاط اتصال اشاره کرد. چنین گروهی از فرآیندهای ایزوله‌شده هیچ گونه دسترسی دیگر به سایر فرآیندهای سیستم ندارند و دسترسی آن‌ها به فایل سیستم تنها محدود به مجموعه‌ای کوچک می‌گردد. از این رو می‌تواند رابط شبکه و جدول مسیریابی مربوط به خود را داشته باشد و می‌تواند طوری پیکربندی شود که تنها مجموعه کوچکی از دستگاه‌های موجود در سیستم را مشاهده کند."

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>container</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there is no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgstr "این ویژگی‌ها می‌توانند به منظور جدا کردن خانواده فرآیند آغازی توسط <command>init</command> با یکدیگر ترکیب شده که نتیجه نهایی آن مشابه با ماشین مجازی است. نام رسمی چنین تنظیمی “مخزن” است (با توجه به نام LXC که برابر است با: <emphasis>LinuX Containers</emphasis>) اما تفاوت عمده آن با ماشین‌های مجازی “واقعی” مانند Xen یا KVM در نبود یک کرنل دوم است؛ مخزن از همان کرنل سیستم میزبان استفاده می‌کند. اینکار مزایا و معایب خود را دارد: مزایای آن شامل عملکرد فوق‌العاده به دلیل نبود overhead و این حقیقت که کرنل یک دید سراسری نسبت به تمام فرآیندهای اجرایی روی سیستم دارد، به این منظور که عملیات زمان‌بندی می‌تواند به شیوه‌ای موثرتر انجام شود نسبت به حالتی که دو کرنل جداگانه باید مجموعه‌‌های مختلف از وظایف را مدیریت می‌کردند. از میان معایت نیز می‌توان به غیرممکن بودن اجرای یک کرنل مختلف درون یک مخزن اشاره کرد (خواه یک نسخه متفاوت لینوکس خواه یک سیستم عامل دیگر)."

msgid "<emphasis>NOTE</emphasis> LXC isolation limits"
msgstr "<emphasis>یادداشت</emphasis> محدودیت‌های انزوای LXC"

msgid "LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:"
msgstr "مخازن LXC سطحی از انزوا را مانند شبیه‌سازهای قدرتمند یا مجازی‌سازهای دیگر فراهم نمی‌کنند. به طور مشخص:"

msgid "since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;"
msgstr "از آنجا که کرنل بین سیستم میزبان و مخازن مشترک است، فرآیندهایی که محدود به مخازن هستند کماکان می‌توانند به پیام‌های کرنل دسترسی داشته باشند که در صورت انتشار پیام‌ها توسط یک مخزن می‌تواند منجر به افشای اطلاعات شود؛"

msgid "for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;"
msgstr "به دلیل مشابه، اگر به یک مخزن نفوذ شود و از یک آسیب‌پذیری کرنل استفاده گردد، سایر مخازن نیز تاثیر منفی می‌پذیرند؛"

msgid "on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers."
msgstr "در فایل سیستم، کرنل به بررسی مجوزهای کاربران و گروه‌ها مبتنی بر شناسه‌های عددی می‌پردازد؛ این شناسه‌ها ممکن است به کاربران و گروه‌های مختلف با توجه به مخزن اختصاص یابند که در صورت اشتراکی بودن قسمت‌های قابل نوشتن فایل سیستم بین مخازن باید مورد توجه قرار گیرد."

msgid "Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container."
msgstr "از آنجا که با مفهوم ایزوله کردن و نه یک راهکار مجازی‌سازی ساده سروکار داریم، برپایی مخازن LXC بسیار پیچیده‌تر از اجرای debian-installer در یک ماشین مجازی است. ابتدا چندین پیشنیاز را بررسی کرده سپس به قسمت پیکربندی شبکه می‌رویم؛ در این قسمت است که می‌توانیم سیستم را درون یک مخزن اجرا کنیم."

msgid "Preliminary Steps"
msgstr "گام‌های مقدماتی"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">lxc</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains the tools required to run LXC, and must therefore be installed."
msgstr "بسته <emphasis role=\"pkg\">lxc</emphasis> شامل ابزار مورد نیاز برای نصب و اجرای LXC است."

msgid "<primary><filename>/sys</filename></primary><secondary><filename>/sys/fs/cgroup</filename></secondary>"
msgstr ""

msgid "LXC also requires the <emphasis>control groups</emphasis> configuration system, which is a virtual filesystem to be mounted on <filename>/sys/fs/cgroup</filename>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration."
msgstr "LXC همچنین به سیستم پیکربندی <emphasis>control groups</emphasis> نیاز دارد که به صورت یک فایل سیستم مجازی به <filename>/sys/fs/cgroup</filename> متصل می‌شود. از آنجا که دبیان ۸ به systemd روی آورده، که خود مبتنی بر گروه‌های کنترل است، اینکار در زمان راه‌اندازی سیستم بدون هیچ پیکربندی خاص صورت می‌گیرد."

msgid "Network Configuration"
msgstr "پیکربندی شبکه"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary>network configuration</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgid "The goal of installing LXC is to set up virtual machines; while we could, of course, keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgstr "هدف از نصب LXC برپایی ماشین‌های مجازی است؛ با اینکه می‌توانیم آن‌ها را به صورت ایزوله در شبکه قرار دهیم و تنها از طریق فایل سیستم با آن‌ها تعامل کنیم، اکثر موارد کاربردی شامل دسترسی حداقلی شبکه به مخازن است. در حالت معمولی، به هر مخزن یک رابط مجازی شبکه اختصاص می‌یابد که از طریق bridge به یک رابط حقیقی شبکه متصل است. این رابط مجازی هم می‌تواند به رابط فیزیکی میزبان (که در این صورت مخزن به صورت مستقیم در شبکه قرار می‌گیرد) هم می‌تواند به رابط مجازی دیگری در میزبان متصل شود (که میزبان کار فیلتر و مسیریابی ترافیک را انجام می‌دهد). در هر صورت، بسته <emphasis role=\"pkg\">bridge-utils</emphasis> مورد نیاز خواهد بود."

#, fuzzy
#| msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance, <literal>eth0</literal> or <literal>enp1s0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgstr "مورد اول به سادگی ویرایش فایل <filename>/etc/network/interfaces</filename>، انتقال پیکربندی برای رابط فیزیکی (برای نمونه <literal>eth0</literal>) به رابط bridge (معمولا <literal>br0</literal>) و پیکربندی پیوند بین آن‌ها است. برای نمونه، اگر فایل پیکربندی رابط شبکه شامل مدخل‌های زیر باشد:"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>br</literal> interface</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>virtualization</primary>"
msgid "<primary><literal>br</literal>, network interface</primary>"
msgstr "<primary>مجازی‌سازی</primary>"

msgid ""
"auto eth0\n"
"iface eth0 inet dhcp"
msgstr ""
"auto eth0\n"
"iface eth0 inet dhcp"

msgid "They should be disabled and replaced with the following:"
msgstr "آن‌ها باید غیرفعال شده و با مدخل‌های زیر جایگزین گردند:"

#, fuzzy
#| msgid ""
#| "auto xenbr0\n"
#| "iface xenbr0 inet dhcp\n"
#| "    bridge_ports eth0\n"
#| "    bridge_maxwait 0\n"
#| "    "
msgid ""
"auto br0\n"
"iface br0 inet dhcp\n"
"    bridge-ports <replaceable>eth0</replaceable>"
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

msgid "The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <literal>eth0</literal> as well as the interfaces defined for the containers."
msgstr "تاثیر این پیکربندی مشابه با حالتی خواهد بود که مخازن به صورت ماشین‌هایی به شبکه فیزیکی یکسانی از طریق میزبان متصل می‌شدند. پیکربندی “bridge” انتقال فریم‌های Ethernet را بین تمام رابط‌های bridged مدیریت می‌کند که شامل <literal>eth0</literal> همراه با رابط‌های تعریف شده برای مخازن می‌باشد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>tap</literal> interface</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgid "In cases where this configuration cannot be used (for instance, if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgstr "در مواری که این پیکربندی نمی‌تواند استفاده شود (برای نمونه اگر هیچ نشانی عمومی IP نتواند به مخازن اختصاص یابد)، یک رابط مجازی <emphasis>tap</emphasis> ایجاد و به bridge متصل می‌شود. معادل توپولوژی شبکه سپس به میزبانی با یک کارت شبکه ثانویه تبدیل شده که به یک سوئیچ جداگانه متصل است، همراه با مخازن متصل به آن سوئیچ. میزبان باید به صورت یک gateway برای مخازنی عمل کند که قصد ارتباط با دنیای خارج را دارند."

msgid "In addition to <emphasis role=\"pkg\">bridge-utils</emphasis>, this “rich” configuration requires the <emphasis role=\"pkg\">vde2</emphasis> package; the <filename>/etc/network/interfaces</filename> file then becomes:"
msgstr "علاوه بر <emphasis role=\"pkg\">bridge-utils</emphasis>، این پیکربندی “غنی” نیازمند بسته <emphasis role=\"pkg\">vde2</emphasis> است؛ فایل <filename>/etc/network/interfaces</filename> سپس به صورت زیر در می‌آید:"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">vde2</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid ""
#| "# Interface eth0 is unchanged\n"
#| "auto eth0\n"
#| "iface eth0 inet dhcp\n"
#| "\n"
#| "# Virtual interface \n"
#| "auto tap0\n"
#| "iface tap0 inet manual\n"
#| "  vde2-switch -t tap0\n"
#| "\n"
#| "# Bridge for containers\n"
#| "auto br0\n"
#| "iface br0 inet static\n"
#| "  bridge-ports tap0\n"
#| "  address 10.0.0.1\n"
#| "  netmask 255.255.255.0"
msgid ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"    vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"    bridge-ports tap0\n"
"    address 10.0.0.1\n"
"    netmask 255.255.255.0"
msgstr ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"

msgid "The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <literal>br0</literal> interface."
msgstr "شبکه می‌تواند یا به صورت ایستا درون مخازن یا به صورت پویا از طریق سرور DHCP درون میزبان برپا شود. چنین سرور DHCP باید طوری پیکربندی شود که به پرس و جوهای موجود در رابط <literal>br0</literal> پاسخ دهد."

msgid "Setting Up the System"
msgstr "برپایی سیستم"

#, fuzzy
#| msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> package includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgstr "اکنون بیاید فایل سیستم مورد نیاز مخزن را برپا کنیم. از آنجا که این “ماشین مجازی” به صورت مستقیم روی سخت‌افزار اجرا نخواهد شد، در مقایسه با یک فایل سیستم استاندارد رعایت برخی نکات ضروری است، به خصوص تا آنجا که به کرنل، دستگاه‌ها و کنسول‌ها مربوط باشد. خوشبختانه <emphasis role=\"pkg\">lxc</emphasis> شامل اسکریپت‌هایی است که اکثر این پیکربندی را به صورت خودکار انجام می‌دهند. برای نمونه، دستورات پیش رو (که نیازمند بسته‌های <emphasis role=\"pkg\">debootstrap</emphasis> و <emphasis role=\"pkg\">rsync</emphasis> هستند) اقدام به نصب یک مخزن دبیان می‌کنند:"

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
#| "</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
#| "Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
#| "Downloading debian minimal ...\n"
#| "I: Retrieving Release \n"
#| "I: Retrieving Release.gpg \n"
#| "[...]\n"
#| "Download complete.\n"
#| "Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
#| "[...]\n"
#| "Root password is 'sSiKhMzI', please change !\n"
#| "root@mirwiz:~# </computeroutput>\n"
#| "        "
msgid ""
"<computeroutput># </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-stable-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"Root password is 'sSiKhMzI', please change !\n"
"root@mirwiz:~# </computeroutput>\n"
"        "

msgid "Note that the filesystem is initially created in <filename>/var/cache/lxc</filename>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required."
msgstr "به یاد داشته باشید که فایل سیستم به صورت اولیه در <filename>/var/cache/lxc</filename> ایجاد شد، سپس به مقصد خود انتقال یافت. اینکار امکان ایجاد مخازن مشابه را با سرعت بیشتری فراهم می‌کند، چرا که تنها عملیات رونوشت‌گیری مورد نیاز است."

#, fuzzy
#| msgid "Note that the debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgid "Note that the Debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgstr "به یاد داشته باشید که اسکریپت ایجاد قالب دبیان یک گزینه <option>--arch</option> به منظور تعیین معماری سیستم و یک گزینه <option>--release</option> به منظور نصب نسخه‌ای بجز نسخه انتشار اصلی از دبیان را قبول می‌کند. همچنین می‌توانید با استفاده از متغیر محیطی <literal>MIRROR</literal> از یک mirror مخصوص به دبیان استفاده کنید."

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package further creates a bridge interface <literal>lxcbr0</literal>, which by default is used by all newly created containers via <filename>/etc/lxc/default.conf</filename> and the <filename>lxc-net</filename> service:"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/lxc/default.conf</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>lxc-net.service</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>veth</literal> interface</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "<primary><literal>veth</literal>, network interface</primary>"
msgstr ""

#, fuzzy
#| msgid ""
#| "lxc.network.type = veth\n"
#| "lxc.network.flags = up\n"
#| "lxc.network.link = br0\n"
#| "lxc.network.hwaddr = 4a:49:43:49:79:20"
msgid ""
"lxc.net.0.type = veth\n"
"lxc.net.0.link = lxcbr0\n"
"lxc.net.0.flags = up"
msgstr ""
"lxc.network.type = veth\n"
"lxc.network.flags = up\n"
"lxc.network.link = br0\n"
"lxc.network.hwaddr = 4a:49:43:49:79:20"

#, fuzzy
#| msgid "These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <literal>br0</literal> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated."
msgid "These entries mean, respectively, that a virtual interface will be created in every new container; that it will automatically be brought up when said container is started; and that it will be automatically connected to the <literal>lxcbr0</literal> bridge on the host. You will find these settings in the created container's configuration (<filename>/var/lib/lxc/testlxc/config</filename>), where also the device' MAC address will be specified in <literal>lxc.net.0.hwaddr</literal>. Should this last entry be missing or disabled, a random MAC address will be generated."
msgstr "وجود این مدخل‌ها به این معنی است که یک رابط مجازی برای مخزن ایجاد خواهد شد؛ که به صورت خودکار هنگام آغاز مخزن شروع می‌شوند؛ که به صورت خودکار به bridge موجود در میزبان بنام <literal>br0</literal> متصل می‌شوند؛ که نشانی MAC آن مطابق بالا خواهد بود. در صورت فقدان یا غیرفعال بودن این گزینه آخر، از یک نشانی MAC تصادفی استفاده خواهد شد."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary>container configuration</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Another useful entry in that file is the setting of the hostname:"
msgstr "یک مدخل مفید دیگر در آن فایل تنظیم نام میزبان است:"

#, fuzzy
#| msgid "lxc.utsname = testlxc"
msgid "lxc.uts.name = testlxc"
msgstr "lxc.utsname = testlxc"

msgid "The newly-created filesystem now contains a minimal Debian system and a network interface."
msgstr ""

msgid "Starting the Container"
msgstr "آغاز مخزن"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-start</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-attach</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Now that our virtual machine image is ready, let's start the container:"
msgid "Now that our virtual machine image is ready, let's start the container with <command>lxc-start --name=testlxc</command>."
msgstr "اکنون که image ماشین مجازی آماده است، بیایید مخزن را آغاز کنیم:"

msgid "In LXC releases following 2.0.8, root passwords are not set by default. We can set one running <command>lxc-attach -n testlxc <replaceable>passwd</replaceable></command> if we want. We can login with:"
msgstr ""

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
#| "</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
#| "</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
#| "\n"
#| "testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
#| "Password: \n"
#| "Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
#| "\n"
#| "The programs included with the Debian GNU/Linux system are free software;\n"
#| "the exact distribution terms for each program are described in the\n"
#| "individual files in /usr/share/doc/*/copyright.\n"
#| "\n"
#| "Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
#| "permitted by applicable law.\n"
#| "root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
#| "<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
#| "root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
#| "root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
#| "root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
#| "root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
#| "root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
#| "root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
#| "root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
#| "root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
#| "root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
#| "root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
#| "root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
#| "root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
#| "root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
#| "root@testlxc:~# </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput><![CDATA[Connected to tty 1\n"
"Type <Ctrl+a q> to exit the console, <Ctrl+a Ctrl+a> to enter Ctrl+a itself\n"
"\n"
"Debian GNU/Linux 11 testlxc tty1\n"
"\n"
"testlxc login: ]]></computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 5.10.0-11-amd64 #1 SMP Debian 5.10.92-1 (2022-01-18) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"Last login: Wed Mar  9 01:45:21 UTC 2022 on console\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf\n"
"</userinput><computeroutput>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root           1  0.0  0.2  18964 11464 ?        Ss   01:36   0:00 /sbin/init\n"
"root          45  0.0  0.2  31940 10396 ?        Ss   01:37   0:00 /lib/systemd/systemd-journald\n"
"root          71  0.0  0.1  99800  5724 ?        Ssl  01:37   0:00 /sbin/dhclient -4 -v -i -pf /run/dhclient.eth0.pid [..]\n"
"root          97  0.0  0.1  13276  6980 ?        Ss   01:37   0:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups\n"
"root         160  0.0  0.0   6276  3928 pts/0    Ss   01:46   0:00 /bin/login -p --\n"
"root         169  0.0  0.0   7100  3824 pts/0    S    01:51   0:00  \\_ -bash\n"
"root         172  0.0  0.0   9672  3348 pts/0    R+   01:51   0:00      \\_ ps auxwf\n"
"root         164  0.0  0.0   5416  2128 pts/1    Ss+  01:49   0:00 /sbin/agetty -o -p -- \\u --noclear [...]\n"
"root@testlxc:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
"</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
"root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
"root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
"root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
"root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
"root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
"root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
"root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
"root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
"root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
"root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
"root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
"root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
"root@testlxc:~# </computeroutput>"

msgid "We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<filename>/var/lib/lxc/testlxc/rootfs</filename>). We can exit the console with <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."
msgstr "اکنون درون مخزن هستیم؛ دسترسی به فرآیندها تنها محدود به آن‌هایی است که توسط مخزن آغاز شده باشند و دسترسی به فایل سیستم تنها بخش کوچکی از فایل سیستم کامل <filename>/var/lib/lxc/testlxc/rootfs</filename> را شامل می‌شود. با استفاده از کلید ترکیبی <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo> می‌توانیم از کنسول خارج شویم."

#, fuzzy
#| msgid "Note that we ran the container as a background process, thanks to the <option>--daemon</option> option of <command>lxc-start</command>. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgid "Note that we ran the container as a background process, thanks to <command>lxc-start</command> starting using the <option>--daemon</option> option by default. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgstr "به یاد داشته باشید که مخزن را به عنوان یک فرآیند پس‌زمینه اجرا کردیم، به لطف گزینه <option>--daemon</option> از <command>lxc-start</command>. با استفاده از دستور <command>lxc-stop --name=testlxc</command> می‌توانیم مخزن را متوقف کنیم."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>LXC</primary><secondary><command>lxc-stop</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <command>lxc-autostart</command> which starts containers whose <literal>lxc.start.auto</literal> option is set to 1). Finer-grained control of the startup order is possible with <literal>lxc.start.order</literal> and <literal>lxc.group</literal>: by default, the initialization script first starts containers which are part of the <literal>onboot</literal> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <literal>lxc.start.order</literal> option."
msgstr "بسته <emphasis role=\"pkg\">lxc</emphasis> شامل یک اسکریپت راه‌اندازی است که به صورت خودکار یک یا چند مخزن را در زمان راه‌اندازی میزبان آغاز می‌کند (مبتنی بر <command>lxc-autostart</command> است که به صورت خودکار مخازن شامل گزینه <literal>lxc.start.auto</literal> برابر ۱ را راه‌اندازی می‌کند). با استفاده از <literal>lxc.start.order</literal> و <literal>lxc.group</literal> می‌توان کنترل بیشتری روی ترتیب اجرای مخازن اعمال کرد: به صورت پیشفرض، اسکریپت راه‌اندازی ابتدا مخازنی را آغاز می‌کند که جزو گروه <literal>onboot</literal> باشند سپس به سراغ مخازن دیگر می‌رود). در هر دو مورد، ترتیب درون هر گروه توسط گزینه <literal>lxc.start.order</literal> مشخص می‌شود."

msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgstr "<emphasis>مطالعه بیشتر</emphasis> مجازی‌سازی انبوه"

msgid "Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <literal>tap</literal> and <literal>veth</literal> interfaces should be enough in many cases."
msgstr "از آنجا که LXC یک سیستم ایزوله‌کردن سبک به حساب می‌آید، می‌تواند به منظور میزبانی از سرورهای مجازی انبوه سازگار شود. پیکربندی شبکه از آنچه در این قسمت توضیح دادیم به مراتب پیچیده‌تر خواهد بود اما پیکربندی “غنی” با استفاده از رابط‌های <literal>tap</literal> و <literal>veth</literal> در اکثر موارد به شیوه توضیح داده شده کافی خواهد بود."

msgid "It may also make sense to share part of the filesystem, such as the <filename>/usr</filename> and <filename>/lib</filename> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <literal>lxc.mount.entry</literal> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage."
msgstr "به منظور پیشگیری از نصب مجدد نرم‌افزارهایی که برای چندین مخزن کاربردی هستند، معقول بنظر می‌رسد که قسمتی از فایل سیستم مانند <filename>/usr</filename> و <filename>/lib</filename> را به اشتراک بگذاریم. اینکار معمولا با مدخل‌های <literal>lxc.mount.entry</literal> درون فایل پیکربندی مخازن انجام می‌شود. یک تاثیر جانبی جالب این است که فرآیندها از حافظه فیزیکی کمتری استفاده می‌کنند، چرا که کرنل قادر به تشخیص برنامه‌هایی است که به صورت اشتراکی کار می‌کنند. هزینه حاشیه‌ای یک مخزن اضافی دیگر می‌تواند به فضای دیسک اختصاص یافته به داده خاص و چند فرآیند اضافی که کرنل برای زمان‌بندی و مدیریت استفاده می‌کند، کاهش یابد."

msgid "We haven't described all the available options, of course; more comprehensive information can be obtained from the <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> and <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual pages and the ones they reference."
msgstr "البته، تمام گزینه‌های موجود را بررسی نکردیم؛ اطلاعات جامع بیشتر از طریق صفحات راهنمای <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> و <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> همراه با سایر مراجع آن قابل دسترس است."

msgid "Virtualization with KVM"
msgstr "مجازی‌سازی با KVM"

msgid "<primary>Kernel-based Virtual Machine</primary><see>KVM</see>"
msgstr ""

msgid "KVM, which stands for <emphasis>Kernel-based Virtual Machine</emphasis>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <command>qemu-*</command> commands: it is still about KVM."
msgstr "KVM، که مخفف عبارت <emphasis>Kernel-based Virtual Machine</emphasis> است، در درجه اول یک افزونه کرنل به حساب می‌آید که اکثر زیرساخت مورد نیاز یک مجازی‌ساز را فراهم می‌کند اما خود یک مجازی‌ساز نیست. کنترل واقعی مجازی‌سازی توسط برنامه‌ای مبتنی بر QEMU انجام می‌شود. نگران نباشید اگر در این قسمت دستورات مربوط به <command>qemu-*</command> را مشاهده کنید: تمام آن‌ها مرتبط با KVM هستند."

msgid "Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <filename>/proc/cpuinfo</filename>."
msgstr "برخلاف سایر سیستم‌های مجازی‌سازی، KVM از ابتدا درون کرنل لینوکس قرار گرفت. توسعه‌دهندگان آن تصمیم گرفتند از مجموعه دستورالعمل‌های پردازنده برای مجازی‌سازی (Intel-VT و AMD-V) استفاده کنند که اینکار باعث می‌شود KVM سبک، ظریف و سازگار با منابع پایین باشد. نقطه ضعف آن این است که KVM روی هر رایانه‌ای نمی‌تواند اجرا شود بلکه فقط برخی پردازنده‌های خاص از آن پشتیبانی می‌کنند. برای رایانه‌های مبتنی بر x86، می‌توانید به دنبال پرچم‌های مخصوص پردازنده به نام “vmx” یا “svm” در فایل <filename>/proc/cpuinfo</filename> بگردید."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary><filename>/proc</filename></primary><secondary><filename>/proc/cpuinfo</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization."
msgstr "با پشتیبانی مداوم Red Hat از توسعه آن، KVM کم و بیش به مرجع مجازی‌سازی در لینوکس تبدیل شده است."

msgid "<primary><command>virt-install</command></primary>"
msgstr "<primary><command>virt-install</command></primary>"

#, fuzzy
#| msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The virtual <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgstr "KVM برخلاف ابزاری مانند VirtualBox، شامل رابط کاربری برای مدیریت ماشین‌های مجازی نیست. بسته <emphasis role=\"pkg\">qemu-kvm</emphasis> تنها شامل یک فایل اجرایی برای آغاز یک ماشین مجازی است، همراه با اسکریپت‌های راه‌اندازی که اقدام به بارگیری افزونه‌های مناسب کرنل می‌کنند."

msgid "<primary>libvirt</primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>OpenVZ</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>UML</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <command>virtual-manager</command> is a graphical interface that uses libvirt to create and manage virtual machines."
msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare, and UML). <command>virt-manager</command> is a graphical interface that uses <emphasis>libvirt</emphasis> to create and manage virtual machines."
msgstr "خوشبختانه، Red Hat برای غلبه بر این مشکل مجموعه ابزاری فراهم کرده است که شامل کتابخانه <emphasis>libvirt</emphasis> و ابزارهای <emphasis>virtual machine manager</emphasis> می‌شوند. libvirt امکان مدیریت ماشین‌های مجازی را به یک شیوه یکسان فراهم می‌کند، جدا از سیستم مجازی‌سازی که در پشت صحنه قرار دارد (هم اکنون از QEMU، KVM، Xen، LXC، OpenVZ، VirtualBox، VMWare و UML پشتیبانی می‌کند). <command>virtual-manager</command> یک رابط گرافیکی است که با استفاده از libvirt ماشین‌های مجازی را مدیریت می‌کند."

msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>daemon</primary><secondary>libvirtd</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>libvirtd.service</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "We first install the required packages, with <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-bin</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgid "We first install the required packages, with <command>apt-get install libvirt-clients libvirt-daemon-system qemu-kvm virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-daemon-system</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. <emphasis role=\"pkg\">libvirt-clients</emphasis> provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgstr "ابتدا با استفاده از <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command> بسته‌های مورد نیاز را نصب می‌کنیم. <emphasis role=\"pkg\">libvirt-bin</emphasis> فرآیند پس‌زمینه <command>libvirtd</command> را فراهم می‌کند، که امکان مدیریت (معمولا راه دور) ماشین‌های مجازی اجرای در سیستم میزبان را فراهم کرده و ماشین‌های مجازی مورد نیاز را در زمان راه‌اندازی میزبان آغاز می‌کند. علاوه بر این، این بسته ابزار خط-فرمان <command>virsh</command> را فراهم می‌کند که امکان کنترل ماشین‌های <command>libvirtd</command> را بوجود می‌آورد."

#, fuzzy
#| msgid "<primary><command>virsh</command></primary>"
msgid "<primary><command>virt-viewer</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

msgid "The <emphasis role=\"pkg\">virtinst</emphasis> package provides <command>virt-install</command>, which allows creating virtual machines from the command line. Finally, <emphasis role=\"pkg\">virt-viewer</emphasis> allows accessing a VM's graphical console."
msgstr "بسته <emphasis role=\"pkg\">virtinst</emphasis> شامل <command>virt-install</command> می‌شود که امکان ایجاد ماشین‌های مجازی از خط فرمان را فراهم می‌کند. در نهایت، <emphasis role=\"pkg\">virt-viewer</emphasis> اجازه دسترسی به کنسول گرافیکی یک ماشین مجازی را بوجود می‌آورد."

msgid "Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <xref linkend=\"sect.lxc.network\" />)."
msgstr "درست مانند Xen و LXC، متداول‌ترین پیکربندی شبکه شامل یک bridge که رابط‌های شبکه ماشین‌های مجازی را گروه‌بندی می‌کند، می‌باشد (<xref linkend=\"sect.lxc.network\" /> را مشاهده کنید)."

msgid "Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network."
msgstr "به طور متقابل، در پیکربندی پیشفرض فراهم شده توسط KVM، یک نشانی خصوصی به ماشین مجازی اختصاص می‌یابد (در محدوده 192.168.122.0/24) و NAT طوری تنظیم می‌شود که ماشین مجازی بتواند به شبکه خارجی دسترسی داشته باشد."

msgid "The rest of this section assumes that the host has an <literal>eth0</literal> physical interface and a <literal>br0</literal> bridge, and that the former is connected to the latter."
msgstr "باقیمانده این قسمت با توجه به اینکه میزبان دارای یک رابط فیزیکی <literal>eth0</literal> و bridge <literal>br0</literal> است ادامه می‌یابد، به طوری که اولی به دومی متصل شده است."

msgid "Installation with <command>virt-install</command>"
msgstr "نصب با <command>virt-install</command>"

msgid "Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line."
msgstr "ایجاد یک ماشین مجازی بسیار شبیه یک سیستم عادی است، با این تفاوت که ویژگی‌های ماشین مجازی به صورت گزینه‌های بی‌پایان در خط فرمان قرار می‌گیرند."

msgid "Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <xref linkend=\"sect.remote-desktops\" /> for details), which will allow us to control the installation process."
msgstr "در عمل، یعنی از یک نصب کننده دبیان استفاده خواهیم کرد، با راه‌اندازی ماشین مجازی روی یک درایو DVD-ROM که به یک تصویر از DVD دبیان ذخیره شده در سیستم میزبان نگاشت شده است. ماشین مجازی از طریق پروتکل VNC کنسول گرافیکی خود را آماده می‌کند (<xref linkend=\"sect.remote-desktops\" /> را مشاهده کنید) که اینکار به ما اجازه می‌دهد فرآیند نصب را کنترل کنیم."

#, fuzzy
#| msgid "We first need to tell libvirtd where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgid "We first need to tell <command>libvirtd</command> where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgstr "ابتدا باید به libvirtd بگوییم تصاویر دیسک را در کجا ذخیره کند، مگر مکان پیشفرض <filename>/var/lib/libvirt/images/</filename> مناسب باشد."

#, fuzzy
#| msgid ""
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
#| "<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
#| "<computeroutput>Pool srv-kvm created\n"
#| "\n"
#| "root@mirwiz:~# </computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>mkdir /srv/kvm\n"
"</userinput><computeroutput># </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm\n"
"</userinput><computeroutput>Pool srv-kvm created\n"
"\n"
"# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"

msgid "<emphasis>TIP</emphasis> Add your user to the libvirt group"
msgstr "<emphasis>نکته</emphasis> افزودن کاربر خود به گروه libvirt"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>group</primary><secondary><literal>libvirt</literal></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yourself to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgstr "تمام نمونه‌های این قسمت فرض را بر این می‌گذارند که شما به عنوان root دستورات را اجرا می‌کنید. اگر قصد کنترل یک libvirt محلی را دارید، یا باید root باشید یا عضوی از گروه <literal>libvirt</literal> (که به صورت پیشفرض فعال نیست). بنابراین به منظور جلوگیری از اجرای تمام دستورات به عنوان root می‌توانید با افزودن کاربر خود به گروه <literal>libvirt</literal> تمام دستورات آن را تحت مجوز کاربری خود اجرا کنید."

msgid "Let us now start the installation process for the virtual machine, and have a closer look at <command>virt-install</command>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed."
msgstr "اکنون بیایید فرآیند نصب ماشین مجازی را آغاز کرده و نگاهی بر مهم‌ترین گزینه‌های <command>virt-install</command> بیندازیم. این دستور، ماشین مجازی و پارامترهای آن را در libvirtd ثبت می‌کند سپس به اجرای آن پرداخته تا فرآیند نصب ادامه یابد."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
#| "               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
#| "               --name testkvm            <co id=\"virtinst.name\"></co>\n"
#| "               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
#| "               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
#| "               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
#| "               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
#| "               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
#| "               --os-type linux           <co id=\"virtinst.os\"></co>\n"
#| "               --os-variant debianwheezy\n"
#| "</userinput><computeroutput>\n"
#| "Starting install...\n"
#| "Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
#| "Creating domain...                    |    0 B     00:00\n"
#| "Guest installation complete... restarting guest.\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --memory 2048             <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10  <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-11.2.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=virbr0   <co id=\"virtinst.network\"></co>\n"
"               --graphics vnc            <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debiantesting\n"
"</userinput><computeroutput>\n"
"\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'\n"
"\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
"               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debianwheezy\n"
"</userinput><computeroutput>\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
"Creating domain...                    |    0 B     00:00\n"
"Guest installation complete... restarting guest.\n"
"</computeroutput>"

msgid "The <literal>--connect</literal> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<literal>/system</literal>) from others (<literal>/session</literal>)."
msgstr "گزینه <literal>--connect</literal> مشخص می‌کند از کدام “hypervisor” استفاده شود. فرم استفاده از آن شامل یک URL همراه با سیستم مجازی‌سازی مرتبط (<literal>xen://</literal>، <literal>qemu://</literal>، <literal>lxc://</literal>، <literal>openvz://</literal>، <literal>vbox://</literal>) و ماشینی که باید از آن میزبانی کند می‌باشد (در صورت استفاده از localhost می‌تواند خالی باشد). علاوه بر این و در مورد QEMU/KVM، هر کاربر می‌تواند با استفاده از مجوزهای محدودشده ماشین‌های مجازی را مدیریت کند و مسیر URL امکان تفاوت قائل شدن بین ماشین‌های “سیستم” (<literal>/system</literal>) را از دیگر (<literal>/session</literal>) فراهم می‌کند."

msgid "Since KVM is managed the same way as QEMU, the <literal>--virt-type kvm</literal> allows specifying the use of KVM even though the URL looks like QEMU."
msgstr "از آنجا که KVM به شیوه مشابه QEMU مدیریت می‌شود، <literal>--virt-type kvm</literal> امکان مشخص کردن استفاده از KVM با وجود تشابه با URL QEMU را فراهم می‌کند."

msgid "The <literal>--name</literal> option defines a (unique) name for the virtual machine."
msgstr "گزینه <literal>--name</literal> یک نام (منحصربفرد) برای ماشین مجازی تعریف می‌کند."

#, fuzzy
#| msgid "The <literal>--ram</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgid "The <literal>--memory</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgstr "گزینه <literal>--ram</literal> میزان RAM (به مگابایت) اختصاص یافته به ماشین مجازی را تعریف می‌کند."

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary><literal>qcow2</literal></primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>raw</literal>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space."
msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>qcow2</literal>) allows starting with a small file that only grows when the virtual machine starts actually using space."
msgstr "گزینه <literal>--disk</literal> مکان فایل تصویری که قرار است هارد دیسک ماشین مجازی در آن قرار گیرد را تعریف می‌کند؛ این فایل با استفاده از پارامتر <literal>size</literal> (به گیگابایت) در صورت موجود نبودن، ایجاد می‌گردد. پارامتر <literal>format</literal> امکان ذخیره‌سازی فایل تصویر را در قالب‌های گوناگون بوجود می‌آورد. قالب پیشفرض (<literal>raw</literal>) یک فایل تکی است که با محتوا و اندازه دیسک سازگاری داشته باشد. در اینجا از یک قالب پیشرفته‌تر استفاده کرده‌ایم، که مختص به QEMU می‌باشد و امکان شروع با یک فایل کوچک را می‌دهد که به مرور زمان و نیاز ماشین مجازی به فضای بیشتر، بزرگ‌تر می‌شود."

msgid "The <literal>--cdrom</literal> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <literal>/dev/cdrom</literal>)."
msgstr "گزینه <literal>--cdrom</literal> به منظور یافتن دیسک نوری برای فرآیند نصب استفاده می‌شود. مسیر می‌تواند شامل یک مسیر محلی برای فایل ISO، یک URL که فایل می‌تواند از آنجا دریافت شود یا فایل دستگاه مربوط به یک درایو فیزیکی CD-ROM باشد (<literal>/dev/cdrom</literal>)."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>network</primary><secondary><literal>virbr</literal> interface</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>libvirt</primary><secondary><literal>virbr</literal></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "<primary><literal>virbr</literal>, network interface</primary>"
msgstr ""

msgid "The <literal>--network</literal> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24)."
msgstr "گزینه <literal>--network</literal> مشخص می‌کند کارت مجازی شبکه چطور با پیکربندی سیستم میزبان ادغام شود. عملکرد پیشفرض آن (که در این نمونه به صورت صریح بیان کرده‌ایم) ادغام آن با شبکه bridge از قبل موجود در سیستم است. اگر چنین bridge موجود نباشد، ماشین مجازی تنها با استفاده از NAT می‌تواند به شبکه فیزیکی دسترسی یابد، بنابراین یک نشانی در محدوده زیرشبکه 192.168.122.0/24 دریافت می‌کند."

msgid "The default network configuration, which contains the definition for a <literal>virbr0</literal> bridge interface, can be edited using <command>virsh net-edit default</command> and started via <command>virsh net-start default</command> if not already done automatically during system start."
msgstr ""

#, fuzzy
#| msgid "<literal>--vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, the <literal>--vnclisten=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgid "<literal>--graphics vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, <literal>--graphics vnc,listen=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgstr "گزینه <literal>--vnc</literal> بیان می‌کند که کنسول گرافیکی باید توسط VNC قابل ارائه باشد. عملکرد پیشفرض سرور VNC این است که تنها به رابط local گوش دهد؛ اگر برنامه VNC در یک میزبان دیگر قرار داشته باشد، برقراری ارتباط نیازمند برپایی تونل SSH می‌باشد (<xref linkend=\"sect.ssh-port-forwarding\" /> را مشاهده کنید). به همین ترتیب، از <literal>--vnclisten=0.0.0.0</literal> می‌توان برای دسترسی به سرور VNC از طریق تمام رابط‌های شبکه استفاده کرد؛ به یاد داشته باشید که در این صورت باید از یک طراحی firewall بهره‌مند شوید."

msgid "The <literal>--os-type</literal> and <literal>--os-variant</literal> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there."
msgstr "گزینه‌های <literal>--os-type</literal> و <literal>--os-variant</literal>، با توجه به برخی از ویژگ‌های سیستم عامل اشاره شده، امکان بهینه‌سازی چندین پارامتر ماشین مجازی را فراهم می‌کنند."

msgid "The full list of OS types can be shown using the <command>osinfo-query os</command> command from the <emphasis role=\"pkg\">libosinfo-bin</emphasis> package."
msgstr ""

msgid "At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <command>virt-viewer</command> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):"
msgstr "در این نقطه، ماشین مجازی در حال اجرا است و به منظور ادامه فرآیند نصب باید به کنسول گرافیکی متصل شویم. اگر عملیات قبل از طریق یک میزکار گرافیکی صورت گرفته باشد، این ارتباط به صورت مستقیم برقرار می‌شود. در غیر اینصورت، یا در حالتی که از راه دور اینکار را انجام می‌دهیم، <command>virt-viewer</command> با استفاده از هر محیط گرافیکی برای باز کردن کنسول گرافیکی می‌تواند اجرا شود (به یاد داشته باشید که دو مرتبه گذرواژه root درخواست می‌شود چرا که ۲ ارتباط SSH مورد نیاز است):"

msgid ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"

#, fuzzy
#| msgid "Installation with <command>virt-install</command>"
msgid "Connecting to installer session using <command>virt-viewer</command>"
msgstr "نصب با <command>virt-install</command>"

msgid "When the installation process ends, the virtual machine is restarted, now ready for use."
msgstr "زمانی که فرآیند نصب به پایان برسد، ماشین مجازی راه‌اندازی مجدد می‌گردد تا قابل استفاده شود."

msgid "Managing Machines with <command>virsh</command>"
msgstr "مدیریت ماشین‌های مجازی با <command>virsh</command>"

msgid "Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <command>libvirtd</command> for the list of the virtual machines it manages:"
msgstr "اکنون که نصب به پایان رسیده است، بیایید چگونگی مدیریت ماشین‌های مجازی را بررسی کنیم. اولین کاری که باید بکنیم پرسش از <command>libvirtd</command> برای فهرستی از ماشین‌های مجازی موجود است:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
#| " Id Name                 State\n"
#| "----------------------------------\n"
#| "  - testkvm              shut off\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  8 testkvm              shut off\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  - testkvm              shut off\n"
"</userinput>"

msgid "Let's start our test virtual machine:"
msgstr "بیایید ماشین مجازی آزمایشی خود را آغاز کنیم:"

msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"

msgid "We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <command>vncviewer</command>):"
msgstr "اکنون می‌توانیم دستورالعمل‌های ارتباط به کنسول گرافیکی را دریافت کنیم (نمایش VNC بازگشتی می‌تواند به عنوان پارامتر <command>vncviewer</command> استفاده شود):"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
#| "</userinput><computeroutput>:0</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>127.0.0.1:0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>:0</computeroutput>"

msgid "Other available <command>virsh</command> subcommands include:"
msgstr "سایر دستورات <command>virsh</command> عبارتند از:"

msgid "<literal>reboot</literal> to restart a virtual machine;"
msgstr "<literal>reboot</literal> برای راه‌اندازی مجدد یک ماشین مجازی؛"

msgid "<literal>shutdown</literal> to trigger a clean shutdown;"
msgstr "<literal>shutdown</literal> برای درخواست یک shutdown تمیز؛"

msgid "<literal>destroy</literal>, to stop it brutally;"
msgstr "<literal>destroy</literal> برای توقف خشن آن؛"

msgid "<literal>suspend</literal> to pause it;"
msgstr "<literal>suspend</literal> برای توقف عادی آن؛"

msgid "<literal>resume</literal> to unpause it;"
msgstr "<literal>resume</literal> برای ادامه فعالیت آن؛"

msgid "<literal>autostart</literal> to enable (or disable, with the <literal>--disable</literal> option) starting the virtual machine automatically when the host starts;"
msgstr "<literal>autostart</literal> برای فعال کردن (یا غیر فعال کردن با گزینه <literal>--disable</literal>) راه‌اندازی ماشین مجازی به صورت خودکار در زمان راه‌اندازی میزبان؛"

msgid "<literal>undefine</literal> to remove all traces of the virtual machine from <command>libvirtd</command>."
msgstr "<literal>undefine</literal> برای حذف تمام نشانه‌های ماشین مجازی از <command>libvirtd</command>."

msgid "All these subcommands take a virtual machine identifier as a parameter."
msgstr "تمام این دستورات شناسه ماشین مجازی را به عنوان یک پارامتر دریافت می‌کنند."

#, fuzzy
#| msgid "Installing an RPM based system in Debian with yum"
msgid "Installing an RPM based chroot in Debian with yum"
msgstr "نصب یک سیستم مبتنی بر RPM در دبیان با استفاده از yum"

#, fuzzy
#| msgid "<primary>LVM</primary>"
msgid "<primary>RPM</primary>"
msgstr "<primary>LVM</primary>"

#, fuzzy
#| msgid "<primary>libvirt</primary>"
msgid "<primary>chroot</primary>"
msgstr "<primary>libvirt</primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>yum</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>rpm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility (available in the package of the same name)."
msgid "If a chroot is meant to run Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>. But if it is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility, available as <command>yum4</command> in the <emphasis role=\"pkg\">nextgen-yum4</emphasis> package, since the original program has been removed from Debian before the <emphasis role=\"distribution\">Bullseye</emphasis> release due to being unmaintained, outdated, and obsoleted by <command>dnf</command>."
msgstr "اگر قرار باشد ماشین مجازی به منظور اجرای دبیان (یا یکی از توزیع‌های آن) راه‌اندازی گردد، سیستم می‌تواند با استفاده از <command>debootstrap</command> همانطور که توضیح داده شد راه‌اندازی شود. اما اگر قرار باشد ماشین مجازی به منظور اجرای یک سیستم مبتنی بر RPM (مانند Fedora، CentOS یا Scientific Linux) راه‌اندازی گردد، اینکار باید با استفاده از ابزار <command>yum</command> صورت گیرد (که در بسته‌ای با همین نام قرار دارد)."

#, fuzzy
#| msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum</command> to extract the remaining set of packages. But since we call <command>yum</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum4</command> to extract the remaining set of packages. But since we call <command>yum4</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgstr "این فرآیند شامل استفاده از <command>rpm</command> به منظور استخراج مجموعه‌ای از فایل‌ها، شامل فایل‌های پیکربندی <command>yum</command>، سپس فراخوانی <command>yum</command> برای استخراج سایر بسته‌های باقیمانده می‌باشد. اما از آنجا که فراخوانی <command>yum</command> خارج از chroot صورت می‌گیرد، باید برخی تغییرات موقتی را ایجاد کنیم. در نمونه زیر، chroot هدف عبارت است از <filename>/srv/centos</filename>."

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
#| "</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
#| "</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
#| "</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
#| "rpm: However assuming you know what you are doing...\n"
#| "warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
#| "</userinput><computeroutput>[...]\n"
#| "# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
#| "</userinput>"
msgid ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-9.2009.0.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-9.2009.0.el7.centos.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-9.2009.0.el7.centos.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum4 --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>chroot /srv/centos/\n"
"</userinput><computeroutput>[root@testsystem /]# </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"

msgid "Automated Installation"
msgstr "نصب خودکار"

msgid "<primary>deployment</primary>"
msgstr "<primary>گسترش</primary>"

msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The Falcot Corp administrators, like many administrators of large IT services, need tools to install (or reinstall) quickly, and automatically if possible, their new machines."
msgstr "مدیر سیستم‌های شرکت فالکوت، مانند سایر مدیر سیستم‌های خدمات بزرگ IT، به ابزاری نیاز دارند که فرآیند نصب (یا بازنصب) ماشین‌های خود را در کمترین زمان و در صورت ممکن به صورت خودکار انجام دهند."

msgid "These requirements can be met by a wide range of solutions. On the one hand, generic tools such as SystemImager handle this by creating an image based on a template machine, then deploy that image to the target systems; at the other end of the spectrum, the standard Debian installer can be preseeded with a configuration file giving the answers to the questions asked during the installation process. As a sort of middle ground, a hybrid tool such as FAI (<emphasis>Fully Automatic Installer</emphasis>) installs machines using the packaging system, but it also uses its own infrastructure for tasks that are more specific to massive deployments (such as starting, partitioning, configuration and so on)."
msgstr "این نیازمندی‌ها توسط طیف گسترده‌ای از راه حل‌ها برطرف می‌شوند. از یک طرف ابزار عمومی مانند SystemImager با ایجاد یک تصویر از ماشین نمونه، آن را روی ماشین‌های هدف گسترش می‌دهد؛ از طرف دیگر، نصب کننده دبیان این قابلیت را دارد که با استفاده از یک فایل پیکربندی خاص به پرسش‌های مطرح شده طی فرآیند نصب به صورت خودکار پاسخ دهد. به عنوان یک راه حل ترکیبی، ابزاری مانند FAI، که مخفف <emphasis>Fully Automatic Installer</emphasis> است، ماشین‌ها را با استفاده از سیستم بسته‌بندی نصب، اما از زیرساخت خود به منظور فرآیندهای پیچیده‌تر مانند راه‌اندازی، پارتیشن‌بندی و پیکربندی استفاده می‌کند."

#, fuzzy
#| msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an i386 image won't fit on a powerpc machine, and so on)."
msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an amd64 image won't fit on a powerpc machine, and so on)."
msgstr "هر یک از این راه حل‌ها نقاط ضعف و قوت خود را دارند: SystemImager مستقل از سیستم‌های بسته‌بندی کار می‌کند که این امر به مدیریت مجموعه‌ای بزرگ از ماشین‌ها با توزیع‌های مختلف لینوکس منجر می‌شود. همچنین شامل یک سیستم بروزرسانی است که نیازمند نصب مجدد نمی‌باشد، اما این سیستم تنها در صورتی قابل اعتماد خواهد بود که هیچ از یک ماشین‌های زیر مجموعه آن به تنهایی تغییر نکرده باشند؛ به عبارت دیگر، کاربر نباید نرم‌افزاری را بروزرسانی یا نصب کند. به همین شکل، بروزرسانی‌های امنیتی نباید به صورت خودکار صورت پذیرند، چرا که باید توسط یک تصویر مرجع و مرکزی از SystemImager مدیریت شوند. این راه حل نیازمند یکپارچه بودن ماشین‌های هدف از نقطه نظر معماری رایانه است، در غیر اینصورت از تصاویر بسیار گوناگونی برای مدیریت آن باید استفاده شود (یک تصویر i386 با ماشین powerpc سازگار نیست)."

#, fuzzy
#| msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly simple-CDD (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even the simple-CDD solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly <emphasis role=\"pkg\">simple-cdd</emphasis> (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even this solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgstr "از طرف دیگر، یک نصب خودکار با استفاده از debian-installer می‌تواند با توجه به نیاز هر ماشین تغییر یابد: برنامه نصب کننده کرنل و بسته‌های نرم‌افزاری مناسب را از مخازن خود دریافت، سخت‌افزار موجود را شناسایی، تمام هارد دیسک را به منظور استفاده بهینه از فضا پارتیشن‌بندی، سیستم مورد نیاز دبیان را نصب و یک راه‌انداز مناسب را تنظیم می‌کند. اگرچه، نصب‌کننده استاندارد تنها نسخه‌های استاندارد دبیان را همراه با سیستم پایه و مجموعه‌ای از “وظایف” انتخابی را نصب می‌کند؛ اینکار از نصب یک سیستم به خصوص همراه با برنامه‌های بسته‌بندی نشده جلوگیری می‌کند. برای رفع این نیازهای خاص نیاز به سفارشی‌سازی نصب کننده است... خوشبختانه، نصب کننده بسیار ماژولار بوده و ابزارهایی برای خودکارسازی این فرآیند سفارشی‌سازی وجود دارند، به خصوص simple-CDD که مخفف <emphasis>Custom Debian Derivative</emphasis> است. این ابزار، با این حال تنها بخش اولیه فرآیند نصب را مدیریت می‌کند؛ این مشکل بزرگی نخواهد بود چرا که ابزار APT امکان گسترش بهینه بروزرسانی‌ها را فراهم می‌کنند."

#, fuzzy
#| msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian), in order to focus more intently on debian-installer and simple-CDD, which are more interesting in a Debian-only context."
msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian, but available as a third-party package), in order to focus more intently on debian-installer and <emphasis role=\"pkg\">simple-cdd</emphasis>, which are more interesting in a Debian-only context."
msgstr "به منظور تمرکز روی debian-installer و simple-CDD، تنها به بررسی کوتاه FAI می‌پردازیم و ابزار SystemImager را نادیده می‌گیریم (که دیگر در دبیان وجود ندارد)، چرا که این ابزارها در محیط دبیان بسیار متداول هستند."

msgid "Fully Automatic Installer (FAI)"
msgstr "نصب‌کننده تمام خودکار (FAI)"

#, fuzzy
#| msgid "<primary>Fully Automatic Installer (FAI)</primary>"
msgid "<primary>Fully Automatic Installer</primary><see>FAI</see>"
msgstr "<primary>نصب‌کننده تمام خودکار (FAI)</primary>"

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>FAI</primary>"
msgstr "<primary>RAID</primary>"

msgid "<foreignphrase>Fully Automatic Installer</foreignphrase> is probably the oldest automated deployment system for Debian, which explains its status as a reference; but its very flexible nature only just compensates for the complexity it involves."
msgstr "<foreignphrase>نصب‌کننده تمام خودکار</foreignphrase> احتمالا قدیمی‌ترین سیستم گسترش خودکار برای دبیان باشد، که جایگاه آن به عنوان یک مرجع را مشخص می‌کند؛ اما طبیعت بسیار سازگار آن به نوعی پیچیدگی‌های درونی‌اش را جبران می‌کند."

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary>FAI</primary><secondary><emphasis role=\"pkg\">fai-server</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary>FAI</primary><secondary><emphasis role=\"pkg\">fai-quickstart</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

msgid "FAI requires a server system to store deployment information and allow target machines to boot from the network. This server requires the <emphasis role=\"pkg\">fai-server</emphasis> package (or <emphasis role=\"pkg\">fai-quickstart</emphasis>, which also brings the required elements for a standard configuration)."
msgstr "FAI نیازمند یک سیستم سرور به منظور نگهداری از اطلاعات راه‌اندازی برای ماشین‌های است که قصد دارند از طریق شبکه به آن متصل گردند. این سرور نیازمند بسته <emphasis role=\"pkg\">fai-server</emphasis> (یا <emphasis role=\"pkg\">fai-quickstart</emphasis>، که عناصر مورد نیاز برای یک پیکربندی استاندارد را گردآوری می‌کند) است."

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/fai/</filename></secondary><see>FAI</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><filename>/etc/fai/nfsroot.conf</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> according to <filename>/etc/fai/nfsroot.conf</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgstr "FAI از یک رویکرد مشخص برای تعریف پروفایل‌های قابل نصب استفاده می‌کند. بجای رونوشت‌گیری ساده از یک مرجع قابل نصب، FAI یک نصب‌کننده تمام عیار است، که با استفاده از مجموعه فایل‌ها و اسکریپت‌های ذخیره‌شده در سرور قابل پیکربندی می‌باشد؛ مکان پیشفرض <filename>/srv/fai/config/</filename> به صورت خودکار ایجاد نمی‌شود، پس مدیر سیستم در کنار سایر فایل‌ها باید آن را ایجاد کند. در اکثر موارد، این فایل‌ها توسط نمونه‌هایی که در بسته مستندات <emphasis role=\"pkg\">fai-doc</emphasis> وجود دارد سفارشی‌سازی می‌شوند، به خصوص دایرکتوری <filename>/usr/share/doc/fai-doc/examples/simple/</filename>."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai-setup</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai-cd</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgstr "زمانی که پروفایل‌ها تعریف شوند، دستور <command>fai-setup</command> عناصر مورد نیاز نصب‌کننده FAI را تولید می‌کند؛ اینکار اغلب به معنی آماده‌سازی یا بروزرسانی یک سیستم حداقلی (NFS-root) در حین فرآیند نصب است. گزینه جایگرین آن ایجاد یک CD قابل اجرا با استفاده از <command>fai-cd</command> است."

msgid "Creating all these configuration files requires some understanding of the way FAI works. A typical installation process is made of the following steps:"
msgstr "ایجاد تمام این فایل‌های پیکربندی نیازمند درک درستی از چگونگی عملکرد FAI می‌باشد. یک فرآیند متداول نصب از گام‌های زیر تشکیل شده است:"

msgid "fetching a kernel from the network, and booting it;"
msgstr "دریافت یک کرنل از شبکه و راه‌اندازی آن؛"

msgid "mounting the root filesystem from NFS;"
msgstr "اتصال فایل سیستم root از NFS؛"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>FAI</primary><secondary><command>fai</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "executing <command>/usr/sbin/fai</command>, which controls the rest of the process (the next steps are therefore initiated by this script);"
msgstr "اجرای <command>/usr/sbin/fai</command>، که باقی فرآیند نصب را کنترل می‌کند (از این رو گام‌های بعدی توسط این اسکریپت فراخوانی می‌شوند)؛"

msgid "copying the configuration space from the server into <filename>/fai/</filename>;"
msgstr "رونوشت‌گیری فضای پیکربندی از سرور درون <filename>/fai/</filename>؛"

msgid "running <command>fai-class</command>. The <filename>/fai/class/[0-9][0-9]*</filename> scripts are executed in turn, and return names of “classes” that apply to the machine being installed; this information will serve as a base for the following steps. This allows for some flexibility in defining the services to be installed and configured."
msgstr "اجرای <command>fai-class</command>. اسکریپت‌های <filename>/fai/class/[0-9][0-9]*</filename> به ترتیب اجرا می‌شوند و نام “کلاس‌های” منطبق با ماشین مورد نظر می‌باشند را بر می‌گردانند؛ از این اطلاعات برای ادامه فرآیند نصب استفاده می‌شود. اینکار موجب انعطاف‌پذیری در تعریف سرویس‌های مورد نیاز برای نصب و پیکربندی می‌شود."

msgid "fetching a number of configuration variables, depending on the relevant classes;"
msgstr "دریافت تعدادی از متغیرهای پیکربندی، با توجه به کلاس‌های مربوطه؛"

msgid "partitioning the disks and formatting the partitions, based on information provided in <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;"
msgstr "پارتیشن‌بندی دیسک‌ها و فرمت کردن پارتیشن‌ها، بر اساس اطلاعات فراهم شده در <filename>/fai/disk_config/<replaceable>class</replaceable></filename>؛"

msgid "mounting said partitions;"
msgstr "اتصال پارتیشن‌های مذکور؛"

msgid "installing the base system;"
msgstr "نصب سیستم پایه؛"

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary>FAI</primary><secondary><command>fai-debconf</command></secondary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "preseeding the Debconf database with <command>fai-debconf</command>;"
msgstr "گردآوری پایگاه‌داده Debconf با استفاده از <command>fai-debconf</command>؛"

msgid "fetching the list of available packages for APT;"
msgstr "دریافت فهرست بروزرسانی‌های موجود برای APT؛"

msgid "installing the packages listed in <filename>/fai/package_config/<replaceable>class</replaceable></filename>;"
msgstr "نصب بسته‌های فهرست شده در <filename>/fai/package_config/<replaceable>class</replaceable></filename>؛"

msgid "executing the post-configuration scripts, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;"
msgstr "اجرای اسکریپت‌های پس از پیکربندی، <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>؛"

msgid "recording the installation logs, unmounting the partitions, and rebooting."
msgstr "ثبت گزارش‌های نصب، قطع اتصال پارتیشن‌ها و راه‌اندازی مجدد."

msgid "Preseeding Debian-Installer"
msgstr "گردآوری debian-installer"

msgid "<primary>preseed</primary>"
msgstr "<primary>preseed</primary>"

msgid "<primary>preconfiguration</primary>"
msgstr "<primary>preconfiguration</primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>installation</primary><secondary>preseeding</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <emphasis>preseeding</emphasis>."
msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <foreignphrase>preseeding</foreignphrase>."
msgstr "در انتها، بهترین ابزار برای نصب سیستم‌های دبیان به طور منطقی باید نصب‌کننده رسمی دبیان باشد. به همین دلیل است، که از ابتدای آن، debian-installer برای کاربرد خودکاری سازی طراحی شده است که از زیرساخت فراهم شده توسط <emphasis role=\"pkg\">debconf</emphasis> استفاده می‌کند. گزینه دوم، از یک طرف امکان کاهش تعداد پرسش‌های مطرح شده را فراهم می‌کند (پرسش‌های پنهان با پاسخ‌های پیشفرض جواب داده می‌شوند)، از طرف دیگر پاسخ‌های پیشفرض به صورت جداگانه ارائه می‌شوند، به این منظور که فرآیند نصب به صورت غیر-تعاملی انجام شود. این ویژگی آخر به نام <emphasis>preseeding</emphasis> شناخته می‌شود."

msgid "<emphasis>GOING FURTHER</emphasis> Debconf with a centralized database"
msgstr "<emphasis>مطالعه بیشتر</emphasis> Debconf همراه با یک پایگاه‌داده مرکزی"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">debconf-doc</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Preseeding allows to provide a set of answers to Debconf questions at installation time, but these answers are static and do not evolve as time passes. Since already-installed machines may need upgrading, and new answers may become required, the <filename>/etc/debconf.conf</filename> configuration file can be set up so that Debconf uses external data sources (such as an LDAP directory server, or a remote file accessed via NFS or Samba). Several external data sources can be defined at the same time, and they complement one another. The local database is still used (for read-write access), but the remote databases are usually restricted to reading. The <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual page describes all the possibilities in detail (you need the <emphasis role=\"pkg\">debconf-doc</emphasis> package)."
msgstr "Preseeding امکان فراهم کردن مجموعه‌ای از پاسخ‌ها به پرسش‌های Debconf در زمان نصب را می‌دهد، اما این پاسخ‌ها ایستا بوده و در گذر زمان تغییر نمی‌کنند. از آنجا که ماشین‌های نصب-شده نیازمند بروزرسانی هستند و ممکن است به پاسخ‌های جدید نیاز باشد، فایل پیکربندی <filename>/etc/debconf.conf</filename> می‌تواند به منظور استفاده Debconf از منابع خارجی داده (مانند یک دایرکتوری سرور LDAP یا فایلی که از طریق NFS یا Samba مورد نیاز باشد)، پیکربندی شود. منابع داده خارجی متفاوتی می‌توانند در یک زمان تعریف شوند تا یکدیگر را کامل کنند. پایگاه‌داده محلی به منظور دسترسی خواندنی-نوشتنی و پایگاه‌داده‌های راه دور به منظور دسترسی فقط-خواندنی محدود می‌شوند. صفحه راهنمای <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> به تشریح تمام احتمالات موجود در این زمینه می‌پردازد (به بسته <emphasis role=\"pkg\">debconf-doc</emphasis> نیاز دارید)."

msgid "<primary><command>debconf</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "Using a Preseed File"
msgstr "استفاده از یک فایل Preseed"

msgid "There are several places where the installer can get a preseeding file:"
msgstr "مکان‌های مختلفی وجود دارد که یک نصب‌کننده می‌تواند فایل preseed را دریافت کند:"

#, fuzzy
#| msgid "<primary>preseed</primary>"
msgid "<primary><filename>preseed.cfg</filename></primary>"
msgstr "<primary>preseed</primary>"

msgid "in the initrd used to start the machine; in this case, preseeding happens at the very beginning of the installation, and all questions can be avoided. The file just needs to be called <filename>preseed.cfg</filename> and stored in the initrd root."
msgstr "در initrd که برای راه‌اندازی ماشین استفاده شده است؛ در این مورد، عملیات preseed در ابتدای فرآیند نصب صورت می‌گیرد و تمام پرسش‌ها می‌توانند نادیده گرفته شوند. این فایل باید بنام <filename>preseed.cfg</filename> در اولین سطح دایرکتوری initrd قرار گیرد."

msgid "on the boot media (CD or USB key); preseeding then happens as soon as the media is mounted, which means right after the questions about language and keyboard layout. The <literal>preseed/file</literal> boot parameter can be used to indicate the location of the preseeding file (for instance, <filename>/cdrom/preseed.cfg</filename> when the installation is done off a CD-ROM, or <filename>/hd-media/preseed.cfg</filename> in the USB-key case)."
msgstr "در رسانه راه‌اندازی (مانند CD یا USB)؛ عملیات preseed به محض اتصال رسانه صورت می‌گیرد، یعنی درست پس از پرسش‌های مربوط به زبان و ساختار صفحه کلید. پارامتر راه‌اندازی <literal>preseed/file</literal> می‌تواند برای شناسایی مکان فایل preseed (برای نمونه، <filename>/cdrom/preseed.cfg</filename> در هنگام نصب از CD-ROM یا <filename>/hd-media/preseed.cfg</filename> در هنگام نصب از USB) مورد استفاده قرار گیرد."

#, fuzzy
#| msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."
msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal> (HTTPS, FTPS, SFTP, etc. are not supported)."
msgstr "از طریق شبکه؛ عملیات preseed پس از پیکربندی (خودکار) شبکه صورت می‌گیرد؛ پارامتر راه‌اندازی مرتبط با آن عبارت است از <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."

msgid "At a glance, including the preseeding file in the initrd looks like the most interesting solution; however, it is rarely used in practice, because generating an installer initrd is rather complex. The other two solutions are much more common, especially since boot parameters provide another way to preseed the answers to the first questions of the installation process. The usual way to save the bother of typing these boot parameters by hand at each installation is to save them into the configuration for <command>isolinux</command> (in the CD-ROM case) or <command>syslinux</command> (USB key)."
msgstr "در یک نگاه، قرار دادن فایل preseed درون initrd ممکن است جالب‌ترین گزینه به نظر آید؛ اگرچه، کمتر از این حالت استفاده می‌شود چرا که ایجاد یک initrd قابل نصب بسیار دشوار است. دو راهکار جایگزین دیگر، بیشتر متداول هستند، به خصوص که پارامترهای راه‌اندازی روشی دیگر برای آماده‌سازی پاسخ‌ها به اولین پرسش‌های فرآیند نصب را فراهم می‌کنند. روش مرسوم برای ذخیره‌سازی این پارامترهای راه‌اندازی و جلوگیری از نوشتن هر کدام در زمان نصب، قرار دادن آن‌ها در پیکربندی مرتبط با <command>isolinux</command> (در مورد CD-ROM) یا <command>syslinux</command> (در مورد USB) است."

msgid "Creating a Preseed File"
msgstr "ایجاد یک فایل Preseed"

msgid "A preseed file is a plain text file, where each line contains the answer to one Debconf question. A line is split across four fields separated by whitespace (spaces or tabs), as in, for instance, <literal>d-i mirror/suite string stable</literal>:"
msgstr "یک فایل preseedاز نوع متنی است که در هر خط آن پاسخ به یک پرسش از Debconf قرار دارد. هر خط به چهار فیلد که با فاصله (space یا tab) از یکدیگر جدا می‌شوند، تقسیم شده است. برای نمونه، در مورد <literal>d-i mirror/suite string stable</literal>:"

msgid "the first field is the “owner” of the question; “d-i” is used for questions relevant to the installer, but it can also be a package name for questions coming from Debian packages;"
msgstr "فیلد اول “مالک” پرسش به حساب می‌آید؛ “d-i” برای پرسش‌هایی استفاده می‌شود که مرتبط با فرآیند نصب هستند، اما می‌تواند در مورد  نام بسته‌های موجود دبیان نیز بکار رود؛"

#, fuzzy
#| msgid "the second field is an identifier for the question;"
msgid "the second field is an identifier for the question (the template name);"
msgstr "فیلد دوم یک شناسه برای پرسش به حساب می‌آید؛"

msgid "third, the type of question;"
msgstr "فیلد سوم نوع پرسش را مشخص می‌کند؛"

msgid "the fourth and last field contains the value for the answer. Note that it must be separated from the third field with a single space; if there are more than one, the following space characters are considered part of the value."
msgstr "فیلد چهارم و آخرین فیلد نیز پاسخ به پرسش را شامل می‌شود. به یاد داشته باشید که از فیلد سوم توسط یک space باید جدا شود؛ اگر بیش از یک فاصله بکار رود، به عنوان بخشی از پاسخ در نظر گرفته می‌شود."

msgid "The simplest way to write a preseed file is to install a system by hand. Then <command>debconf-get-selections --installer</command> will provide the answers concerning the installer. Answers about other packages can be obtained with <command>debconf-get-selections</command>. However, a cleaner solution is to write the preseed file by hand, starting from an example and the reference documentation: with such an approach, only questions where the default answer needs to be overridden can be preseeded; using the <literal>priority=critical</literal> boot parameter will instruct Debconf to only ask critical questions, and use the default answer for others."
msgstr "ساده‌ترین روش برای ایجاد یک فایل preseed نصب یک سیستم به صورت دستی است. سپس <command>debconf-get-selections --installer</command> پاسخ‌های مرتبط با آن را فراهم می‌کند. پاسخ‌های مرتبط با سایر بسته‌ها نیز توسط <command>debconf-get-selections</command> گردآوری می‌شوند. اگرچه، راهکار بهتر در این مورد نوشتن فایل preseed به صورت دستی است، که از یک فایل نمونه و مستندات مرجع می‌توان استفاده کرد: با چنین رویکردی، تنها پرسش‌هایی که پاسخ‌های پیشفرض داشته باشند می‌توانند آماده‌سازی شوند؛ استفاده از پارامتر راه‌اندازی <literal>priority=critical</literal> به Debconf دستور می‌دهد که تنها به پرسش‌های حیاتی پاسخ دهد و از پاسخ‌های پیشفرض برای سایر پرسش‌ها استفاده کند."

msgid "Pre-setting a value in a preseed file automatically instructs the Debian installer to not ask that question. This happens, because loading the preseed file does not just set the given value(s), but also marks each of the affected dialogs as “seen“ by the user. Thus it is possible to pre-set a question's value and still present the dialog to the user by resetting the “seen“ flag. Beware that order in this case matters and that the value has to be preseeded before setting the dialog to “unseen“ as shown in the following example:"
msgstr ""

msgid ""
"d-i netcfg/hostname string worker\n"
"d-i netcfg/hostname seen false"
msgstr ""

#, fuzzy
#| msgid "<primary><command>debconf</command></primary>"
msgid "<primary><command>debconf-get-selections</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

msgid "<emphasis>DOCUMENTATION</emphasis> Installation guide appendix"
msgstr "<emphasis>مستندات</emphasis> ضمیمه راهنمای نصب"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>preseed</primary><secondary>all templates</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"
msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. There are also collections of all debconf templates extracted from each component and suite of Debian: <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/apb\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/example-preseed.txt\" /> <ulink type=\"block\" url=\"https://preseed.debian.net/\" />"
msgstr "راهنمای نصب، که به صورت آنلاین موجود است، شامل مستندات کامل درباره استفاده از یک فایل preseed در یک ضمیمه جداگانه است. همچنین شامل یک فایل نمونه همراه با توضیحات می‌باشد، که می‌تواند به عنوان پایه‌ای برای سفارشی‌سازی‌های محلی استفاده شود. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"

msgid "Preseeding an installation is often not as straightforward as one would wish. It sometimes requires to understand how packages process the given values in their scripts. Don't hesitate to ask on the <email>debian-cd@lists.debian.org</email> mailing list or in the <literal>#debian-cd</literal> IRC channel if you require help. Also be aware that some complex setups still cannot be achieved by preseeding."
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>mailing lists</primary><secondary><email>debian-cd@lists.debian.org</email></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Creating a Customized Boot Media"
msgstr "ایجاد یک رسانه راه‌اندازی سفارشی‌"

msgid "Knowing where to store the preseed file is all very well, but the location isn't everything: one must, one way or another, alter the installation boot media to change the boot parameters and add the preseed file."
msgstr "دانستن اینکه یک فایل preseed در کجا ذخیره شود خوب است، اما کافی نیست: مدیر سیستم باید به شیوه‌ای رسانه‌ راه‌اندازی نصب را تغییر دهد که پارامترهای راه‌اندازی تغییر کرده و فایل preseed به آن اضافه شود."

msgid "Booting From the Network"
msgstr "راه‌اندازی از طریق شبکه"

#, fuzzy
#| msgid "<primary>LXC</primary>"
msgid "<primary>PXE</primary>"
msgstr "<primary>LXC</primary>"

#, fuzzy
#| msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"
msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/stable/amd64/ch04s05\" />"
msgstr "زمانی که یک رایانه از طریق شبکه راه‌اندازی می‌شود، سروری که عناصر راه‌اندازی را ارسال می‌کند همچنین شامل پارامترهای راه‌اندازی نیز می‌باشد. بنابراین، تغییرات مورد نظر باید در پیکربندی PXE سرور راه‌اندازی اعمال شوند؛ به طور خاص، در فایل پیکربندی <filename>/tftpboot/pxelinux.cfg/default</filename>. برپایی راه‌اندازی شبکه برای اینکار یک پیشنیاز به حساب می‌آید: برای جزئیات بیشتر راهنمای نصب را مشاهده کنید. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"

msgid "Preparing a Bootable USB Key"
msgstr "آماده‌سازی یک USB قابل اجرا"

#, fuzzy
#| msgid "<primary><emphasis>VirtualBox</emphasis></primary>"
msgid "<primary><filename>syslinux.cfg</filename></primary>"
msgstr "<primary><emphasis>VirtualBox</emphasis></primary>"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>syslinux</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>isolinux</primary>"
msgstr "<primary>Munin</primary>"

#, fuzzy
#| msgid "<primary>Icinga</primary>"
msgid "<primary><filename>grub.cfg</filename></primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>:"
msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>, copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>."
msgstr "زمانی که یک حافظه قابل راه‌اندازی آماده‌سازی شد (<xref linkend=\"sect.install-usb\" /> را مشاهده کنید)، برخی عملیات اضافی مورد نیاز است. فرض می‌کنیم که محتوای حافظه در <filename>/media/usbdisk/</filename> قرار دارد:"

msgid "If you have been using a hybrid ISO image to create the bootable USB stick, then you have to edit <filename>/media/usbdisk/boot/grub/grub.cfg</filename> (for the EFI boot screen):"
msgstr ""

#, fuzzy
#| msgid "syslinux.cfg file and preseeding parameters"
msgid "boot/grub/grub.cfg file and preseeding parameters"
msgstr "فایل syslinux.cfg و پارامترهای عملیات آماده‌سازی"

#, fuzzy
#| msgid ""
#| "default vmlinuz\n"
#| "append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgid ""
"menuentry --hotkey=i 'Install' {\n"
"    set background_color=black\n"
"    linux    /install.amd/vmlinuz preseed/file=/cdrom/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 --- quiet \n"
"    initrd   /install.amd/initrd.gz\n"
"}"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

msgid "And you have to edit <filename>/media/usbdisk/isolinux/isolinux.cfg</filename> (for BIOS boot) or one of the files it utilizes - e.g. <filename>/media/usbdisk/isolinux/txt.cfg</filename> - to add required boot parameters:"
msgstr ""

#, fuzzy
#| msgid "syslinux.cfg file and preseeding parameters"
msgid "isolinux/txt.cfg file and preseeding parameters"
msgstr "فایل syslinux.cfg و پارامترهای عملیات آماده‌سازی"

#, fuzzy
#| msgid ""
#| "default vmlinuz\n"
#| "append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgid ""
"label install\n"
"        menu label ^Install\n"
"        kernel [...]\n"
"        append preseed/file=/cdrom/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=/install.amd/initrd.gz --- quiet"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

#, fuzzy
#| msgid "edit <filename>/media/usbdisk/syslinux.cfg</filename> and add required boot parameters (see example below)."
msgid "If you have been using the <filename>hd-media</filename> installer image for a custom USB stick, edit <filename>/media/usbdisk/syslinux.cfg</filename> and add the required boot parameters as shown in the example below:"
msgstr "ویرایش <filename>/media/usbdisk/syslinux.cfg</filename> و افزودن پارامترهای راه‌اندازی مورد نیاز (مثال زیر را مشاهده کنید)."

msgid "syslinux.cfg file and preseeding parameters"
msgstr "فایل syslinux.cfg و پارامترهای عملیات آماده‌سازی"

msgid ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

msgid "Creating a CD-ROM Image"
msgstr "ایجاد یک تصویر CD-ROM"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">debian-cd</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>genisoimage</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>mkisofs</command></primary>"
msgstr "<primary><command>xm</command></primary>"

#, fuzzy
#| msgid "<primary><command>xm</command></primary>"
msgid "<primary><command>xorriso</command></primary>"
msgstr "<primary><command>xm</command></primary>"

msgid "A USB key is a read-write media, so it was easy for us to add a file there and change a few parameters. In the CD-ROM case, the operation is more complex, since we need to regenerate a full ISO image. This task is handled by <emphasis role=\"pkg\">debian-cd</emphasis>, but this tool is rather awkward to use: it needs a local mirror, and it requires an understanding of all the options provided by <filename>/usr/share/debian-cd/CONF.sh</filename>; even then, <command>make</command> must be invoked several times. <filename>/usr/share/debian-cd/README</filename> is therefore a very recommended read."
msgstr "یک حافظه USB از نوع رسانه‌های خواندنی-نوشتنی است، پس تغییر فایل در آن و افزودن پارامترها کار آسانی است. در مورد CD-ROM، این عملیات از پیچیدگی بیشتری برخوردار است چرا که نیازمند تولید مجدد تصویر ISO از آن می‌باشد. این وظیفه توسط <emphasis role=\"pkg\">debian-cd</emphasis> مدیریت می‌شود، اما استفاده از این ابزار برای اینکار مطلوب نیست: نیازمند یک mirror محلی و درک از گزینه‌های موجود در <filename>/usr/share/debian-cd/CONF.sh</filename> است؛ حتی در این صورت، <command>make</command> چندین بار باید فراخوانی شود. بنابراین مطالعه <filename>/usr/share/debian-cd/README</filename> به شدت توصیه می‌گردد."

#, fuzzy
#| msgid "Having said that, debian-cd always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific file is <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgid "Having said that, <emphasis role=\"pkg\">debian-cd</emphasis> always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific files are <filename>$TDIR/$CODENAME/CD1/isolinux/isolinux.cfg</filename> and <filename>$TDIR/$CODENAME/CD1/boot/grub/grub.cfg</filename> as shown above). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgstr "با این تفاسیر، debian-cd به شیوه مشابهی عمل می‌کند: یک دایرکتوری “image” همراه با محتوای دقیق از CD-ROM تولید، سپس با استفاده از ابزاری مانند <command>genisoimage</command>، <command>mkisofs</command> یا <command>xorriso</command> به فایل ISO تبدیل می‌شود. دایرکتوری image پس از گام <command>make image-trees</command> در debian-cd نهایی می‌گردد. در این نقطه، فایل preseed را درون دایرکتوری متناسب آن قرار می‌دهیم (معمولا <filename>$TDIR/$CODENAME/CD1/</filename>، که پارامترهای $TDIR و $CODENAME توسط فایل پیکربندی <filename>CONF.sh</filename> تعریف می‌شوند). CD-ROM از <command>isolinux</command> به عنوان راه‌انداز خود استفاده می‌کند که فایل پیکربندی آن باید توسط آنچه که debian-cd تولید کرده است سازگار باشد، تا بتوان پارامترهای راه‌اندازی مورد نیاز را وارد کرد (فایل مشخص آن عبارت است از <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). در انتها فرآیند “عادی” می‌تواند ادامه یابد و می‌توان تصویر ISO را با استفاده از <command>make image CD=1</command> (یا <command>make images</command> در صورت نیاز به چندین CD-ROM) ایجاد کنیم."

msgid "Simple-CDD: The All-In-One Solution"
msgstr "Simple-CDD: یک راهکار جامع"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">simple-cdd</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

msgid "Simply using a preseed file is not enough to fulfill all the requirements that may appear for large deployments. Even though it is possible to execute a few scripts at the end of the normal installation process, the selection of the set of packages to install is still not quite flexible (basically, only “tasks” can be selected); more important, this only allows installing official Debian packages, and precludes locally-generated ones."
msgstr "استفاده از یک فایل preseed به تنهایی تمام پیشنیازهای فرآیندهای بزرگ توسعه را محقق نمی‌کند. با اینکه امکان اجرای چند اسکریپت در انتهای فرآیند نصب وجود دارد، مجموعه بسته‌هایی که باید نصب گردند به سادگی قابل دسترس نمی‌باشند (معمولا، فقط “task” می‌تواند انتخاب شود)؛ مهمتر اینکه، این روش منجر به نصب بسته‌های رسمی از دبیان می‌شود و سایر بسته‌های محلی را نادیده می‌گیرد."

#, fuzzy
#| msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what Simple-CDD (in the <emphasis role=\"pkg\">simple-cdd</emphasis> package) does."
msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what <emphasis role=\"pkg\">simple-cdd</emphasis> does."
msgstr "از طرف دیگر، debian-cd می‌تواند بسته‌های خارجی را یکپارچه‌سازی کند و debian-installer می‌تواند با درج گام‌های جدید در فرآیند نصب توسعه یابد. با ترکیب این قابلیت‌ها باید بتوانیم یک نصب‌کننده سفارشی را برای نیازهای خود ایجاد کنیم؛ همچنین باید قادر باشد برخی سرویس‌ها را پس از نصب بسته‌های آن‌ها پیکربندی کند. خوشبختانه، این یک حالت فرضی نیست، زیرا دقیقا کاری است که Simple-CDD (در بسته <emphasis role=\"pkg\">simple-cdd</emphasis>) انجام می‌دهد."

#, fuzzy
#| msgid "The purpose of Simple-CDD is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgid "The purpose of this tool is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgstr "هدف Simple-CDD این است که هر فردی بتواند یک توزیع مشتق شده از دبیان را با انتخاب مجموعه‌ای از بسته‌های موجود، پیکربندی آن‌ها با Debconf، افزودن نرم‌افزار خاص و اجرای اسکریپت‌های سفارشی در انتهای فرآیند نصب، ایجاد کند. این رویکرد با فلسفه “سیستم عامل جهانی” (شعار دبیان) سازگاری دارد، چرا که هر فردی می‌تواند آن را با نیاز خود سازگار سازد."

msgid "Creating Profiles"
msgstr "ایجاد پروفایل‌ها"

msgid "Simple-CDD defines “profiles” that match the FAI “classes” concept, and a machine can have several profiles (determined at installation time). A profile is defined by a set of <filename>profiles/<replaceable>profile</replaceable>.*</filename> files:"
msgstr "Simple-CDD “پروفایل” را تعریف می‌کند که با مفهوم “کلاس” در FAI سازگار هستند و یک ماشین می‌تواند چندین پروفایل داشته باشد (که در زمان نصب مشخص می‌شوند). یک پروفایل توسط مجموعه فایل‌های <filename>profiles/<replaceable>profile</replaceable>.*</filename> تعریف می‌شود:"

msgid "the <filename>.description</filename> file contains a one-line description for the profile;"
msgstr "فایل <filename>.description</filename> شامل توضیح یک خطی درباره پروفایل است؛"

msgid "the <filename>.packages</filename> file lists packages that will automatically be installed if the profile is selected;"
msgstr "فایل <filename>.packages</filename> شامل بسته‌هایی است که در صورت انتخاب شدن پروفایل به شیوه خودکار نصب می‌گردند؛"

msgid "the <filename>.downloads</filename> file lists packages that will be stored onto the installation media, but not necessarily installed;"
msgstr "فایل <filename>.downloads</filename> شامل بسته‌هایی است که درون رسانه نصب ذخیره‌سازی می‌شوند، اما الزامی در نصب آن‌ها وجود ندارد؛"

msgid "the <filename>.preseed</filename> file contains preseeding information for Debconf questions (for the installer and/or for packages);"
msgstr "فایل <filename>.preseed</filename> شامل اطلاعات آماده‌سازی برای پرسش‌های Debconf (برای نصب‌کننده و/یا بسته‌ها) می‌باشد؛"

msgid "the <filename>.postinst</filename> file contains a script that will be run at the end of the installation process;"
msgstr "فایل <filename>.postinst</filename> شامل اسکریپتی است که در انتهای فرآیند نصب اجرا می‌شود؛"

#, fuzzy
#| msgid "lastly, the <filename>.conf</filename> file allows changing some Simple-CDD parameters based on the profiles to be included in an image."
msgid "lastly, the <filename>.conf</filename> file allows changing some parameters based on the profiles to be included in an image."
msgstr "در انتها، فایل <filename>.conf</filename> امکان تغییر برخی پارامترهای Simple-CDD را بر اساس پروفایل‌های موجود در آن فراهم می‌کند."

msgid "The <literal>default</literal> profile has a particular role, since it is always selected; it contains the bare minimum required for Simple-CDD to work. The only thing that is usually customized in this profile is the <literal>simple-cdd/profiles</literal> preseed parameter: this allows avoiding the question, introduced by Simple-CDD, about what profiles to install."
msgstr "پروفایل <literal>default</literal> نقش ویژه‌ای دارد چرا که همیشه انتخاب می‌شود؛ شامل حداقل‌های مورد نیاز توسط Simple-CDD است. تنها موردی که در این پروفایل سفارشی می‌شود پارامتر آماده‌سازی <literal>simple-cdd/profiles</literal> است: امکان رد کردن پرسش، که توسط Simple-CDD، درباره پروفایل قابل نصب را فراهم می‌کند."

msgid "Note also that the commands will need to be invoked from the parent directory of the <filename>profiles</filename> directory."
msgstr "به یاد داشته باشید که دستورات باید از دایرکتوی والد <filename>profiles</filename> فراخوانی شوند."

msgid "Configuring and Using <command>build-simple-cdd</command>"
msgstr "پیکربندی و استفاده از <command>build-simple-cdd</command>"

msgid "<primary><command>build-simple-cdd</command></primary>"
msgstr "<primary><command>build-simple-cdd</command></primary>"

msgid "<emphasis>QUICK LOOK</emphasis> Detailed configuration file"
msgstr "<emphasis>نگاه سریع</emphasis> فایل پیکربندی همراه با جزئیات"

#, fuzzy
#| msgid "An example of a Simple-CDD configuration file, with all possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). This can be used as a starting point when creating a custom configuration file."
msgid "An example of a Simple-CDD configuration file, with most possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed</filename>). This can be used as a starting point when creating a custom configuration file. Unfortunately not everything is documented there, so some variables are only listed and explained in <filename>/usr/lib/python3/dist-packages/simple_cdd/variables.py</filename>."
msgstr "یک نمونه از فایل پیکربندی Simple-CDD، همراه با تمام پارامترهای ممکن، درون <filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename> قرار دارد. از این فایل می‌توان به عنوان نقطه آغاز برای ایجاد فایل پیکربندی سفارشی استفاده کرد."

msgid "It is further important to familiarize yourself with the variables understood by <filename>/usr/share/debian-cd/CONF.sh</filename>."
msgstr ""

msgid "Simple-CDD requires many parameters to operate fully. They will most often be gathered in a configuration file, which <command>build-simple-cdd</command> can be pointed at with the <literal>--conf</literal> option, but they can also be specified via dedicated parameters given to <command>build-simple-cdd</command>. Here is an overview of how this command behaves, and how its parameters are used:"
msgstr "Simple-CDD به پارامترهای بسیاری برای عملکرد جامع نیاز دارد. آن‌ها اغلب درون یک فایل پیکربندی قرار دارند، که <command>build-simple-cdd</command> می‌تواند با گزینه <literal>--conf</literal> به آن اشاره کند، همچنین می‌توانند با استفاده از پارامترهای انحصاری به <command>build-simple-cdd</command> ارجاع شوند. در اینجا به این دستور و پارامترهای مورد نیاز آن نگاهی می‌اندازیم:"

msgid "the <literal>profiles</literal> parameter lists the profiles that will be included on the generated CD-ROM image;"
msgstr "پارامتر <literal>profiles</literal> فهرستی از پروفایل‌های قابل اجرا در CD-ROM ایجاد شده را فهرست می‌کند؛"

msgid "based on the list of required packages, Simple-CDD downloads the appropriate files from the server mentioned in <literal>server</literal>, and gathers them into a partial mirror (which will later be given to debian-cd);"
msgstr "بر اساس فهرست بسته‌های مورد نیاز، Simple-CDD فایل‌های متناسب با آن‌ها را از سرور اشاره شده در <literal>server</literal> دانلود کرده و آن‌ها را درون یک mirror موقت قرار می‌دهد (که در ادامه به debian-cd داده می‌شود)؛"

msgid "the custom packages mentioned in <literal>local_packages</literal> are also integrated into this local mirror;"
msgstr "بسته‌های سفارشی موجود در <literal>local_packages</literal> نیز درون همین mirror محلی قرار می‌گیرند؛"

msgid "debian-cd is then executed (within a default location that can be configured with the <literal>debian_cd_dir</literal> variable), with the list of packages to integrate;"
msgstr "سپس debian-cd (درون یک مکان پیشفرض که می‌تواند با متغیر <literal>debian_cd_dir</literal> پیکربندی شود) همراه با فهرستی از بسته‌ها به منظور یکپارچه‌سازی اجرا می‌شود؛"

msgid "once debian-cd has prepared its directory, Simple-CDD applies some changes to this directory:"
msgstr "زمانی که debian-cd دایرکتوری خود را آماده کند، Simple-CDD برخی تغییرات را در این دایرکتوری انجام می‌دهد:"

msgid "files containing the profiles are added in a <filename>simple-cdd</filename> subdirectory (that will end up on the CD-ROM);"
msgstr "فایل‌هایی که شامل پروفایل‌ها هستند درون یک دایرکتوری زیر مجموعه <filename>simple-cdd</filename> (واقع در CD-ROM نهایی) قرار می‌گیرند؛"

msgid "other files listed in the <literal>all_extras</literal> parameter are also added;"
msgstr "سایر فایل‌های فهرست شده در پارامتر <literal>all_extras</literal> نیز افزوده می‌شوند؛"

msgid "the boot parameters are adjusted so as to enable the preseeding. Questions concerning language and country can be avoided if the required information is stored in the <literal>language</literal> and <literal>country</literal> variables."
msgstr "پارامترهای راه‌اندازی طوری تنظیم می‌شوند که عملیات preseed فعال گردد. در صورت ذخیره‌سازی اطلاعات لازم در متغیرهای <literal>language</literal> و <literal>country</literal>، پرسش‌های مربوط به زبان و کشور نادیده گرفته می‌شوند."

msgid "debian-cd then generates the final ISO image."
msgstr "سپس debian-cd فایل نهایی ISO را تولید می‌کند."

msgid "Generating an ISO Image"
msgstr "تولید یک فایل ISO"

#, fuzzy
#| msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-8.0-amd64-CD-1.iso</filename>."
msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-11-amd64-CD-1.iso</filename>."
msgstr "زمانی که یک فایل پیکربندی ایجاد و پروفایل‌های خود را تعریف کردیم، گام باقیمانده فراخوانی <command>build-simple-cdd --conf simple-cdd.conf</command> است. پس از چند دقیقه، فایل نهایی را در <filename>images/debian-8.0-amd64-CD-1.iso</filename> دریافت می‌کنیم."

#, fuzzy
#| msgid "<primary>Munin</primary>"
msgid "<primary>monitoring</primary>"
msgstr "<primary>Munin</primary>"

msgid "<primary>Munin</primary>"
msgstr "<primary>Munin</primary>"

msgid "<primary>Nagios</primary>"
msgstr "<primary>Nagios</primary>"

msgid "Monitoring is a generic term, and the various involved activities have several goals: on the one hand, following usage of the resources provided by a machine allows anticipating saturation and the subsequent required upgrades; on the other hand, alerting the administrator as soon as a service is unavailable or not working properly means that the problems that do happen can be fixed sooner."
msgstr "مانیتورینگ یک عبارت عمومی است و فعالیت‌های مرتبط با آن اهداف گوناگونی را دنبال می‌کنند: از یک طرف، پیگیری منابع مصرفی فراهم شده توسط ماشین امکان پیشبینی میزان اشباع و بروزرسانی‌های متعاقب با آن را فراهم می‌کند؛ از طرف دیگر، هشدار به مدیر سیستم به محض اینکه یک سرویس از دسترس خارج شود یا به درستی کار نکند به معنی رفع سریع‌تر مشکلات در زمان بروز حادثه است."

msgid "<emphasis>Munin</emphasis> covers the first area, by displaying graphical charts for historical values of a number of parameters (used RAM, occupied disk space, processor load, network traffic, Apache/MySQL load, and so on). <emphasis>Nagios</emphasis> covers the second area, by regularly checking that the services are working and available, and sending alerts through the appropriate channels (e-mails, text messages, and so on). Both have a modular design, which makes it easy to create new plug-ins to monitor specific parameters or services."
msgstr "<emphasis>Munin</emphasis> با نمایش نمودارهای گرافیکی برای مقادیر مختلف از پارامترهای متعدد (حافظه مصرفی، فضای اشغال شده دیسک، بار پردازنده، ترافیک شبکه، بار وب سرور/پایگاه‌داده و از این قبیل) ناحیه اول را پوشش می‌دهد. <emphasis>Nagios</emphasis> با بررسی مداوم سرویس‌ها و نحوه کارکرد و قابل دسترس بودن آن‌ها، همراه با ارسال پیام به مدیر سیستم با استفاده از کانال‌های مناسب (ایمیل، پیامک و از این قبیل) ناحیه دوم را پوشش می‌دهد. هر دو ابزار ساختاری ماژولار دارند که به توسعه هر یک از آن‌ها و افزودن پارامترها یا سرویس‌های خاص کمک می‌کند."

msgid "<emphasis>ALTERNATIVE</emphasis> Zabbix, an integrated monitoring tool"
msgstr "<emphasis>جایگزین</emphasis> Zabbix، یک ابزار مانیتورینگ یکپارچه"

msgid "<primary>Zabbix</primary>"
msgstr "<primary>Zabbix</primary>"

#, fuzzy
#| msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"http://www.zabbix.com/\" />"
msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"https://zabbix.com/\" />"
msgstr "با اینکه استفاده از Munin و Nagios بسیار متداول است، آن‌ها تنها بازیکنان موجود در میدان مانیتورینگ نیستند و هر کدام نیز تنها نصف وظایف (نمودار از یک طرف، هشدار از طرف دیگر) را انجام می‌دهند. Zabbix، از طرف دیگر هر دو بخش مانیتورینگ را یکپارچه می‌کند؛ همچنین شامل یک رابط وب به منظور پیکربندی جنبه‌های مختلف مانیتورینگ می‌باشد. طی چند سال گذشته فراز و نشیب‌های بسیاری را پشت سر گذاشته و هم اکنون به عنوان یک رقیب جدی به حساب می‌آید. در سرور مانیتورینگ، می‌توانید <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> یا <emphasis role=\"pkg\">zabbix-server-mysql</emphasis> را همراه با <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> به منظور دسترسی به رابط وب نصب کنید. در سیستم‌های میزبان که قصد مانیتور کردن آن‌ها را دارید می‌توانید <emphasis role=\"pkg\">zabbix-agent</emphasis> را نصب کنید که داده‌ها را به سمت سرور می‌فرستد. <ulink type=\"block\" url=\"http://www.zabbix.com/\" />"

msgid "<emphasis>ALTERNATIVE</emphasis> Icinga, a Nagios fork"
msgstr "<emphasis>جایگزین</emphasis> Icinga، یک fork از Nagios"

msgid "<primary>Icinga</primary>"
msgstr "<primary>Icinga</primary>"

#, fuzzy
#| msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"
msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"https://icinga.com/\" />"
msgstr "به موجب اختلاف نظر در مدل توسعه انتخابی برای Nagios (که توسط یک شرکت کنترل می‌شود)، تعدادی از توسعه‌دهندگان آن را fork و از نام جدید Icinga استفاده کردند. Icinga کماکان ـ تا جای ممکن - با پیکربندی‌ها و پلاگین‌های Nagios سازگار، اما ویژگی‌های اضافی را به آن افزوده است. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"

msgid "Setting Up Munin"
msgstr "راه‌اندازی Munin"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary>grapher</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The purpose of Munin is to monitor many machines; therefore, it quite naturally uses a client/server architecture. The central host — the grapher — collects data from all the monitored hosts, and generates historical graphs."
msgstr "هدف Munin مانیتور کردن ماشین‌های متعدد است؛ بنابراین، طبیعی است که از معماری کلاینت/سرور استفاده کند. میزبان مرکزی - یا grapher - داده را از تمام میزبان‌های قابل مانیتور کردن دریافت کرده و نمودارهای گرافیکی تولید می‌کند."

msgid "Configuring Hosts To Monitor"
msgstr "پیکربندی میزبان‌ها برای مانیتور شدن"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>server</primary><secondary>munin-node</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The first step is to install the <emphasis role=\"pkg\">munin-node</emphasis> package. The daemon installed by this package listens on port 4949 and sends back the data collected by all the active plugins. Each plugin is a simple program returning a description of the collected data as well as the latest measured value. Plugins are stored in <filename>/usr/share/munin/plugins/</filename>, but only those with a symbolic link in <filename>/etc/munin/plugins/</filename> are really used."
msgstr "اولین گام نصب بسته <emphasis role=\"pkg\">munin-node</emphasis> است. فرآیند پس‌زمینه‌ای که توسط این بسته نصب می‌شود به درگاه ۴۹۴۹ گوش کرده و داده‌های دریافتی از پلاگین‌های فعال را ارسال می‌کند. هر پلاگین یک برنامه ساده است که توضیح مرتبط با داده دریافتی همراه با آخرین مقدار بدست آمده را باز می‌گرداند. پلاگین‌ها در مسیر <filename>/usr/share/munin/plugins/</filename> ذخیره شده‌اند اما تنها آن‌هایی که به صورت پیوند نمادین در <filename>/etc/munin/plugins/</filename> قرار داشته باشند، استفاده می‌گردند."

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/munin/</filename></secondary><see>Munin</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/plugins/</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary>plugins</secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this auto-configuration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the Plugin Gallery can be interesting even though not all plugins have comprehensive documentation. <ulink type=\"block\" url=\"https://gallery.munin-monitoring.org\" />"
msgstr ""

#, fuzzy
#| msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the <ulink url=\"http://gallery.munin-monitoring.org\">Plugin Gallery</ulink> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgid "However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgstr "زمانی که بسته نصب شود، مجموعه‌ای از پلاگین‌های فعال بر اساس نرم‌افزار موجود و پیکربندی فعلی میزبان تشخیص داده می‌شوند. اگرچه، این پیکربندی خودکار وابسته به قابلیتی است که هر پلاگین باید فراهم کرده باشد و بهتر است که نتایج را به صورت دستی مرور و ویرایش کنیم. مرور <ulink url=\"http://gallery.munin-monitoring.org\">گالری پلاگین</ulink> می‌تواند جالب باشد با این وجود که همه پلاگین‌ها ممکن است شامل مستندات جامع نباشند. با این حال، تمام پلاگین‌ها اسکریپت هستند که اکثر آن‌ها ساده بوده و دارای توضیحات خوبی می‌باشند. مرور <filename>/etc/munin/plugins/</filename> شیوه خوبی برای اطلاع از کارکرد هر پلاگین و تشخیص اینکه کدام یک باید حذف شود می‌باشد. به طور مشابه، فعال‌سازی یک پلاگین جالب در <filename>/usr/share/munin/plugins/</filename> به سادگی ایجاد پیوند نمادین با استفاده از <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command> می‌باشد. به یاد داشته باشید اگر نام پلاگین به زیرخط یا “_” تمام شود، پلاگین نیازمند یک پارامتر است. این پارامتر باید در نام مرتبط با پیوند نمادین ذخیره‌سازی شود؛ برای نمونه، پلاگین “if_” باید همراه با پیوند نمادین <filename>if_eth0</filename> فعال‌سازی شود، تا بتواند ترافیک شبکه رابط eth0 را مانیتور کند."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/munin-node.conf</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>service</primary><secondary><filename>munin-node.service</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>service munin-node restart</command>."
msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>systemctl restart munin-node</command>."
msgstr "زمانی که تمام پلاگین‌ها راه‌اندازی شوند، پیکربندی فرآیند پس‌زمینه به منظور تعریف کنترل دسترسی به داده‌های گردآوری شده باید بروزرسانی گردد. اینکار شامل عبارت‌های <literal>allow</literal> در فایل <filename>/etc/munin/munin-node.conf</filename> می‌شود. پیکربندی پیشفرض به صورت <literal>allow ^127\\.0\\.0\\.1$</literal> است که تنها اجازه دسترسی به میزبان محلی را می‌دهد. یک مدیرسیستم معمولا خطی مشابه را همراه با نشانی IP میزبان grapher می‌افزاید، سپس اقدام به راه‌اندازی مجدد فرآیند پس‌زمینه با استفاده از <command>service munin-node restart</command> می‌کند."

msgid "<emphasis>GOING FURTHER</emphasis> Creating local plugins"
msgstr "<emphasis>مطالعه بیشتر</emphasis> ایجاد پلاگین‌های محلی"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><command>munin-run</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"
msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"https://guide.munin-monitoring.org/en/latest/plugin/writing.html\" />"
msgstr "Munin شامل مستندات کاملی درباره چگونگی عملکرد و توسعه پلاگین‌ها است. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"

msgid "A plugin is best tested when run in the same conditions as it would be when triggered by munin-node; this can be simulated by running <command>munin-run <replaceable>plugin</replaceable></command> as root. A potential second parameter given to this command (such as <literal>config</literal>) is passed to the plugin as a parameter."
msgstr "یک پلاگین بهتر است در شرایط مشابه با فراخوانی توسط munin-node مورد آزمون قرار گیرد؛ این عمل با اجرای <command>munin-run <replaceable>plugin</replaceable></command> به عنوان root شبیه‌سازی می‌شود. یک پارامتر دوم احتمالی که به این دستور داده می‌شود (از جمله <literal>config</literal>) به عنوان یک پارامتر به پلاگین فرستاده می‌شود."

msgid "When a plugin is invoked with the <literal>config</literal> parameter, it must describe itself by returning a set of fields:"
msgstr "زمانی که یک پلاگین توسط پارامتر <literal>config</literal> فراخوانی می‌شود، باید خود را با بازگرداندن مجموعه‌ای از فیلدها تعریف کند:"

#, fuzzy
#| msgid ""
#| "<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
#| "</userinput><computeroutput>graph_title Load average\n"
#| "graph_args --base 1000 -l 0\n"
#| "graph_vlabel load\n"
#| "graph_scale no\n"
#| "graph_category system\n"
#| "load.label load\n"
#| "graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
#| "load.info 5 minute load average\n"
#| "</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"

#, fuzzy
#| msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"https://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgstr "فیلدهای موجود مختلف توسط “مرجع پلاگین” موجود در قسمت “راهنمای Munin” توضیح داده شده‌اند. <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"

msgid "When invoked without a parameter, the plugin simply returns the last measured values; for instance, executing <command>sudo munin-run load</command> could return <literal>load.value 0.12</literal>."
msgstr "زمانی که بدون پارامتر فراخوانی می‌شود، پلاگین به سادگی آخرین مقدار محاسبه شده را باز می‌گرداند؛ برای نمونه، اجرای <command>sudo munin-run load</command> می‌تواند مقدار <literal>load.value 0.12</literal> را باز گرداند."

msgid "Finally, when a plugin is invoked with the <literal>autoconf</literal> parameter, it should return “yes” (and a 0 exit status) or “no” (with a 1 exit status) according to whether the plugin should be enabled on this host."
msgstr "در نهایت، زمانی که یک پلاگین توسط پارامتر <literal>autoconf</literal> فراخوانی می‌شود، باید مقدار “yes” (گزارش خروج ۰) یا “no” (گزارش خروج ۱) با توجه به اینکه آیا پلاگین باید در این میزبان فعال شود یا خیر را باز گرداند."

msgid "Configuring the Grapher"
msgstr "پیکربندی Grapher"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><command>munin-cron</command></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Munin</primary><secondary><filename>/etc/munin/munin.conf</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The “grapher” is simply the computer that aggregates the data and generates the corresponding graphs. The required software is in the <emphasis role=\"pkg\">munin</emphasis> package. The standard configuration runs <command>munin-cron</command> (once every 5 minutes), which gathers data from all the hosts listed in <filename>/etc/munin/munin.conf</filename> (only the local host is listed by default), saves the historical data in RRD files (<emphasis>Round Robin Database</emphasis>, a file format designed to store data varying in time) stored under <filename>/var/lib/munin/</filename> and generates an HTML page with the graphs in <filename>/var/cache/munin/www/</filename>."
msgstr "“grapher” در واقع رایانه‌ای است که داده‌ها را گردآوری کرده و نمودارهای مرتبط با آن را رسم می‌کند. نرم‌افزار مورد نیاز در بسته <emphasis role=\"pkg\">munin</emphasis> قرار دارد. پیکربندی استاندارد <command>munin-cron</command> را هر ۵ دقیقه یکبار اجرا کرده، تا اطلاعات از تمام میزبان‌های موجود در <filename>/etc/munin/munin.conf</filename> گردآوری شوند (فقط میزبان محلی به صورت پیشفرض قرار دارد)، داده‌های بدست آمده را در فایل‌های RRD، که مخفف <emphasis>Round Robin Database</emphasis> و مناسب ذخیره‌سازی داده‌های متغیر در طول زمان است، ذخیره‌سازی می‌کند که این فایل‌ها در مسیر <filename>/var/lib/munin/</filename> قرار دارند و در نهایت یک صفحه HTML همراه با نمودارها در <filename>/var/cache/munin/www/</filename> ایجاد می‌کند."

msgid "All monitored machines must therefore be listed in the <filename>/etc/munin/munin.conf</filename> configuration file. Each machine is listed as a full section with a name matching the machine and at least an <literal>address</literal> entry giving the corresponding IP address."
msgstr "بنابراین تمام ماشین‌های مانیتور شده باید در فایل پیکربندی <filename>/etc/munin/munin.conf</filename> قرار داشته باشند. هر ماشین به عنوان یک قسمت کامل همراه با نام آن و حداقل یک مدخل <literal>address</literal> که شامل نشانی IP ماشین است، قرار می‌گیرد."

msgid ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"
msgstr ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"

msgid "Sections can be more complex, and describe extra graphs that could be created by combining data coming from several machines. The samples provided in the configuration file are good starting points for customization."
msgstr "قسمت‌ها می‌توانند پیچیده‌تر باشند، تا با ترکیب داده‌های بدست آمده از چند ماشین نمودارهای اضافی رسم گردد. مثال‌های موجود در فایل پیکربندی نقطه آغاز مناسبی برای سفارشی‌کردن این فرآیند هستند."

msgid "The last step is to publish the generated pages; this involves configuring a web server so that the contents of <filename>/var/cache/munin/www/</filename> are made available on a website. Access to this website will often be restricted, using either an authentication mechanism or IP-based access control. See <xref linkend=\"sect.http-web-server\" /> for the relevant details."
msgstr "آخرین گام انتشار صفحات تولید شده است؛ اینکار نیازمند پیکربندی سرور وب به گونه‌ای است که محتوای <filename>/var/cache/munin/www/</filename> از طریق یک وبسایت قابل دسترس باشد. دسترسی به این وبسایت می‌تواند با استفاده از مکانیزم احرازهویت یا کنترل دسترسی مبتنی بر IP مدیریت شود. برای جزئیات مرتبط <xref linkend=\"sect.http-web-server\" /> را مشاهده کنید."

msgid "Setting Up Nagios"
msgstr "راه‌اندازی Nagios"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">nagios4</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary><emphasis role=\"pkg\">monitoring-plugins</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

msgid "Unlike Munin, Nagios does not necessarily require installing anything on the monitored hosts; most of the time, Nagios is used to check the availability of network services. For instance, Nagios can connect to a web server and check that a given web page can be obtained within a given time."
msgstr "برخلاف Munin، الزامی بر نصب Nagios روی میزبان‌های مانیتور شده نیست؛ از Nagios بیشتر به منظور بررسی موجود بودن سرویس‌های شبکه استفاده می‌شود. برای نمونه، Nagios می‌تواند به یک سرور وب متصل شده و بررسی کند یک صفحه مشخص در زمان مشخص قابل دسترس است یا خیر."

msgid "Installing"
msgstr "نصب"

msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios4</emphasis> and <emphasis role=\"pkg\">monitoring-plugins</emphasis> packages. Installing the packages configures the web interface and the Apache server. The <literal>authz_groupfile</literal> and <literal>auth_digest</literal> Apache modules must be enabled, for that execute:"
msgstr ""

msgid ""
"<computeroutput># </computeroutput><userinput>a2enmod authz_groupfile\n"
"</userinput><computeroutput>Considering dependency authz_core for authz_groupfile:\n"
"Module authz_core already enabled\n"
"Module authz_core already enabled\n"
"Enabling module authz_groupfile.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>a2enmod auth_digest\n"
"</userinput><computeroutput>Considering dependency authn_core for auth_digest:\n"
"Module authn_core already enabled\n"
"Enabling module auth_digest.\n"
"To activate the new configuration, you need to run:\n"
"  systemctl restart apache2\n"
"# </computeroutput><userinput>systemctl restart apache2\n"
"</userinput>"
msgstr ""

msgid "<primary><filename>/etc</filename></primary><secondary><filename>/etc/nagios4/</filename></secondary><see>Nagios</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/hdigest.users</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "Adding other users is a simple matter of inserting them in the <filename>/etc/nagios4/hdigest.users</filename> file."
msgstr ""

#, fuzzy
#| msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios3/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios4/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgstr "نشانی <literal>http://<replaceable>server</replaceable>/nagios3/</literal> در مرورگر، رابط وب مربوط به آن را نمایش می‌دهد؛ به طور مشخص، به یاد داشته باشید که Nagios برخی پارامترهای ماشینی که در آن اجرا می‌شود را مانیتور می‌کند. با این حال، برخی ویژگی‌های تعاملی از جمله افزودن دیدگاه به یک میزبان ممکن است کار نکند. این ویژگی‌های در پیکربندی پیشفرض برای Nagios غیرفعال هستند، که بنا بر دلایل امنیتی بسیار محدود کننده است."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/nagios.cfg</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "As documented in <filename>/usr/share/doc/nagios3/README.Debian</filename>, enabling some features involves editing <filename>/etc/nagios3/nagios.cfg</filename> and setting its <literal>check_external_commands</literal> parameter to “1”. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgid "Enabling some features involves editing <filename>/etc/nagios4/nagios.cfg</filename>. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgstr "همانطور که در <filename>/usr/share/doc/nagios3/README.Debian</filename> توضیح داده شده است، فعال‌سازی برخی ویژگی‌ها نیازمند ویرایش <filename>/etc/nagios3/nagios.cfg</filename> و تنظیم پارامتر <literal>check_external_commands</literal> آن به “1” است. همچنین نیاز داریم تا مجوزهای نوشتن روی دایرکتوری که توسط Nagios استفاده می‌شود را با استفاده از دستورات مشابه زیر تنظیم کنیم:"

#, fuzzy
#| msgid ""
#| "<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
#| "<computeroutput>[...]\n"
#| "# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
#| "</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
#| "</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
#| "<computeroutput>[...]</computeroutput>"
msgid ""
"<computeroutput># </computeroutput><userinput>systemctl stop nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios4/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios4\n"
"</userinput><computeroutput># </computeroutput><userinput>systemctl start nagios4\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
"</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
"<computeroutput>[...]</computeroutput>"

msgid "Configuring"
msgstr "پیکربندی"

#, fuzzy
#| msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios3/nagios.cfg</filename>."
msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios4/nagios.cfg</filename>."
msgstr "رابط وب Nagios بسیار کاربردی است، اما اجازه تغییر پیکربندی یا افزودن میزبان‌ها و سرویس‌های قابل مانیتور را نمی‌دهد. پیکربندی کلی از طریق فایل‌های اشاره شده در فایل پیکربندی مرکزی <filename>/etc/nagios3/nagios.cfg</filename> مدیریت می‌شود."

msgid "These files should not be dived into without some understanding of the Nagios concepts. The configuration lists objects of the following types:"
msgstr "بدون درک از مفاهیم Nagios نباید مستقیم سراغ این فایل‌ها رفت. پیکربندی شامل اشیایی از نوع زیر می‌باشد:"

msgid "a <emphasis>host</emphasis> is a machine to be monitored;"
msgstr "یک <emphasis>host</emphasis> ماشینی است که باید مانیتور شود؛"

msgid "a <emphasis>hostgroup</emphasis> is a set of hosts that should be grouped together for display, or to factor some common configuration elements;"
msgstr "یک <emphasis>hostgroup</emphasis> مجموعه‌ای از میزبان‌ها است که برای نمایش یا در نظر گرفتن برخی عناصر پیکربندی، باید گروه‌بندی شوند؛"

msgid "a <emphasis>service</emphasis> is a testable element related to a host or a host group. It will most often be a check for a network service, but it can also involve checking that some parameters are within an acceptable range (for instance, free disk space or processor load);"
msgstr "یک <emphasis>service</emphasis> شامل عنصری قابل آزمون مرتبط با یک میزبان یا گروه میزبانی است. در اکثر موارد برای بررسی یک سرویس شبکه بکار می‌رود، اما می‌تواند برای بررسی محدوده مجاز برخی پارامترها نیز استفاده شود (برای نمونه، فضای آزاد دیسک یا بار پردازنده)؛"

msgid "a <emphasis>servicegroup</emphasis> is a set of services that should be grouped together for display;"
msgstr "یک <emphasis>servicegroup</emphasis> مجموعه‌ای از سرویس‌ها است که برای نمایش باید گروه‌بندی شوند؛"

msgid "a <emphasis>contact</emphasis> is a person who can receive alerts;"
msgstr "یک <emphasis>contact</emphasis> شخصی است که می‌تواند هشدارها را دریافت کند؛"

msgid "a <emphasis>contactgroup</emphasis> is a set of such contacts;"
msgstr "یک <emphasis>contactgroup</emphasis> مجموعه‌ای از چنین افرادی است؛"

msgid "a <emphasis>timeperiod</emphasis> is a range of time during which some services have to be checked;"
msgstr "یک <emphasis>timeperiod</emphasis> بازه‌ای از زمان است که طی آن برخی سرویس‌ها باید بررسی شوند؛"

msgid "a <emphasis>command</emphasis> is the command line invoked to check a given service."
msgstr "یک <emphasis>command</emphasis> دستوری است که به منظور بررسی یک سرویس مشخص فراخوانی می‌شود."

msgid "According to its type, each object has a number of properties that can be customized. A full list would be too long to include, but the most important properties are the relations between the objects."
msgstr "با توجه به نوع، هر شی شامل قابلیت‌هایی است که می‌تواند سفارشی شود. فهرست کامل آن بسیار طولانی است، اما مهم‌ترین قابلیت‌ها همان روابط بین اشیا است."

msgid "A <emphasis>service</emphasis> uses a <emphasis>command</emphasis> to check the state of a feature on a <emphasis>host</emphasis> (or a <emphasis>hostgroup</emphasis>) within a <emphasis>timeperiod</emphasis>. In case of a problem, Nagios sends an alert to all members of the <emphasis>contactgroup</emphasis> linked to the service. Each member is sent the alert according to the channel described in the matching <emphasis>contact</emphasis> object."
msgstr "یک <emphasis>service</emphasis> از <emphasis>command</emphasis> استفاده می‌کند تا وضعیت یک قابلیت موجود در<emphasis>host</emphasis> (یا <emphasis>hostgroup</emphasis>) را طی بازه <emphasis>timeperiod</emphasis> بررسی کند. در صورت بروز مشکل، Nagios یک هشدار به تمام اعضای موجود در <emphasis>contactgroup</emphasis> مرتبط با سرویس ارسال می‌کند. با توجه به کانال تعریف شده برای هر فرد در شی <emphasis>contact</emphasis>، پیام برای وی فرستاده می‌شود."

#, fuzzy
#| msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios3/conf.d/</filename> are a good source of information on how they work."
msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios4/conf.d/</filename> are a good source of information on how they work."
msgstr "یک سیستم ارث‌گرایی امکان اشتراک‌گذاری مجموعه‌ای از قابلیت‌ها را بین چندین شی مختلف بدون تکرار اطلاعات فراهم می‌کند. علاوه بر این، پیکربندی پیشفرض شامل برخی از اشیای استاندارد می‌باشد؛ در اکثر موارد، تعریف میزبان‌ها، سرویس‌ها و افراد جدید به سادگی مشتق شدن از این اشیای عمومی است. فایل‌های موجود در <filename>/etc/nagios3/conf.d/</filename> منبع خوبی از اطلاعات درباره چگونگی کارکرد این اشیا است."

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios4/conf.d/</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

msgid "The Falcot Corp administrators use the following configuration:"
msgstr "مدیرسیستم‌های شرکت فالکوت از پیکربندی زیر استفاده می‌کنند:"

#, fuzzy
#| msgid "<filename>/etc/nagios3/conf.d/falcot.cfg</filename> file"
msgid "<filename>/etc/nagios4/conf.d/falcot.cfg</filename> file"
msgstr "فایل <filename>/etc/nagios3/conf.d/falcot.cfg</filename>"

#, fuzzy
#| msgid ""
#| "define contact{\n"
#| "    name                            generic-contact\n"
#| "    service_notification_period     24x7\n"
#| "    host_notification_period        24x7\n"
#| "    service_notification_options    w,u,c,r\n"
#| "    host_notification_options       d,u,r\n"
#| "    service_notification_commands   notify-service-by-email\n"
#| "    host_notification_commands      notify-host-by-email\n"
#| "    register                        0 ; Template only\n"
#| "}\n"
#| "define contact{\n"
#| "    use             generic-contact\n"
#| "    contact_name    rhertzog\n"
#| "    alias           Raphael Hertzog\n"
#| "    email           hertzog@debian.org\n"
#| "}\n"
#| "define contact{\n"
#| "    use             generic-contact\n"
#| "    contact_name    rmas\n"
#| "    alias           Roland Mas\n"
#| "    email           lolando@debian.org\n"
#| "}\n"
#| "\n"
#| "define contactgroup{\n"
#| "    contactgroup_name     falcot-admins\n"
#| "    alias                 Falcot Administrators\n"
#| "    members               rhertzog,rmas\n"
#| "}\n"
#| "\n"
#| "define host{\n"
#| "    use                   generic-host ; Name of host template to use\n"
#| "    host_name             www-host\n"
#| "    alias                 www.falcot.com\n"
#| "    address               192.168.0.5\n"
#| "    contact_groups        falcot-admins\n"
#| "    hostgroups            debian-servers,ssh-servers\n"
#| "}\n"
#| "define host{\n"
#| "    use                   generic-host ; Name of host template to use\n"
#| "    host_name             ftp-host\n"
#| "    alias                 ftp.falcot.com\n"
#| "    address               192.168.0.6\n"
#| "    contact_groups        falcot-admins\n"
#| "    hostgroups            debian-servers,ssh-servers\n"
#| "}\n"
#| "\n"
#| "# 'check_ftp' command with custom parameters\n"
#| "define command{\n"
#| "    command_name          check_ftp2\n"
#| "    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
#| "}\n"
#| "\n"
#| "# Generic Falcot service\n"
#| "define service{\n"
#| "    name                  falcot-service\n"
#| "    use                   generic-service\n"
#| "    contact_groups        falcot-admins\n"
#| "    register              0\n"
#| "}\n"
#| "\n"
#| "# Services to check on www-host\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   HTTP\n"
#| "    check_command         check_http\n"
#| "}\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   HTTPS\n"
#| "    check_command         check_https\n"
#| "}\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             www-host\n"
#| "    service_description   SMTP\n"
#| "    check_command         check_smtp\n"
#| "}\n"
#| "\n"
#| "# Services to check on ftp-host\n"
#| "define service{\n"
#| "    use                   falcot-service\n"
#| "    host_name             ftp-host\n"
#| "    service_description   FTP\n"
#| "    check_command         check_ftp2\n"
#| "}"
msgid ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.12\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"
msgstr ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"

#, fuzzy
#| msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>."
msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios4/conf.d/services_nagios2.cfg</filename>."
msgstr "این فایل پیکربندی دو میزبان مانیتور شده را تعریف می‌کند. اولی سرور وب است که بررسی‌های موجود برای درگاه‌های ۸۰ و ۴۴۳ انجام می‌شود. Nagios همچنین بررسی می‌کند که سرور SMTP روی درگاه ۲۵ اجرا شود. دومی سرور FTP است که بررسی به منظور دریافت پاسخ ظرف مدت ۲۰ ثانیه در آن صورت می‌گیرد. فراتر از این تاخیر، یک <emphasis>warning</emphasis> صادر می‌شود؛ فراتر از ۳۰ ثانیه، هشدار به منزله بحرانی در نظر گرفته می‌شود. رابط وب Nagios همچنین نشان می‌دهد که سرویس SSH در حال مانیتور شدن است: این اطلاعات از میزبان‌هایی می‌آید که متعلق به گروه <literal>ssh-servers</literal> هستند. سرویس استاندارد منطبق با آن در فایل <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename> تعریف شده است."

msgid "Note the use of inheritance: an object is made to inherit from another object with the “use <replaceable>parent-name</replaceable>”. The parent object must be identifiable, which requires giving it a “name <replaceable>identifier</replaceable>” property. If the parent object is not meant to be a real object, but only to serve as a parent, giving it a “register 0” property tells Nagios not to consider it, and therefore to ignore the lack of some parameters that would otherwise be required."
msgstr "کاربرد ارث‌گرایی را به یاد داشته باشید: یک شی می‌تواند از شی دیگری با استفاده از “use <replaceable>parent-name</replaceable>” ارث‌بری کند. شی والد باید قابل شناسایی باشد، که نیازمند اختصاص قابلیت “name <replaceable>identifier</replaceable>” به آن می‌باشد. اگر قرار بر واقعی نبودن شی والد باشد، اما تنها به صورت والد عمل کند، اختصاص قابلیت “register 0” به آن به Nagios می‌گوید که از آن شی صرف نظر کند، که در این صورت نبود برخی پارامترهای مورد نیاز آن مشکلی را بوجود نمی‌آورد."

msgid "<emphasis>DOCUMENTATION</emphasis> List of object properties"
msgstr "<emphasis>مستندات</emphasis> فهرستی از قابلیت‌های شی"

#, fuzzy
#| msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation provided by the <emphasis role=\"pkg\">nagios3-doc</emphasis> package. This documentation is directly accessible from the web interface, with the “Documentation” link in the top left corner. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation hosted on <ulink url=\"https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/4/en/\" />. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgstr "درک عمیق‌تر از روش‌های دیگری برای پیکربندی Nagios می‌تواند با استفاده از مستندات موجود در بسته <emphasis role=\"pkg\">nagios3-doc</emphasis> حاصل شود. این مستندات به صورت مستقیم از رابط وب، در پیوند “Documentation” موجود در گوشه بالا سمت چپ قابل دسترسی هستند. شامل فهرستی از تمام انواع اشیا همراه با تمام قابلیت‌های موجود در آن‌ها می‌شود. همچنین توضیح می‌دهد چطور پلاگین‌های جدید را ایجاد کنیم."

msgid "<emphasis>GOING FURTHER</emphasis> Remote tests with NRPE"
msgstr "<emphasis>مطالعه بیشتر</emphasis> آزمون‌های راه‌دور با استفاده از NRPE"

msgid "<primary>Nagios Remote Plugin Executor</primary><see>NRPE</see>"
msgstr ""

#, fuzzy
#| msgid "<primary>RAID</primary>"
msgid "<primary>NRPE</primary>"
msgstr "<primary>RAID</primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgid "<primary>Nagios</primary><secondary><emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

#, fuzzy
#| msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgid "<primary>Nagios</primary><secondary><emphasis role=\"pkg\">nagios-nrpe-server</emphasis></secondary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

#, fuzzy
#| msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgid "<primary>Nagios</primary><secondary><filename>/etc/nagios/nrpe.cfg</filename></secondary>"
msgstr "<primary>نصب</primary><secondary>نصب خودکار</secondary>"

#, fuzzy
#| msgid "<primary><command>xe</command></primary>"
msgid "<primary><command>check_nrpe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

#, fuzzy
#| msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <emphasis>check_nrpe</emphasis> command."
msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <command>check_nrpe</command> command."
msgstr "بسیاری پلاگین‌های Nagios امکان بررسی برخی پارامترهای محلی برای یک میزبان را فراهم می‌کنند؛ اگر ماشین‌های زیادی به این بررسی‌ها نیاز داشته باشند در صورتی که یک نصب مرکزی تمام اطلاعات را گردآوری کند، پلاگین NRPE یا <emphasis>Nagios Remote Plugin Executor</emphasis> باید راه‌اندازی گردد. بسته <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> باید در سرور Nagios و بسته <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> باید در میزبان‌هایی که نیازمند برخی آزمون‌ها هستند، نصب شود. گزینه دوم پیکربندی خود را از <filename>/etc/nagios/nrpe.cfg</filename> دریافت می‌کند. این فایل همراه با نشانی‌های IP ماشین‌هایی که قصد اجرای این آزمون‌ها را دارند، باید شامل آزمون‌هایی باشد که از راه‌دور آغاز می‌شوند. در طرف Nagios، فعال‌سازی این آزمون‌های جدید به سادگی افزودن‌ سرویس‌های متناظر با آن‌ها با استفاده از دستور <emphasis>check_nrpe</emphasis> است."

#~ msgid "<primary><emphasis>VMWare</emphasis></primary>"
#~ msgstr "<primary><emphasis>VMWare</emphasis></primary>"

#~ msgid "<primary><emphasis>Bochs</emphasis></primary>"
#~ msgstr "<primary><emphasis>Bochs</emphasis></primary>"

#~ msgid "<primary><emphasis>QEMU</emphasis></primary>"
#~ msgstr "<primary><emphasis>QEMU</emphasis></primary>"

#~ msgid "<primary><emphasis>KVM</emphasis></primary>"
#~ msgstr "<primary><emphasis>KVM</emphasis></primary>"

#~ msgid "<primary><emphasis>LXC</emphasis></primary>"
#~ msgstr "<primary><emphasis>LXC</emphasis></primary>"

#~ msgid "<emphasis>DOCUMENTATION</emphasis> <command>xl</command> options"
#~ msgstr "<emphasis>مستندات</emphasis> گزینه‌های <command>xl</command>"

#~ msgid ""
#~ "#auto eth0\n"
#~ "#iface eth0 inet dhcp\n"
#~ "\n"
#~ "auto br0\n"
#~ "iface br0 inet dhcp\n"
#~ "  bridge-ports eth0"
#~ msgstr ""
#~ "#auto eth0\n"
#~ "#iface eth0 inet dhcp\n"
#~ "\n"
#~ "auto br0\n"
#~ "iface br0 inet dhcp\n"
#~ "  bridge-ports eth0"

#~ msgid "The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<filename>/var/lib/lxc/testlxc/config</filename>) and add a few <literal>lxc.network.*</literal> entries:"
#~ msgstr "فایل سیستم تازه ایجاد شده اکنون شامل یک سیستم پایه دبیان است و مخزن پیشفرض آن هیچ رابط شبکه‌ای ندارد (بجز گزینه loopback). از آنجا که این مورد نظر ما نیست، به ویرایش فایل پیکربندی مخزن (<filename>/var/lib/lxc/testlxc/config</filename>) پرداخته و چندین مدخل <literal>lxc.network.*</literal> را در آن ایجاد می‌کنیم:"

#~ msgid "copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>"
#~ msgstr "رونوشت گرفتن از فایل preseed در <filename>/media/usbdisk/preseed.cfg</filename>"

#~ msgid "<primary>simple-cdd</primary>"
#~ msgstr "<primary>simple-cdd</primary>"

#~ msgid ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"
#~ msgstr ""
#~ "<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
#~ "</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
#~ "</userinput>"

#~ msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> and <emphasis role=\"pkg\">nagios3-doc</emphasis> packages. Installing the packages configures the web interface and creates a first <literal>nagiosadmin</literal> user (for which it asks for a password). Adding other users is a simple matter of inserting them in the <filename>/etc/nagios3/htpasswd.users</filename> file with Apache's <command>htpasswd</command> command. If no Debconf question was displayed during installation, <command>dpkg-reconfigure nagios3-cgi</command> can be used to define the <literal>nagiosadmin</literal> password."
#~ msgstr "اولین گام در راه‌اندازی Nagios نصب بسته‌های <emphasis role=\"pkg\">nagios3</emphasis>، <emphasis role=\"pkg\">nagios-plugins</emphasis> و <emphasis role=\"pkg\">nagios3-doc</emphasis> است. نصب بسته‌ها منجر به پیکربندی یک رابط وب و ایجاد کاربر <literal>nagiosadmin</literal> می‌شود (که برای آن گذرواژه درخواست خواهد شد). افزودن سایر کاربران به سادگی درج آن‌ها در فایل <filename>/etc/nagios3/htpasswd.users</filename> با استفاده از دستور <command>htpasswd</command> در آپاچی است. اگر در زمان نصب پرسشی از طرف Debconf مطرح نشد، می‌توان از <command>dpkg-reconfigure nagios3-cgi</command> برای تعریف گذرواژه <literal>nagiosadmin</literal> استفاده کرد."
